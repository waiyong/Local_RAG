{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.readers.file import PDFReader, DocxReader\n",
    "from llama_index.readers.file.tabular import PandasExcelReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.evaluation import generate_question_context_pairs\n",
    "\n",
    "from llm_loader import load_llm\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create QnA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_evaluation_directories(base_path: str = \"./Document/evaluation_datasets\") -> dict:\n",
    "    \"\"\"Create and return evaluation directory structure\"\"\"\n",
    "    paths = {\n",
    "        \"base\": Path(base_path),\n",
    "        \"llm\": Path(base_path) / \"llm_generated\",\n",
    "        \"manual\": Path(base_path) / \"manual\",\n",
    "        \"results\": Path(base_path) / \"results\",\n",
    "        \"debug\": Path(base_path) / \"debug\",\n",
    "    }\n",
    "    \n",
    "    for path in paths.values():\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = setup_evaluation_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory: str = \"./Document\"):\n",
    "    \"\"\"Load documents using LlamaIndex readers\"\"\"\n",
    "    file_readers = {\n",
    "        \".pdf\": PDFReader(),\n",
    "        \".docx\": DocxReader(),\n",
    "        \".xlsx\": PandasExcelReader()\n",
    "    }\n",
    "    \n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_dir=directory,\n",
    "        file_extractor=file_readers,\n",
    "        filename_as_id=True\n",
    "    )\n",
    "    \n",
    "    documents = reader.load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_evaluation_dataset(\n",
    "    documents,\n",
    "    num_questions_per_chunk: int = 3,\n",
    "    base_output_path: str = \"./Document/evaluation_datasets\"\n",
    "):\n",
    "    \"\"\"Create evaluation dataset using LlamaIndex's generate_question_context_pairs\"\"\"\n",
    "\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = Path(base_output_path) / f\"evaluation_dataset_{timestamp}.json\"\n",
    "    \n",
    "    # Create node parser for chunking\n",
    "    node_parser = SentenceSplitter(\n",
    "        chunk_size=512,\n",
    "        chunk_overlap=50\n",
    "    )\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    print(f\"Total documents: {len(documents)}\")\n",
    "    print(f\"Total chunks: {len(nodes)}\")\n",
    "    print(f\"Estimated questions to generate: {len(nodes) * num_questions_per_chunk}\")\n",
    "\n",
    "    # You can also see chunk distribution per document\n",
    "    doc_chunks = {}\n",
    "    for node in nodes:\n",
    "        doc_id = node.metadata.get('file_name', 'unknown')\n",
    "        doc_chunks[doc_id] = doc_chunks.get(doc_id, 0) + 1\n",
    "\n",
    "    print(\"\\nChunks per document:\")\n",
    "    for doc_id, count in doc_chunks.items():\n",
    "        print(f\"- {doc_id}: {count} chunks\")\n",
    "\n",
    "    # Generate questions with proper prompt\n",
    "    question_prompt = PromptTemplate(\"\"\"\n",
    "    Generate {num_questions} specific questions from this text. \n",
    "    Questions should be complete, answerable questions about the content:\n",
    "    \n",
    "    Text: {context}\n",
    "    \n",
    "    Questions:\n",
    "    \"\"\")\n",
    "\n",
    "    # Generate question-context pairs\n",
    "    print(\"Generating question-context pairs...\")\n",
    "    qc_pairs = generate_question_context_pairs(\n",
    "        nodes=nodes,\n",
    "        llm=Settings.llm,\n",
    "        num_questions_per_chunk=num_questions_per_chunk,\n",
    "        prompt_template=question_prompt\n",
    "    )\n",
    "    \n",
    "    # Format into dataset structure\n",
    "    dataset = {\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"query\": pair[0],\n",
    "                \"reference_contexts\": [pair[1]],  # Keeping original context handling\n",
    "                \"reference_answer\": pair[2] if len(pair) > 2 else \"\",\n",
    "                \"query_by\": {\n",
    "                    \"model_name\": \"llama-2\",\n",
    "                    \"type\": \"ai\",\n",
    "                    \"timestamp\": timestamp\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"source\": getattr(pair[1], 'metadata', {}).get('file_name', ''),\n",
    "                    \"doc_id\": getattr(pair[1], 'node_id', '')\n",
    "                }\n",
    "            }\n",
    "            for pair in qc_pairs\n",
    "        ],\n",
    "        \"dataset_info\": {\n",
    "            \"total_documents\": len(documents),\n",
    "            \"total_chunks\": len(nodes),\n",
    "            \"questions_per_chunk\": num_questions_per_chunk,\n",
    "            \"creation_timestamp\": timestamp,\n",
    "            \"chunk_distribution\": doc_chunks\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save dataset with timestamp\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"Created dataset with {len(dataset['examples'])} examples\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Rewrite the user's follow-up question as a standalone question.\n",
    "\n",
    "1. Include all relevant past context.\n",
    "2. Keep it natural and grammatically correct.\n",
    "3. If already standalone, return it unchanged.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<User's Follow-Up Question>\n",
    "{question}\n",
    "\n",
    "<Rewritten Standalone Question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "response_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "You are an AI assistant providing structured responses.\n",
    "\n",
    "### **Instructions:**\n",
    "- Answer clearly and concisely.\n",
    "- Summarize retrieved context to avoid duplication.\n",
    "- Summarize the key facts efficiently.\n",
    "- If the context lacks enough details, say: \"I don’t have enough information.\"\n",
    "- Format responses in natural sentences.\n",
    "\n",
    "<Retrieved Context>\n",
    "{context}\n",
    "\n",
    "<User's Query>\n",
    "{question}\n",
    "\n",
    "### **AI Response:**\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (3904) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 documents\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = load_llm()\n",
    "Settings.llm = llm\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_folder=\"./embedding_cache\"\n",
    ")\n",
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 documents\n",
      "Total documents: 4\n",
      "Total chunks: 14\n",
      "Estimated questions to generate: 42\n",
      "\n",
      "Chunks per document:\n",
      "- AIBots - FAQs.docx: 5 chunks\n",
      "- Aibot - Datalab.docx: 5 chunks\n",
      "- Synthetic Data Dictionary.xlsx: 1 chunks\n",
      "- Synthetic Standard Operating Procedures for Research Data Laboratory.docx: 3 chunks\n",
      "Generating question-context pairs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14/14 [01:43<00:00,  7.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created dataset with 4 examples\n",
      "Saved to: Document/evaluation_datasets/llm_generated/evaluation_dataset_20250320_153607.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "eval_dataset = create_evaluation_dataset(\n",
    "    documents=documents,\n",
    "    num_questions_per_chunk=3,\n",
    "    base_output_path=paths['llm']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DataLab Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Data Dictionaries</td>\n",
       "      <td>What do the data fields and values mean?</td>\n",
       "      <td>Please refer to the code table.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Lab Availability</td>\n",
       "      <td>When does the data lab have available terminal...</td>\n",
       "      <td>Please check the resource calendar at the MOM ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Software Availability</td>\n",
       "      <td>What software is available on the Data Lab ter...</td>\n",
       "      <td>Tableau, Microsoft Office, Python, R, Stata</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Data Availability</td>\n",
       "      <td>What is the latest year and month of data avai...</td>\n",
       "      <td>Please refer to the Software and Data Products...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tableau/Code Generation</td>\n",
       "      <td>How do I create my own groups in Tableau?</td>\n",
       "      <td>Select the Dimension to Group:\\n\\nIn the Data ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Category                                           Question  \\\n",
       "0        Data Dictionaries           What do the data fields and values mean?   \n",
       "1         Lab Availability  When does the data lab have available terminal...   \n",
       "2    Software Availability  What software is available on the Data Lab ter...   \n",
       "3        Data Availability  What is the latest year and month of data avai...   \n",
       "4  Tableau/Code Generation          How do I create my own groups in Tableau?   \n",
       "\n",
       "                                              Answer  \n",
       "0                    Please refer to the code table.  \n",
       "1  Please check the resource calendar at the MOM ...  \n",
       "2        Tableau, Microsoft Office, Python, R, Stata  \n",
       "3  Please refer to the Software and Data Products...  \n",
       "4  Select the Dimension to Group:\\n\\nIn the Data ...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'Evaluation Scores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manual_questions(excel_path: str)-> Dict:\n",
    "    \"\"\"Load manual questions from Excel and convert to dataset format\"\"\"\n",
    "    \n",
    "    human_truth_df = pd.read_csv(excel_path)[['Category', 'Question', 'Answer']]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Convert Excel data to dataset format\n",
    "    manual_dataset = {\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"category\": row[\"Category\"],\n",
    "                \"question\": row['Question'],  # Will be filled during evaluation\n",
    "                \"reference_answer\": row[\"Answer\"],\n",
    "                \"query_by\": {\n",
    "                    \"model_name\": \"human\",\n",
    "                    \"type\": \"expert\",\n",
    "                    \"timestamp\": timestamp\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"source\": f\"{file_name}.xlsx\",\n",
    "                    \"doc_id\": f\"manual_{idx}\"\n",
    "                }\n",
    "            }\n",
    "            for idx, row in human_truth_df.iterrows()\n",
    "        ],\n",
    "        \"dataset_info\": {\n",
    "            \"creation_timestamp\": timestamp,\n",
    "            \"source\": \"manual\",\n",
    "            \"total_questions\": len(human_truth_df)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return manual_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'Data Dictionaries',\n",
       " 'question': 'What do the data fields and values mean?',\n",
       " 'reference_answer': 'Please refer to the code table.',\n",
       " 'query_by': {'model_name': 'human',\n",
       "  'type': 'expert',\n",
       "  'timestamp': '20250320_162642'},\n",
       " 'metadata': {'source': 'Evaluation Scores.csv.xlsx', 'doc_id': 'manual_0'}}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_dataset_df = load_manual_questions(str(paths['manual'])+'/' + file_name)\n",
    "manual_dataset_df['examples'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_debug_dataframe(evaluator) -> pd.DataFrame:\n",
    "    \"\"\"Create DataFrame for debugging evaluation results\"\"\"\n",
    "    debug_data = []\n",
    "    \n",
    "    for result in evaluator.results:\n",
    "        # Get reference context and answer\n",
    "        ref_context = evaluator.rag_dataset.reference_contexts[result['query_idx']]\n",
    "        ref_answer = evaluator.rag_dataset.reference_answers[result['query_idx']]\n",
    "        \n",
    "        debug_data.append({\n",
    "            'Question': result['query'],\n",
    "            'Reference Answer': ref_answer,\n",
    "            'Generated Answer': result['generated_answer'],\n",
    "            'Reference Context': ref_context,\n",
    "            'Retrieved Chunks': [node.node.text for node in result.get('retrieved_contexts', [])],\n",
    "            'Retrieval Time': f\"{result['retrieval_time']:.3f}s\",\n",
    "            'Accuracy': result['retrieval_metrics']['accuracy'],\n",
    "            'Semantic Score': result['semantic_score']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(debug_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagDataset:\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Initialize with JSON data\n",
    "        \n",
    "        Args:\n",
    "            data (dict): Dictionary containing examples with queries and contexts\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.examples = self.data.get(\"examples\", [])\n",
    "    \n",
    "    @property \n",
    "    def queries(self):\n",
    "        \"\"\"Get list of all queries\"\"\"\n",
    "        return [example[\"query\"] for example in self.examples]\n",
    "    \n",
    "    @property\n",
    "    def reference_contexts(self):\n",
    "        \"\"\"Get list of reference contexts for each example\"\"\"\n",
    "        # Handle both string and dict contexts\n",
    "        contexts = []\n",
    "        for example in self.examples:\n",
    "            context = example.get(\"reference_contexts\", [])\n",
    "            # If context is dictionary, get values\n",
    "            if isinstance(context[0], dict):\n",
    "                contexts.append(list(context[0].values())[0])\n",
    "            else:\n",
    "                contexts.append(context[0])\n",
    "        return contexts\n",
    "    \n",
    "    @property\n",
    "    def reference_answers(self):\n",
    "        \"\"\"Get list of reference answers\"\"\"\n",
    "        return [example.get(\"reference_answer\", \"\") for example in self.examples]\n",
    "    \n",
    "    def get_example(self, idx):\n",
    "        \"\"\"Get complete example at index\"\"\"\n",
    "        if idx >= len(self.examples):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset with {len(self.examples)} examples\")\n",
    "        return self.examples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class EvaluationMetrics:\n",
    "    def __init__(self, rag_dataset):\n",
    "        \"\"\"Initialize with RAG dataset and metrics storage\"\"\"\n",
    "        self.rag_dataset = rag_dataset\n",
    "        self.results = []\n",
    "        self.retrieval_times: List[float] = []\n",
    "        self.retrieval_accuracies: List[float] = []\n",
    "        self.semantic_scores: List[float] = []\n",
    "        self.mrr_scores: List[float] = []\n",
    "        self.ndcg_scores: List[float] = []\n",
    "        self.precision_at_k: Dict[int, List[float]] = {1: [], 3: [], 5: []}\n",
    "        self.map_scores: List[float] = []\n",
    "        # use global embedding model\n",
    "        self.embedding_model = Settings.embed_model\n",
    "\n",
    "    def evaluate_retrieval_accuracy(self, retrieved_contexts: List[str], query_idx: int)-> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate accuracy of retrieved contexts against reference contexts\n",
    "        \n",
    "        Args:\n",
    "            retrieved_contexts: List of retrieved text chunks\n",
    "            query_idx: Index of query in dataset\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing accuracy metrics\n",
    "        \"\"\"\n",
    "        reference_contexts = self.rag_dataset.reference_contexts[query_idx]\n",
    "\n",
    "        retrieved_texts = [node.node.text for node in retrieved_contexts]\n",
    "        \n",
    "        # Convert contexts to sets of sentences for comparison\n",
    "        retrieved_set = set(' '.join(retrieved_texts).split('.'))\n",
    "        reference_set = set(' '.join(reference_contexts).split('.'))\n",
    "        \n",
    "        # Calculate metrics\n",
    "        correct_retrievals = len(retrieved_set.intersection(reference_set))\n",
    "        precision = correct_retrievals / len(retrieved_set) if retrieved_set else 0\n",
    "        recall = correct_retrievals / len(reference_set) if reference_set else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        mrr = self._calculate_mrr(retrieved_texts, reference_contexts)\n",
    "        ndcg = self._calculate_ndcg(retrieved_texts, reference_contexts)\n",
    "        map_score = self._calculate_map(retrieved_texts, reference_contexts)\n",
    "        # Calculate P@k for different k values\n",
    "        precision_k = {}\n",
    "        for k in self.precision_at_k.keys():\n",
    "            p_at_k = self._calculate_precision_at_k(retrieved_texts, reference_contexts, k)\n",
    "            self.precision_at_k[k].append(p_at_k)\n",
    "            precision_k[f\"p@{k}\"] = p_at_k\n",
    "        \n",
    "        # Store scores\n",
    "        self.mrr_scores.append(mrr)\n",
    "        self.ndcg_scores.append(ndcg)\n",
    "        self.map_scores.append(map_score)\n",
    "        \n",
    "        return {\n",
    "            **precision_k,\n",
    "            \"mrr\": mrr,\n",
    "            \"ndcg\": ndcg,\n",
    "            \"map\": map_score,\n",
    "            \"accuracy\": f1,  # Using F1 score as accuracy metric\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"correct_retrievals\": correct_retrievals,\n",
    "            \"total_retrieved\": len(retrieved_set),\n",
    "            \"total_reference\": len(reference_set)\n",
    "        }\n",
    "    \n",
    "    def _calculate_mrr(self, retrieved, reference) -> float:\n",
    "        \"\"\"Calculate Mean Reciprocal Rank\"\"\"\n",
    "        for i, doc in enumerate(retrieved, 1):\n",
    "            if doc in reference:\n",
    "                return 1.0 / i\n",
    "        return 0.0\n",
    "    \n",
    "    def _calculate_ndcg(self, retrieved, reference, k=None) -> float:\n",
    "        \"\"\"Calculate NDCG\"\"\"\n",
    "        if k is None:\n",
    "            k = len(retrieved)\n",
    "        \n",
    "        relevance = [1 if doc in reference else 0 for doc in retrieved[:k]]\n",
    "        ideal = sorted(relevance, reverse=True)\n",
    "        \n",
    "        dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal))\n",
    "        \n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    def _calculate_precision_at_k(self, retrieved, reference, k: int) -> float:\n",
    "        \"\"\"Calculate Precision@K\"\"\"\n",
    "        retrieved_k = retrieved[:k]\n",
    "        relevant_k = sum(1 for doc in retrieved_k if doc in reference)\n",
    "        return relevant_k / k if k > 0 else 0.0\n",
    "\n",
    "    def _calculate_map(self, retrieved, reference) -> float:\n",
    "        \"\"\"Calculate Mean Average Precision\"\"\"\n",
    "        if not reference:\n",
    "            return 0.0\n",
    "        \n",
    "        precisions = []\n",
    "        relevant_found = 0\n",
    "        \n",
    "        for i, doc in enumerate(retrieved, 1):\n",
    "            if doc in reference:\n",
    "                relevant_found += 1\n",
    "                precision_at_i = relevant_found / i\n",
    "                precisions.append(precision_at_i)\n",
    "        \n",
    "        return sum(precisions) / len(reference) if precisions else 0.0\n",
    "        \n",
    "    def evaluate_semantic_quality(self, generated_answer: str, query_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Evaluate semantic similarity between generated and reference answers\n",
    "        \n",
    "        Args:\n",
    "            generated_answer: Generated answer to evaluate\n",
    "            query_idx: Index of query in dataset\n",
    "            \n",
    "        Returns:\n",
    "            Dictionary containing semantic quality metrics\n",
    "        \"\"\"\n",
    "        reference_answer = self.rag_dataset.reference_answers[query_idx]\n",
    "        \n",
    "        # Get embeddings using LlamaIndex's embedding model\n",
    "        gen_embedding = self.embedding_model.get_text_embedding(generated_answer)\n",
    "        ref_embedding = self.embedding_model.get_text_embedding(reference_answer)\n",
    "        \n",
    "        # Calculate cosine similarity\n",
    "        similarity = cosine_similarity(\n",
    "            np.array(gen_embedding).reshape(1, -1),\n",
    "            np.array(ref_embedding).reshape(1, -1)\n",
    "        )[0][0]\n",
    "        \n",
    "        return {\n",
    "            \"semantic_similarity\": similarity,\n",
    "            \"generated_length\": len(generated_answer.split()),\n",
    "            \"reference_length\": len(reference_answer.split())\n",
    "        }\n",
    "    \n",
    "    def evaluate_all_queries(self, query_engine, llm=None):\n",
    "        \"\"\"\n",
    "        Evaluate all queries in the dataset\n",
    "        \n",
    "        Args:\n",
    "            query_engine: RAG query engine for retrieving contexts\n",
    "            llm: Language model for generating answers (optional)\n",
    "        \"\"\"\n",
    "        print(f\"Evaluating {len(self.rag_dataset.queries)} queries...\")\n",
    "        \n",
    "        for idx, query in enumerate(tqdm(self.rag_dataset.queries)):\n",
    "            # Measure retrieval time and get contexts\n",
    "            start_time = time.time()\n",
    "            retrieved_contexts = query_engine.retrieve(query)\n",
    "            retrieval_time = time.time() - start_time\n",
    "            self.retrieval_times.append(retrieval_time)\n",
    "            \n",
    "            # Generate answer if LLM provided\n",
    "            generated_answer = \"\"\n",
    "            if llm:\n",
    "                generated_answer = self._generate_answer(query_engine, query)\n",
    "            \n",
    "            # Evaluate retrieval accuracy\n",
    "            retrieval_metrics = self.evaluate_retrieval_accuracy(\n",
    "                retrieved_contexts,\n",
    "                idx\n",
    "            )\n",
    "            self.retrieval_accuracies.append(retrieval_metrics['accuracy'])\n",
    "            \n",
    "            # Evaluate semantic quality if answer generated\n",
    "            semantic_score = 0.0\n",
    "            if generated_answer:\n",
    "                semantic_metrics = self.evaluate_semantic_quality(\n",
    "                    generated_answer,\n",
    "                    idx\n",
    "                )\n",
    "                semantic_score = semantic_metrics['semantic_similarity']\n",
    "                self.semantic_scores.append(semantic_score)\n",
    "            \n",
    "            # Store complete results\n",
    "            result = {\n",
    "                \"query_idx\": idx,\n",
    "                \"query\": query,\n",
    "                \"retrieval_time\": retrieval_time,\n",
    "                \"retrieval_metrics\": retrieval_metrics,\n",
    "                \"generated_answer\": generated_answer,\n",
    "                \"semantic_score\": semantic_score,\n",
    "                \"timestamp\": datetime.now().isoformat()\n",
    "            }\n",
    "            self.results.append(result)\n",
    "            \n",
    "    def get_summary_metrics(self):\n",
    "        \"\"\"Get summary of all evaluation metrics\"\"\"\n",
    "        summary = {\n",
    "            \"total_queries\": len(self.results),\n",
    "            \"avg_retrieval_time\": np.mean(self.retrieval_times),\n",
    "            \"avg_retrieval_accuracy\": np.mean(self.retrieval_accuracies),\n",
    "            \"avg_semantic_score\": np.mean(self.semantic_scores) if self.semantic_scores else 0.0,\n",
    "            \"avg_mrr\": np.mean(self.mrr_scores),\n",
    "            \"avg_ndcg\": np.mean(self.ndcg_scores),\n",
    "            \"avg_map\": np.mean(self.map_scores),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        # Add average P@k scores\n",
    "        for k in self.precision_at_k.keys():\n",
    "            summary[f\"avg_p@{k}\"] = np.mean(self.precision_at_k[k])\n",
    "        \n",
    "        return summary\n",
    "    \n",
    "    def plot_results(self):\n",
    "        \"\"\"Enhanced visualization with ranking metrics\"\"\"\n",
    "        plt.style.use('seaborn-v0_8')\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        \n",
    "        # Create grid for subplots\n",
    "        gs = fig.add_gridspec(2, 3)\n",
    "        \n",
    "        # Plot 1: Retrieval Times\n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        sns.histplot(self.retrieval_times, kde=True, ax=ax1)\n",
    "        ax1.set_title('Retrieval Time Distribution')\n",
    "        ax1.set_xlabel('Time (seconds)')\n",
    "        \n",
    "        # Plot 2: Accuracy Metrics\n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        accuracy_data = pd.DataFrame({\n",
    "            'F1': self.retrieval_accuracies,\n",
    "            'MRR': self.mrr_scores,\n",
    "            'NDCG': self.ndcg_scores,\n",
    "            'MAP': self.map_scores\n",
    "        })\n",
    "        sns.boxplot(data=accuracy_data, ax=ax2)\n",
    "        ax2.set_title('Ranking Metrics Distribution')\n",
    "        ax2.set_ylabel('Score')\n",
    "        \n",
    "        # Plot 3: P@K Values\n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        p_at_k_data = pd.DataFrame({f'P@{k}': scores \n",
    "                                   for k, scores in self.precision_at_k.items()})\n",
    "        sns.boxplot(data=p_at_k_data, ax=ax3)\n",
    "        ax3.set_title('Precision@K Distribution')\n",
    "        ax3.set_ylabel('Score')\n",
    "\n",
    "                # Plot 4: Semantic Scores\n",
    "        ax4 = fig.add_subplot(gs[1, 0])\n",
    "        if self.semantic_scores:\n",
    "            sns.histplot(self.semantic_scores, kde=True, ax=ax4)\n",
    "            ax4.set_title('Semantic Score Distribution')\n",
    "            ax4.set_xlabel('Semantic Score')\n",
    "        \n",
    "        # Plot 5: Metrics Correlation\n",
    "        ax5 = fig.add_subplot(gs[1, 1:])\n",
    "        metrics = np.column_stack([\n",
    "            self.retrieval_accuracies,\n",
    "            self.mrr_scores,\n",
    "            self.ndcg_scores,\n",
    "            self.map_scores\n",
    "        ])\n",
    "        sns.heatmap(\n",
    "            np.corrcoef(metrics.T),\n",
    "            annot=True,\n",
    "            xticklabels=['F1', 'MRR', 'NDCG', 'MAP'],\n",
    "            yticklabels=['F1', 'MRR', 'NDCG', 'MAP'],\n",
    "            ax=ax5\n",
    "        )\n",
    "        ax5.set_title('Metrics Correlation')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def save_results(self, filepath):\n",
    "        \"\"\"Save evaluation results with source information\"\"\"\n",
    "        output = {\n",
    "            \"summary_metrics\": self.get_summary_metrics(),\n",
    "            \"detailed_results\": self.results,\n",
    "            \"metadata\": {\n",
    "                \"timestamp\": datetime.now().isoformat(),\n",
    "                \"total_queries\": len(self.results),\n",
    "                \"sources\": {\n",
    "                    \"llm\": len([r for r in self.results if r.get(\"source\") == \"llm\"]),\n",
    "                    \"manual\": len([r for r in self.results if r.get(\"source\") == \"manual\"])\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(output, f, indent=2)\n",
    "\n",
    "    def _generate_answer(self, query_engine, query):\n",
    "        \"\"\"Helper to generate answer using Query Engine\"\"\"\n",
    "        try:\n",
    "            # Use the query engine directly since it already has the prompt setup\n",
    "            response = query_engine.query(query)\n",
    "            return str(response)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer: {e}\")\n",
    "            return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_pipeline(\n",
    "    dataset_path: str,\n",
    "    documents,\n",
    "    response_prompt: PromptTemplate,\n",
    "    use_manual: bool = False,\n",
    "    manual_path: str = None,\n",
    "    results_path: Path = None\n",
    ") -> tuple:\n",
    "    \"\"\"Run evaluation and optionally save results\"\"\"\n",
    "    # load llm generated QnA\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        llm_dataset = json.load(f)\n",
    "    # check for manual datasets\n",
    "    if use_manual and manual_path:\n",
    "        manual_dataset = load_manual_questions(manual_path)\n",
    "\n",
    "        # Combine datasets\n",
    "        combined_dataset = {\n",
    "            \"examples\": llm_dataset[\"examples\"] + manual_dataset[\"examples\"],\n",
    "            \"dataset_info\": {\n",
    "                \"total_questions\": len(llm_dataset[\"examples\"]) + len(manual_dataset[\"examples\"]),\n",
    "                \"sources\": ['llm', 'manual'],\n",
    "                \"creation_timestamp\": datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            }\n",
    "        }\n",
    "        eval_dataset = combined_dataset\n",
    "    else:\n",
    "        eval_dataset = llm_dataset \n",
    "        \n",
    "    \n",
    "    # Setup RAG components\n",
    "    rag_dataset = RagDataset(eval_dataset)\n",
    "    index = VectorStoreIndex.from_documents(documents, show_progress=True)\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"compact\",\n",
    "        response_prompt=response_prompt,\n",
    "        similarity_top_k=3,\n",
    "        max_tokens=300,\n",
    "        streaming=False\n",
    "    )\n",
    "    \n",
    "    # Run evaluation\n",
    "    evaluator = EvaluationMetrics(rag_dataset)\n",
    "    evaluator.evaluate_all_queries(query_engine, llm=Settings.llm)\n",
    "    \n",
    "    # Analyze results by source\n",
    "    if use_manual:\n",
    "        llm_results = [r for r in evaluator.results \n",
    "                      if eval_dataset[\"examples\"][r[\"query_idx\"]][\"query_by\"][\"type\"] == \"ai\"]\n",
    "        manual_results = [r for r in evaluator.results \n",
    "                         if eval_dataset[\"examples\"][r[\"query_idx\"]][\"query_by\"][\"type\"] == \"expert\"]\n",
    "        \n",
    "        print(\"\\nResults by Source:\")\n",
    "        print(f\"LLM Questions ({len(llm_results)})\")\n",
    "        print(f\"Manual Questions ({len(manual_results)})\")\n",
    "    \n",
    "    return evaluator, eval_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37535659cd1b44a890b0a85640e3e0ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55b39fa02bea4372a7dea7f4b3f63085",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating 4 queries...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [01:38<00:00, 24.62s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "evaluator_llm, dataset_llm = run_evaluation_pipeline(\n",
    "    dataset_path=str(paths['llm'] / \"evaluation_dataset_20250320_153607.json\"),\n",
    "    documents=documents,\n",
    "    response_prompt=response_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "evaluator_llm.save_results(paths[\"results\"] / f\"evaluation_results_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_df = create_debug_dataframe(evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_df.to_csv(str(paths['debug'] / f\"evaluation_debug_{timestamp}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
