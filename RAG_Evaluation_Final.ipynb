{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import List, Dict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "from typing import List, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "\n",
    "from llama_index.core import (\n",
    "    SimpleDirectoryReader,\n",
    "    VectorStoreIndex,\n",
    "    Settings,\n",
    "    PromptTemplate\n",
    ")\n",
    "from llama_index.readers.file import PDFReader, DocxReader\n",
    "from llama_index.readers.file.tabular import PandasExcelReader\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.evaluation import generate_question_context_pairs, SemanticSimilarityEvaluator, FaithfulnessEvaluator\n",
    "\n",
    "from llm_loader import load_llm\n",
    "import time\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "from helper import inspect_chunks\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create QnA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_evaluation_directories(base_path: str = \"./Document/evaluation_datasets\") -> dict:\n",
    "    \"\"\"Create and return evaluation directory structure\"\"\"\n",
    "    paths = {\n",
    "        \"base\": Path(base_path),\n",
    "        \"llm\": Path(base_path) / \"llm_generated\",\n",
    "        \"manual\": Path(base_path) / \"manual\",\n",
    "        \"results\": Path(base_path) / \"results\",\n",
    "        \"debug\": Path(base_path) / \"debug\",\n",
    "    }\n",
    "    \n",
    "    for path in paths.values():\n",
    "        path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = setup_evaluation_directories()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory: str = \"./Document\"):\n",
    "    \"\"\"Load documents using LlamaIndex readers\"\"\"\n",
    "    file_readers = {\n",
    "        \".pdf\": PDFReader(),\n",
    "        \".docx\": DocxReader(),\n",
    "        \".xlsx\": PandasExcelReader()\n",
    "    }\n",
    "    \n",
    "    reader = SimpleDirectoryReader(\n",
    "        input_dir=directory,\n",
    "        file_extractor=file_readers,\n",
    "        filename_as_id=True\n",
    "    )\n",
    "    \n",
    "    documents = reader.load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 4 documents\n"
     ]
    }
   ],
   "source": [
    "documents = load_documents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "# Initialize LLM\n",
    "llm = load_llm()\n",
    "Settings.llm = llm\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_folder=\"./embedding_cache\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_document_chunks(documents, chunk_size:int = 512, chunk_overlap: int = 50, num_questions_per_chunk:int = 3):\n",
    "    \"\"\"Create chunks of a document\"\"\"\n",
    "    # Create node parser for chunking\n",
    "    node_parser = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    nodes = node_parser.get_nodes_from_documents(documents)\n",
    "    print(f\"Total documents: {len(documents)}\")\n",
    "    print(f\"Total chunks: {len(nodes)}\")\n",
    "    print(f\"Estimated questions to generate: {len(nodes) * num_questions_per_chunk}\")\n",
    "\n",
    "    # You can also see chunk distribution per document\n",
    "    doc_chunks = {}\n",
    "    for node in nodes:\n",
    "        doc_id = node.metadata.get('file_name', 'unknown')\n",
    "        doc_chunks[doc_id] = doc_chunks.get(doc_id, 0) + 1\n",
    "\n",
    "    print(\"\\nChunks per document:\")\n",
    "    for doc_id, count in doc_chunks.items():\n",
    "        print(f\"- {doc_id}: {count} chunks\")\n",
    "\n",
    "    return nodes, doc_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_qa_pairs(nodes, num_questions_per_chunk:int = 3, base_output_path: str = \"./Document/evaluation_datasets\"):\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    output_path = Path(base_output_path) / f\"evaluation_dataset_{timestamp}.json\"\n",
    "\n",
    "    # Generate questions with proper prompt\n",
    "    question_prompt = PromptTemplate(\"\"\"\n",
    "    Given the context information and not prior knowledge, generate only questions based on the below query.\n",
    "\n",
    "    Generate {num_questions} specific questions from this text. \n",
    "    Questions should be complete, answerable questions about the content:\n",
    "\n",
    "    Text: {context}\n",
    "\n",
    "    Questions:\n",
    "    \"\"\")\n",
    "\n",
    "    qc_pairs = generate_question_context_pairs(\n",
    "        nodes=nodes,\n",
    "        llm=Settings.llm,\n",
    "        num_questions_per_chunk=3,\n",
    "        qa_generate_prompt_tmpl=question_prompt\n",
    "    )\n",
    "    qa_dataset = dict(qc_pairs)\n",
    "\n",
    "    queries = qa_dataset.get(\"queries\", {})\n",
    "    corpus = qa_dataset.get(\"corpus\", {})\n",
    "    relevant_docs = qa_dataset.get(\"relevant_docs\", {})\n",
    "\n",
    "\n",
    "    dataset = {\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"query\": question,\n",
    "                \"reference_contexts\": [corpus[doc_id] for doc_id in relevant_docs[query_id]],\n",
    "                \"query_by\": {\n",
    "                    \"model_name\": \"llama-2\",\n",
    "                    \"type\": \"ai\",\n",
    "                    \"timestamp\": timestamp\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"generated_query_id\": query_id\n",
    "                }\n",
    "            }\n",
    "            for query_id, question in queries.items()\n",
    "        ],\n",
    "        \"dataset_info\": {\n",
    "            \"total_chunks\": len(nodes),\n",
    "            \"questions_per_chunk\": num_questions_per_chunk,\n",
    "            \"creation_timestamp\": timestamp,\n",
    "        }\n",
    "        }\n",
    "\n",
    "    \n",
    "    # Save dataset with timestamp\n",
    "    output_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(output_path, 'w') as f:\n",
    "        json.dump(dataset, f, indent=2)\n",
    "    \n",
    "    print(f\"Created dataset with {len(dataset['examples'])} examples\")\n",
    "    print(f\"Saved to: {output_path}\")\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def validate_dataset(dataset: dict, require_answer: bool = False) -> bool:\n",
    "    \"\"\"Validate dataset format and content.\n",
    "    \n",
    "    Args:\n",
    "        dataset (dict): The evaluation dataset.\n",
    "        require_answer (bool): Whether a reference answer is required.\n",
    "        \n",
    "    Returns:\n",
    "        bool: True if validation passes, False otherwise.\n",
    "    \"\"\"\n",
    "    # Check if the main keys exist\n",
    "    for key in ['examples', 'dataset_info']:\n",
    "        if key not in dataset:\n",
    "            print(f\"Dataset is missing key: {key}\")\n",
    "            return False\n",
    "\n",
    "    for example in dataset['examples']:\n",
    "        # Check question format\n",
    "        query = example.get('query', '')\n",
    "        if not isinstance(query, str) or len(query.split()) < 4:\n",
    "            print(f\"Invalid question: {query}\")\n",
    "            return False\n",
    "            \n",
    "        # Check context format: must be a non-empty list of strings\n",
    "        contexts = example.get('reference_contexts', [])\n",
    "        if not isinstance(contexts, list) or not contexts or \\\n",
    "           not all(isinstance(c, str) and c.strip() for c in contexts):\n",
    "            print(f\"Invalid or empty context for question: {query}\")\n",
    "            return False\n",
    "            \n",
    "        # Check answer if required\n",
    "        if require_answer:\n",
    "            answer = example.get('reference_answer', '')\n",
    "            if not answer or not isinstance(answer, str) or not answer.strip():\n",
    "                print(f\"Missing or invalid answer for question: {query}\")\n",
    "                return False\n",
    "    \n",
    "    return True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total documents: 4\n",
      "Total chunks: 14\n",
      "Estimated questions to generate: 42\n",
      "\n",
      "Chunks per document:\n",
      "- AIBots - FAQs.docx: 5 chunks\n",
      "- Aibot - Datalab.docx: 5 chunks\n",
      "- Synthetic Data Dictionary.xlsx: 1 chunks\n",
      "- Synthetic Standard Operating Procedures for Research Data Laboratory.docx: 3 chunks\n",
      "Total Documents (unique sources): 4\n",
      "Total Chunks: 14\n",
      "\n",
      "Chunks per Document:\n",
      "Source\n",
      "AIBots - FAQs.docx                                                           5\n",
      "Aibot - Datalab.docx                                                         5\n",
      "Synthetic Standard Operating Procedures for Research Data Laboratory.docx    3\n",
      "Synthetic Data Dictionary.xlsx                                               1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Chunk Details:\n"
     ]
    }
   ],
   "source": [
    "nodes, doc_chunks = create_document_chunks(documents)\n",
    "inspect_chunk_df = inspect_chunks(nodes, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Chunk #</th>\n",
       "      <th>Text Preview</th>\n",
       "      <th>Length</th>\n",
       "      <th>Source</th>\n",
       "      <th>Start Idx</th>\n",
       "      <th>End Idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>What are the Data Lab's operating hours?\\n\\nThe MOM Data Lab is open from Tuesdays to Thursdays, 9.30 AM to 5.30 PM and is closed from 12 to 2 PM. The lab may also be closed on days when no admins are a...</td>\n",
       "      <td>1952</td>\n",
       "      <td>AIBots - FAQs.docx</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>I have not received a confirmation for my lab booking, can I still head down?\\n\\nIf you did not receive any confirmation, please email us first. There may be cases where there are multiple requests for ...</td>\n",
       "      <td>1924</td>\n",
       "      <td>AIBots - FAQs.docx</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>What kinds of data are available in the Data Lab?\\n\\nThe Data lab houses various datasets and data products/dashboards. Please refer to this our Software and Data Products page for more info.\\n\\n \\n\\nFor ge...</td>\n",
       "      <td>2140</td>\n",
       "      <td>AIBots - FAQs.docx</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>How long does it take to get my data?\\n\\nOur service level agreement is to process your extraction request within 7 working days. However. please note that complex and/or sensitive requests may be subje...</td>\n",
       "      <td>1290</td>\n",
       "      <td>AIBots - FAQs.docx</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>What do I do when I cannot access the MRSD_External folder?\\n\\nFOR EXTERNAL USERS\\n1. Ensure that Symantec Endpoint Protection is on by right clicking on the small arrow on the bottom right corner of the...</td>\n",
       "      <td>1121</td>\n",
       "      <td>AIBots - FAQs.docx</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Chunk #  \\\n",
       "0        1   \n",
       "1        2   \n",
       "2        3   \n",
       "3        4   \n",
       "4        5   \n",
       "\n",
       "                                                                                                                                                                                                        Text Preview  \\\n",
       "0      What are the Data Lab's operating hours?\\n\\nThe MOM Data Lab is open from Tuesdays to Thursdays, 9.30 AM to 5.30 PM and is closed from 12 to 2 PM. The lab may also be closed on days when no admins are a...   \n",
       "1      I have not received a confirmation for my lab booking, can I still head down?\\n\\nIf you did not receive any confirmation, please email us first. There may be cases where there are multiple requests for ...   \n",
       "2  What kinds of data are available in the Data Lab?\\n\\nThe Data lab houses various datasets and data products/dashboards. Please refer to this our Software and Data Products page for more info.\\n\\n \\n\\nFor ge...   \n",
       "3      How long does it take to get my data?\\n\\nOur service level agreement is to process your extraction request within 7 working days. However. please note that complex and/or sensitive requests may be subje...   \n",
       "4     What do I do when I cannot access the MRSD_External folder?\\n\\nFOR EXTERNAL USERS\\n1. Ensure that Symantec Endpoint Protection is on by right clicking on the small arrow on the bottom right corner of the...   \n",
       "\n",
       "   Length              Source Start Idx End Idx  \n",
       "0    1952  AIBots - FAQs.docx                    \n",
       "1    1924  AIBots - FAQs.docx                    \n",
       "2    2140  AIBots - FAQs.docx                    \n",
       "3    1290  AIBots - FAQs.docx                    \n",
       "4    1121  AIBots - FAQs.docx                    "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inspect_chunk_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_dataset = create_llm_qa_pairs(\n",
    "    nodes=documents,\n",
    "    num_questions_per_chunk=3,\n",
    "    base_output_path=paths['llm']\n",
    ")\n",
    "\n",
    "if not validate_dataset(eval_dataset):\n",
    "    print(\"Dataset validation failed - regenerating questions required\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Rewrite the user's follow-up question as a standalone question.\n",
    "\n",
    "1. Include all relevant past context.\n",
    "2. Keep it natural and grammatically correct.\n",
    "3. If already standalone, return it unchanged.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<User's Follow-Up Question>\n",
    "{question}\n",
    "\n",
    "<Rewritten Standalone Question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "response_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "You are an AI assistant providing structured responses.\n",
    "\n",
    "### **Instructions:**\n",
    "- Answer clearly and concisely.\n",
    "- Summarize retrieved context to avoid duplication.\n",
    "- Summarize the key facts efficiently.\n",
    "- If the context lacks enough details, say: \"I don’t have enough information.\"\n",
    "- Format responses in natural sentences.\n",
    "\n",
    "<Retrieved Context>\n",
    "{context}\n",
    "\n",
    "<User's Query>\n",
    "{question}\n",
    "\n",
    "### **AI Response:**\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read DataLab Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'Evaluation Scores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_manual_questions(excel_path: str)-> Dict:\n",
    "    \"\"\"Load manual questions from Excel and convert to dataset format\"\"\"\n",
    "    \n",
    "    human_truth_df = pd.read_csv(excel_path)[['Category', 'Question', 'Answer']]\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    # Convert Excel data to dataset format\n",
    "    manual_dataset = {\n",
    "        \"examples\": [\n",
    "            {\n",
    "                \"category\": row[\"Category\"],\n",
    "                \"question\": row['Question'],  # Will be filled during evaluation\n",
    "                \"reference_answer\": row[\"Answer\"],\n",
    "                \"query_by\": {\n",
    "                    \"model_name\": \"human\",\n",
    "                    \"type\": \"expert\",\n",
    "                    \"timestamp\": timestamp\n",
    "                },\n",
    "                \"metadata\": {\n",
    "                    \"source\": f\"{file_name}.xlsx\",\n",
    "                    \"doc_id\": f\"manual_{idx}\"\n",
    "                }\n",
    "            }\n",
    "            for idx, row in human_truth_df.iterrows()\n",
    "        ],\n",
    "        \"dataset_info\": {\n",
    "            \"creation_timestamp\": timestamp,\n",
    "            \"source\": \"manual\",\n",
    "            \"total_questions\": len(human_truth_df)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return manual_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'category': 'Data Dictionaries',\n",
       " 'question': 'What do the data fields and values mean?',\n",
       " 'reference_answer': 'Please refer to the code table.',\n",
       " 'query_by': {'model_name': 'human',\n",
       "  'type': 'expert',\n",
       "  'timestamp': '20250324_170335'},\n",
       " 'metadata': {'source': 'Evaluation Scores.csv.xlsx', 'doc_id': 'manual_0'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "manual_dataset_df = load_manual_questions(str(paths['manual'])+'/' + file_name)\n",
    "manual_dataset_df['examples'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RUN EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_debug_dataframe(evaluator) -> pd.DataFrame:\n",
    "    \"\"\"Create DataFrame for debugging evaluation results\"\"\"\n",
    "    debug_data = []\n",
    "    \n",
    "    for result in evaluator.results:\n",
    "        # Get reference context and answer\n",
    "        ref_context = evaluator.rag_dataset.reference_contexts[result['query_idx']]\n",
    "        ref_answer = evaluator.rag_dataset.reference_answers[result['query_idx']]\n",
    "        \n",
    "        debug_data.append({\n",
    "            'Question': result['query'],\n",
    "            'Reference Answer': ref_answer,\n",
    "            'Generated Answer': result['generated_answer'],\n",
    "            'Reference Context': ref_context,\n",
    "            'Retrieved Chunks': [node.node.text for node in result.get('retrieved_contexts', [])],\n",
    "            'Retrieval Time': f\"{result['retrieval_time']:.3f}s\",\n",
    "            'Accuracy': result['retrieval_metrics']['accuracy'],\n",
    "            'Semantic Score': result['semantic_score']\n",
    "        })\n",
    "    \n",
    "    df = pd.DataFrame(debug_data)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RagDataset:\n",
    "    def __init__(self, data):\n",
    "        \"\"\"Initialize with JSON data\n",
    "        \n",
    "        Args:\n",
    "            data (dict): Dictionary containing examples with queries and contexts\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.examples = self.data.get(\"examples\", [])\n",
    "    \n",
    "    @property \n",
    "    def queries(self):\n",
    "        \"\"\"Get list of all queries\"\"\"\n",
    "        return [example[\"query\"] for example in self.examples]\n",
    "    \n",
    "    @property\n",
    "    def reference_contexts(self):\n",
    "        \"\"\"Get list of reference contexts for each example\"\"\"\n",
    "        # Handle both string and dict contexts\n",
    "        contexts = []\n",
    "        for example in self.examples:\n",
    "            context = example.get(\"reference_contexts\", [])\n",
    "            # If context is dictionary, get values\n",
    "            if isinstance(context[0], dict):\n",
    "                contexts.append(list(context[0].values())[0])\n",
    "            else:\n",
    "                contexts.append(context[0])\n",
    "        return contexts\n",
    "    \n",
    "    @property\n",
    "    def reference_answers(self):\n",
    "        \"\"\"Get list of reference answers\"\"\"\n",
    "        return [example.get(\"reference_answer\", \"\") for example in self.examples]\n",
    "    \n",
    "    def get_example(self, idx):\n",
    "        \"\"\"Get complete example at index\"\"\"\n",
    "        if idx >= len(self.examples):\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset with {len(self.examples)} examples\")\n",
    "        return self.examples[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvaluationMetrics:\n",
    "    \"\"\"Class for computing and aggregating retrieval metrics.\"\"\"\n",
    "    def __init__(self, rag_dataset):\n",
    "        self.rag_dataset = rag_dataset\n",
    "        self.results = []\n",
    "        self.retrieval_times: List[float] = []\n",
    "        self.retrieval_accuracies: List[float] = []\n",
    "        self.semantic_scores: List[float] = []\n",
    "        self.faithfulness_scores: List[float] = []\n",
    "        self.mrr_scores: List[float] = []\n",
    "        self.ndcg_scores: List[float] = []\n",
    "        self.precision_at_k: Dict[int, List[float]] = {1: [], 3: [], 5: []}\n",
    "        self.hit_rates: List[float] = []\n",
    "        # Use the global embedding model from Settings\n",
    "        self.embedding_model = Settings.embed_model\n",
    "\n",
    "    def compute_retrieval_metrics(self, retrieved_contexts: List, query_idx: int) -> Dict:\n",
    "        \"\"\"\n",
    "        Compute retrieval metrics for a single query.\n",
    "        \"\"\"\n",
    "        reference_contexts = self.rag_dataset.reference_contexts[query_idx]\n",
    "        retrieved_texts = [node.node.text for node in retrieved_contexts]\n",
    "        \n",
    "        # Convert texts into sets of sentences (this method can be refined)\n",
    "        retrieved_set = set(' '.join(retrieved_texts).split('.'))\n",
    "        reference_set = set(' '.join(reference_contexts).split('.'))\n",
    "        \n",
    "        correct = len(retrieved_set.intersection(reference_set))\n",
    "        precision = correct / len(retrieved_set) if retrieved_set else 0\n",
    "        recall = correct / len(reference_set) if reference_set else 0\n",
    "        f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        mrr = self._calculate_mrr(retrieved_texts, reference_contexts)\n",
    "        ndcg = self._calculate_ndcg(retrieved_texts, reference_contexts)\n",
    "        hit = 1.0 if correct > 0 else 0.0\n",
    "        self.hit_rates.append(hit)\n",
    "        \n",
    "        p_at_k = {}\n",
    "        for k in self.precision_at_k.keys():\n",
    "            p = self._calculate_precision_at_k(retrieved_texts, reference_contexts, k)\n",
    "            self.precision_at_k[k].append(p)\n",
    "            p_at_k[f\"p@{k}\"] = p\n",
    "        \n",
    "        self.mrr_scores.append(mrr)\n",
    "        self.ndcg_scores.append(ndcg)\n",
    "        \n",
    "        return {\n",
    "            **p_at_k,\n",
    "            \"mrr\": mrr,\n",
    "            \"ndcg\": ndcg,\n",
    "            \"f1\": f1,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"hit_rate\": hit,\n",
    "            \"correct_retrievals\": correct,\n",
    "            \"total_retrieved\": len(retrieved_set),\n",
    "            \"total_reference\": len(reference_set)\n",
    "        }\n",
    "    \n",
    "    def _calculate_mrr(self, retrieved, reference) -> float:\n",
    "        for i, doc in enumerate(retrieved, 1):\n",
    "            if doc in reference:\n",
    "                return 1.0 / i\n",
    "        return 0.0\n",
    "    \n",
    "    def _calculate_ndcg(self, retrieved, reference, k=None) -> float:\n",
    "        if k is None:\n",
    "            k = len(retrieved)\n",
    "        relevance = [1 if doc in reference else 0 for doc in retrieved[:k]]\n",
    "        ideal = sorted(relevance, reverse=True)\n",
    "        dcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(relevance))\n",
    "        idcg = sum(rel / np.log2(i + 2) for i, rel in enumerate(ideal))\n",
    "        return dcg / idcg if idcg > 0 else 0.0\n",
    "\n",
    "    def _calculate_precision_at_k(self, retrieved, reference, k: int) -> float:\n",
    "        retrieved_k = retrieved[:k]\n",
    "        relevant_k = sum(1 for doc in retrieved_k if doc in reference)\n",
    "        return relevant_k / k if k > 0 else 0.0\n",
    "\n",
    "    def aggregate_summary(self) -> Dict:\n",
    "        summary = {\n",
    "            \"avg_mrr\": np.mean(self.mrr_scores),\n",
    "            \"avg_ndcg\": np.mean(self.ndcg_scores),\n",
    "            \"avg_hit_rate\": np.mean(self.hit_rates),\n",
    "            \"avg_p@1\": np.mean(self.precision_at_k[1]),\n",
    "            \"avg_p@3\": np.mean(self.precision_at_k[3]),\n",
    "            \"avg_p@5\": np.mean(self.precision_at_k[5]),\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        return summary\n",
    "\n",
    "    def plot_metrics(self):\n",
    "        plt.style.use(\"seaborn-v0_8\")\n",
    "        fig = plt.figure(figsize=(20, 10))\n",
    "        gs = fig.add_gridspec(2, 3)\n",
    "        \n",
    "        ax1 = fig.add_subplot(gs[0, 0])\n",
    "        sns.histplot(self.hit_rates, kde=True, ax=ax1)\n",
    "        ax1.set_title(\"Hit Rate Distribution\")\n",
    "        \n",
    "        ax2 = fig.add_subplot(gs[0, 1])\n",
    "        data = np.column_stack([self.mrr_scores, self.ndcg_scores])\n",
    "        df_metrics = pd.DataFrame(data, columns=[\"MRR\", \"NDCG\"])\n",
    "        sns.boxplot(data=df_metrics, ax=ax2)\n",
    "        ax2.set_title(\"MRR & NDCG Distribution\")\n",
    "        \n",
    "        ax3 = fig.add_subplot(gs[0, 2])\n",
    "        p_at_k_data = pd.DataFrame({f\"P@{k}\": scores for k, scores in self.precision_at_k.items()})\n",
    "        sns.boxplot(data=p_at_k_data, ax=ax3)\n",
    "        ax3.set_title(\"Precision@K Distribution\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation(rag_dataset, query_engine, llm=None):\n",
    "    \"\"\"\n",
    "    Run evaluation on the provided dataset.\n",
    "    \n",
    "    For LLM-generated datasets (use_manual=False), the pipeline:\n",
    "      - Uses a query engine to generate answers.\n",
    "      - Evaluates retrieval using FaithfulnessEvaluator (with reference contexts) \n",
    "        and semantic similarity (if a reference answer is provided).\n",
    "    \n",
    "    For expert (human) datasets (use_manual=True), the pipeline:\n",
    "      - Uses the query engine to generate answers.\n",
    "      - Skips retrieval/faithfulness evaluation.\n",
    "      - Evaluates the generated answer only via SemanticSimilarityEvaluator against\n",
    "        the human-provided reference answer.\n",
    "    \n",
    "    Returns:\n",
    "        detailed_results: A list of evaluation results for each query.\n",
    "        (Optionally, you could also return aggregated metrics.)\n",
    "    \"\"\"\n",
    "\n",
    "    evaluator = EvaluationMetrics(rag_dataset)\n",
    "    detailed_results = []\n",
    "    \n",
    "    # Instantiate standardized evaluators from LlamaIndex\n",
    "    semantic_evaluator = SemanticSimilarityEvaluator(\n",
    "        embed_model=Settings.embed_model,\n",
    "        similarity_threshold=0.8\n",
    "    )\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(\n",
    "        llm=Settings.llm\n",
    "    )\n",
    "    \n",
    "    for idx, query in enumerate(tqdm(rag_dataset.queries)):\n",
    "        start_time = time.time()\n",
    "        retrieved_contexts = query_engine.retrieve(query)\n",
    "        retrieval_time = time.time() - start_time\n",
    "        evaluator.retrieval_times.append(retrieval_time)\n",
    "        \n",
    "        generated_answer = \"\"\n",
    "        if llm:\n",
    "            try:\n",
    "                generated_answer = str(query_engine.query(query))\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating answer for query '{query}': {e}\")\n",
    "        \n",
    "        retrieval_metrics = evaluator.compute_retrieval_metrics(retrieved_contexts, idx)\n",
    "        evaluator.retrieval_accuracies.append(retrieval_metrics[\"f1\"])\n",
    "        \n",
    "        # Evaluate semantic similarity using standardized evaluator (if reference answer is available)\n",
    "        semantic_score = 0.0\n",
    "        if generated_answer and rag_dataset.reference_answers[idx]:\n",
    "            sem_result = semantic_evaluator.evaluate_response(\n",
    "                query=\"\",\n",
    "                response=generated_answer,\n",
    "                reference=rag_dataset.reference_answers[idx]\n",
    "            )\n",
    "            semantic_score = sem_result.score if hasattr(sem_result, \"score\") else 0.0\n",
    "            evaluator.semantic_scores.append(semantic_score)\n",
    "        \n",
    "        # Evaluate faithfulness using standardized evaluator (using retrieved contexts as supporting evidence)\n",
    "        faithfulness_score = 0.0\n",
    "        if generated_answer and retrieved_contexts:\n",
    "            # Combine the retrieved texts into a single string\n",
    "            context_text = \" \".join([ctx.node.text for ctx in retrieved_contexts])\n",
    "            faith_result = faithfulness_evaluator.evaluate_response(\n",
    "                query=\"\",\n",
    "                response=generated_answer,\n",
    "                contexts=[context_text]\n",
    "            )\n",
    "            faithfulness_score = faith_result.score if hasattr(faith_result, \"score\") else 0.0\n",
    "            evaluator.faithfulness_scores.append(faithfulness_score)\n",
    "        \n",
    "        result = {\n",
    "            \"query_idx\": idx,\n",
    "            \"query\": query,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"retrieval_metrics\": retrieval_metrics,\n",
    "            \"generated_answer\": generated_answer,\n",
    "            \"semantic_score\": semantic_score,\n",
    "            \"faithfulness_score\": faithfulness_score,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        detailed_results.append(result)\n",
    "        evaluator.results.append(result)\n",
    "    \n",
    "    return evaluator, detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path=str(paths['llm'] / \"evaluation_dataset_20250321_154239.json\")\n",
    "documents_input=documents\n",
    "use_manual=False\n",
    "response_prompt=response_prompt\n",
    "manual_path: str = None\n",
    "results_path: Path = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9dba5ded55b48148dadab9fab9da742",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "468c251b85b145cf9184979904eb251f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load dataset: either LLM-generated or expert-generated.\n",
    "if use_manual and manual_path:\n",
    "    eval_dataset = load_manual_questions(manual_path)\n",
    "else:\n",
    "    with open(dataset_path, 'r') as f:\n",
    "        eval_dataset = json.load(f)\n",
    "\n",
    "# Wrap dataset in your RagDataset (which provides properties for queries, reference answers, etc.)\n",
    "rag_dataset = RagDataset(eval_dataset)\n",
    "\n",
    "# Build the retrieval components (only used if retrieval evaluation is desired)\n",
    "index = VectorStoreIndex.from_documents(documents_input, show_progress=True)\n",
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"compact\",\n",
    "    response_prompt=response_prompt,\n",
    "    similarity_top_k=3,\n",
    "    max_tokens=300,\n",
    "    streaming=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_evaluator = SemanticSimilarityEvaluator(\n",
    "    embed_model=Settings.embed_model,\n",
    "    similarity_threshold=0.8\n",
    ")\n",
    "\n",
    "# For LLM-generated dataset, instantiate the FaithfulnessEvaluator.\n",
    "faithfulness_evaluator = None\n",
    "if not use_manual:\n",
    "    faithfulness_evaluator = FaithfulnessEvaluator(\n",
    "        llm=Settings.llm\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [1:37:38<00:00, 488.24s/it]  \n"
     ]
    }
   ],
   "source": [
    "detailed_results = []\n",
    "for idx, query in enumerate(tqdm(rag_dataset.queries)):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        response_obj = query_engine.query(query)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer for query '{query}': {e}\")\n",
    "        response_obj = None\n",
    "    retrieval_time = time.time() - start_time\n",
    "    \n",
    "    semantic_score = 0.0\n",
    "    faithfulness_score = None\n",
    "    \n",
    "    # Evaluate semantic similarity using the standardized evaluator.\n",
    "    if response_obj and rag_dataset.reference_answers[idx]:\n",
    "        sem_result = semantic_evaluator.evaluate_response(\n",
    "            query=\"\",\n",
    "            response=response_obj,\n",
    "            reference=rag_dataset.reference_answers[idx]\n",
    "        )\n",
    "        semantic_score = sem_result.score if hasattr(sem_result, \"score\") else 0.0\n",
    "    \n",
    "    # For LLM-generated datasets, evaluate faithfulness using the response's source_nodes.\n",
    "    if not use_manual and response_obj and faithfulness_evaluator:\n",
    "        faith_result = faithfulness_evaluator.evaluate_response(\n",
    "            query=\"\",\n",
    "            response=response_obj\n",
    "        )\n",
    "        faithfulness_score = faith_result.score if hasattr(faith_result, \"score\") else 0.0\n",
    "    \n",
    "    result_entry = {\n",
    "        \"query_idx\": idx,\n",
    "        \"query\": query,\n",
    "        \"generated_answer\": response_obj.response if response_obj else \"\",\n",
    "        \"retrieval_time\": retrieval_time,\n",
    "        \"semantic_score\": semantic_score,\n",
    "        \"faithfulness_score\": faithfulness_score,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "    detailed_results.append(result_entry)\n",
    "\n",
    "if results_path:\n",
    "    results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(detailed_results, f, indent=2)\n",
    "    print(f\"Results saved to: {results_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'query_idx': 0,\n",
       "  'query': 'What is the purpose of the context provided?',\n",
       "  'generated_answer': '\\nThe context provided is a set of guidelines for managing and safeguarding the Research Data Laboratory, a secure facility designed for conducting exploratory analysis on sensitive data. The document outlines the access control framework, laboratory security protocols, data management framework, incident management, and compliance and enforcement procedures for the Research Data Laboratory.',\n",
       "  'retrieval_time': 8.802693128585815,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 0.0,\n",
       "  'timestamp': '2025-03-24T17:10:21.649104'},\n",
       " {'query_idx': 1,\n",
       "  'query': '2. Who is the author of the text?',\n",
       "  'generated_answer': '\\nThe text does not contain any information about the author. It appears to be a set of guidelines or procedures for managing and safeguarding a research data laboratory.',\n",
       "  'retrieval_time': 15.972921133041382,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 0.0,\n",
       "  'timestamp': '2025-03-24T17:10:54.206704'},\n",
       " {'query_idx': 2,\n",
       "  'query': '3. What is the main topic of the text?',\n",
       "  'generated_answer': '\\nThe main topic of the text is the establishment of guidelines for managing and safeguarding the Research Data Laboratory, a secure facility designed for conducting exploratory analysis on sensitive data. The procedures outlined in the text govern who may access the facility, what data is available, and how the laboratory and its data must be protected.',\n",
       "  'retrieval_time': 23.755664825439453,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 0.0,\n",
       "  'timestamp': '2025-03-24T17:11:43.899571'},\n",
       " {'query_idx': 3,\n",
       "  'query': 'What is the purpose of the context information provided?',\n",
       "  'generated_answer': '\\nThe context information provided is a set of guidelines for managing and safeguarding the Research Data Laboratory, a secure facility designed for conducting exploratory analysis on sensitive data. The procedures outlined in the document govern who may access the facility, what data is available, and how the laboratory and its data must be protected.',\n",
       "  'retrieval_time': 28.899451971054077,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 1.0,\n",
       "  'timestamp': '2025-03-24T17:12:57.137050'},\n",
       " {'query_idx': 4,\n",
       "  'query': '2. Who is the intended audience for the context information?',\n",
       "  'generated_answer': '2. The intended audience for the context information is individuals seeking access to the Research Data Laboratory and those who need to understand the guidelines, access control framework, laboratory security protocols, data management framework, incident management, and compliance and enforcement procedures related to the facility.',\n",
       "  'retrieval_time': 28.307905673980713,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 1.0,\n",
       "  'timestamp': '2025-03-24T17:13:51.595411'},\n",
       " {'query_idx': 5,\n",
       "  'query': '3. What are the key points discussed in the context information?',\n",
       "  'generated_answer': '\\n1. The context information outlines the guidelines for managing and safeguarding the Research Data Laboratory, a secure facility designed for conducting exploratory analysis on sensitive data.\\n2. Access to the Research Data Laboratory is restricted to personnel with Category 2 security clearance or those granted temporary access under exceptional circumstances.\\n3. The laboratory maintains strict security through multiple control measures, including personal item restrictions, access control, and data security requirements.\\n4. Data access and extraction requests must follow a structured approval process, and users must adhere to comprehensive data protection protocols.\\n5. The laboratory maintains strict data lifecycle management, including storage guidelines, file classification, and clear deletion protocols.\\n6. In the event of a security incident, immediate actions are taken to contain the incident, investigate, and implement corrective measures.\\n7. Violations of security protocols may result in disciplinary action.\\n8. The Data Lab houses various datasets and data products, and users are allowed to share the data with approved recipients.\\n9. Data extracted from the Data Lab must not be transmitted out without prior written authorization, and officers are responsible for any misrepresentation of the data.\\n10',\n",
       "  'retrieval_time': 43.336918115615845,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 1.0,\n",
       "  'timestamp': '2025-03-24T17:15:21.121541'},\n",
       " {'query_idx': 6,\n",
       "  'query': 'What is the purpose of the context provided?',\n",
       "  'generated_answer': '\\nThe context provided is a set of guidelines for managing and safeguarding the Research Data Laboratory, a secure facility designed for conducting exploratory analysis on sensitive data. The document outlines the access control framework, laboratory security protocols, data management framework, incident management, and compliance and enforcement procedures for the Research Data Laboratory.',\n",
       "  'retrieval_time': 28.93760323524475,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 0.0,\n",
       "  'timestamp': '2025-03-24T18:24:21.906842'},\n",
       " {'query_idx': 7,\n",
       "  'query': '2. Who is the author of the text?',\n",
       "  'generated_answer': '\\nThe text does not contain any author information. It appears to be a set of guidelines or procedures for managing and safeguarding a Research Data Laboratory.',\n",
       "  'retrieval_time': 90.88341188430786,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 0.0,\n",
       "  'timestamp': '2025-03-24T18:37:57.239629'},\n",
       " {'query_idx': 8,\n",
       "  'query': '3. What is the main topic of the text?',\n",
       "  'generated_answer': '\\nThe main topic of the text is the establishment of guidelines for managing and safeguarding the Research Data Laboratory, a secure facility designed for conducting exploratory analysis on sensitive data. The procedures outlined in the text govern who may access the facility, what data is available, and how the laboratory and its data must be protected.',\n",
       "  'retrieval_time': 384.32435607910156,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 0.0,\n",
       "  'timestamp': '2025-03-24T18:44:48.430409'},\n",
       " {'query_idx': 9,\n",
       "  'query': 'What is the name of the company that manufactures the product mentioned in the text?',\n",
       "  'generated_answer': ' MOM (Ministry of Manpower) is not a company that manufactures a product. Instead, it is a Singaporean government ministry responsible for managing employment-related issues. The MOM Data Lab is a facility provided by the Ministry for authorized officers to conduct exploratory analysis on granular employment-related data.',\n",
       "  'retrieval_time': 23.466892957687378,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 0.0,\n",
       "  'timestamp': '2025-03-24T18:45:37.844698'},\n",
       " {'query_idx': 10,\n",
       "  'query': '2. What is the product that the company is known for?',\n",
       "  'generated_answer': '2. The company is known for providing secure facilities and guidelines for conducting exploratory analysis on sensitive data in the Research Data Laboratory.',\n",
       "  'retrieval_time': 25.804704666137695,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 1.0,\n",
       "  'timestamp': '2025-03-24T18:46:29.796335'},\n",
       " {'query_idx': 11,\n",
       "  'query': '3. In what year was the company founded?',\n",
       "  'generated_answer': '18 Havelock Rd, Singapore 59764, Level 2, MOM HQ is the location of the MOM Data Lab. The company was founded in 1968.',\n",
       "  'retrieval_time': 26.92216420173645,\n",
       "  'semantic_score': 0.0,\n",
       "  'faithfulness_score': 0.0,\n",
       "  'timestamp': '2025-03-24T18:47:23.007270'}]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detailed_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detailed_results = []\n",
    "for idx, query in enumerate(tqdm(rag_dataset.queries)):\n",
    "    start_time = time.time()\n",
    "    retrieved_contexts = query_engine.retrieve(query)\n",
    "    retrieval_time = time.time() - start_time\n",
    "    \n",
    "    # Use the response object directly\n",
    "    try:\n",
    "        response_obj = query_engine.query(query)\n",
    "    except Exception as e:\n",
    "        print(f\"Error generating answer for query '{query}': {e}\")\n",
    "        response_obj = None\n",
    "    \n",
    "    # Evaluate semantic similarity using standardized evaluator.\n",
    "    semantic_score = 0.0\n",
    "    if response_obj and rag_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_evaluation_pipeline(\n",
    "    dataset_path: str,\n",
    "    documents,\n",
    "    response_prompt: PromptTemplate,\n",
    "    use_manual: bool = False,\n",
    "    manual_path: str = None,\n",
    "    results_path: Path = None\n",
    ") -> tuple:\n",
    "    \"\"\"\n",
    "    Run evaluation on the provided dataset.\n",
    "\n",
    "    For an LLM-generated dataset (use_manual=False), the evaluation uses both\n",
    "    retrieval metrics (and faithfulness evaluation) and semantic similarity.\n",
    "    \n",
    "    For an expert (human) dataset (use_manual=True), only the generated answer's \n",
    "    semantic similarity to the reference answer is evaluated.\n",
    "\n",
    "    Returns:\n",
    "        evaluator: (optional) An object with aggregated metrics.\n",
    "        detailed_results: List of evaluation results per query.\n",
    "    \"\"\"\n",
    "    # Load dataset: either LLM-generated or expert-generated.\n",
    "    if use_manual and manual_path:\n",
    "        eval_dataset = load_manual_questions(manual_path)\n",
    "    else:\n",
    "        with open(dataset_path, 'r') as f:\n",
    "            eval_dataset = json.load(f)\n",
    "\n",
    "    # Wrap dataset in your RagDataset (which provides properties for queries, reference answers, etc.)\n",
    "    rag_dataset = RagDataset(eval_dataset)\n",
    "\n",
    "    # Build the retrieval components (only used if retrieval evaluation is desired)\n",
    "    index = VectorStoreIndex.from_documents(documents_input, show_progress=True)\n",
    "    query_engine = index.as_query_engine(\n",
    "        response_mode=\"compact\",\n",
    "        response_prompt=response_prompt,\n",
    "        similarity_top_k=3,\n",
    "        max_tokens=300,\n",
    "        streaming=False\n",
    "    )\n",
    "\n",
    "    semantic_evaluator = SemanticSimilarityEvaluator(\n",
    "    embed_model=Settings.embed_model,\n",
    "    similarity_threshold=0.8\n",
    ")\n",
    "\n",
    "    # For LLM-generated dataset, instantiate the FaithfulnessEvaluator.\n",
    "    faithfulness_evaluator = None\n",
    "    if not use_manual:\n",
    "        faithfulness_evaluator = FaithfulnessEvaluator(\n",
    "            llm=Settings.llm\n",
    "        )\n",
    "    \n",
    "    detailed_results = []\n",
    "    for idx, query in enumerate(tqdm(rag_dataset.queries)):\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            response_obj = query_engine.query(query)\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating answer for query '{query}': {e}\")\n",
    "            response_obj = None\n",
    "        retrieval_time = time.time() - start_time\n",
    "        \n",
    "        semantic_score = 0.0\n",
    "        faithfulness_score = None\n",
    "        \n",
    "        # Evaluate semantic similarity using the standardized evaluator.\n",
    "        if response_obj and rag_dataset.reference_answers[idx]:\n",
    "            sem_result = semantic_evaluator.evaluate_response(\n",
    "                query=\"\",\n",
    "                response=response_obj,\n",
    "                reference=rag_dataset.reference_answers[idx]\n",
    "            )\n",
    "            semantic_score = sem_result.score if hasattr(sem_result, \"score\") else 0.0\n",
    "        \n",
    "        # For LLM-generated datasets, evaluate faithfulness using the response's source_nodes.\n",
    "        if not use_manual and response_obj and faithfulness_evaluator:\n",
    "            faith_result = faithfulness_evaluator.evaluate_response(\n",
    "                query=\"\",\n",
    "                response=response_obj\n",
    "            )\n",
    "            faithfulness_score = faith_result.score if hasattr(faith_result, \"score\") else 0.0\n",
    "        \n",
    "        result_entry = {\n",
    "            \"query_idx\": idx,\n",
    "            \"query\": query,\n",
    "            \"generated_answer\": response_obj.response if response_obj else \"\",\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"semantic_score\": semantic_score,\n",
    "            \"faithfulness_score\": faithfulness_score,\n",
    "            \"timestamp\": datetime.now().isoformat()\n",
    "        }\n",
    "        detailed_results.append(result_entry)\n",
    "\n",
    "    if results_path:\n",
    "        results_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        with open(results_path, 'w') as f:\n",
    "            json.dump(detailed_results, f, indent=2)\n",
    "        print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b13479b36b24299adab16f88349da92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Parsing nodes:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "26e9e4e120b34162b4c5a33ad7dcb88b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating embeddings:   0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/12 [00:10<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'response'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m evaluator_llm, dataset_llm \u001b[38;5;241m=\u001b[39m \u001b[43mrun_evaluation_pipeline\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpaths\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mllm\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mevaluation_dataset_20250321_154239.json\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_manual\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresponse_prompt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresponse_prompt\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[24], line 88\u001b[0m, in \u001b[0;36mrun_evaluation_pipeline\u001b[0;34m(dataset_path, documents, response_prompt, use_manual, manual_path, results_path)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m use_manual \u001b[38;5;129;01mand\u001b[39;00m generated_answer \u001b[38;5;129;01mand\u001b[39;00m retrieved_contexts:\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;66;03m# Combine retrieved texts into one context string\u001b[39;00m\n\u001b[1;32m     87\u001b[0m     context_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([ctx\u001b[38;5;241m.\u001b[39mnode\u001b[38;5;241m.\u001b[39mtext \u001b[38;5;28;01mfor\u001b[39;00m ctx \u001b[38;5;129;01min\u001b[39;00m retrieved_contexts])\n\u001b[0;32m---> 88\u001b[0m     faith_result \u001b[38;5;241m=\u001b[39m \u001b[43mfaithfulness_evaluator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mevaluate_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerated_answer\u001b[49m\n\u001b[1;32m     90\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     faithfulness_score \u001b[38;5;241m=\u001b[39m faith_result\u001b[38;5;241m.\u001b[39mscore \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(faith_result, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     93\u001b[0m result_entry \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery_idx\u001b[39m\u001b[38;5;124m\"\u001b[39m: idx,\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mquery\u001b[39m\u001b[38;5;124m\"\u001b[39m: query,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m\"\u001b[39m: datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39misoformat()\n\u001b[1;32m    100\u001b[0m }\n",
      "File \u001b[0;32m/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/llama_index/core/evaluation/base.py:103\u001b[0m, in \u001b[0;36mBaseEvaluator.evaluate_response\u001b[0;34m(self, query, response, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m contexts: Optional[Sequence[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 103\u001b[0m     response_str \u001b[38;5;241m=\u001b[39m \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresponse\u001b[49m\n\u001b[1;32m    104\u001b[0m     contexts \u001b[38;5;241m=\u001b[39m [node\u001b[38;5;241m.\u001b[39mget_content() \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m response\u001b[38;5;241m.\u001b[39msource_nodes]\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate(\n\u001b[1;32m    107\u001b[0m     query\u001b[38;5;241m=\u001b[39mquery, response\u001b[38;5;241m=\u001b[39mresponse_str, contexts\u001b[38;5;241m=\u001b[39mcontexts, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    108\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'response'"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "evaluator_llm, dataset_llm = run_evaluation_pipeline(\n",
    "    dataset_path=str(paths['llm'] / \"evaluation_dataset_20250321_154239.json\"),\n",
    "    documents=documents,\n",
    "    use_manual=False,\n",
    "    response_prompt=response_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "evaluator_llm.save_results(paths[\"results\"] / f\"evaluation_results_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_df = create_debug_dataframe(evaluator_llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug_df.to_csv(str(paths['debug'] / f\"evaluation_debug_{timestamp}.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
