{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings, StorageContext\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from config import DOCUMENT_PATH, EMBEDDING_MODEL_NAME, EMBEDDING_MODEL_PATH, TOP_K,MAX_TOKENS_GENERATE,MEMORY_LENGTH,DOCUMENT_PATH_PERSONAL\n",
    "from llm_loader import load_llm\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "from llama_index.core.node_parser import HierarchicalNodeParser, SemanticSplitterNodeParser\n",
    "from llama_index.core.postprocessor import MetadataReplacementPostProcessor, SimilarityPostprocessor\n",
    "from llama_index.core.retrievers import AutoMergingRetriever\n",
    "from llama_index.core.query_engine import RetrieverQueryEngine\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "from llama_index.packs.fusion_retriever.hybrid_fusion.base import HybridFusionRetrieverPack\n",
    "# Retriever imports\n",
    "from llama_index.retrievers.bm25 import BM25Retriever\n",
    "from llama_index.core.retrievers import QueryFusionRetriever\n",
    "\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n",
    "\n",
    "from helper import check_collection_exist\n",
    "from datetime import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (8192) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current embedding model: HuggingFaceEmbedding\n",
      "Successfully generated embedding of length: 384\n"
     ]
    }
   ],
   "source": [
    "llm = load_llm()\n",
    "Settings.llm = llm  # Assign model globally in LlamaIndex\n",
    "\n",
    "# Initialize and verify embedding model\n",
    "embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_folder=\"./embedding_cache\"\n",
    ")\n",
    "\n",
    "# Explicitly set the embedding model\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "# Verify the embedding model\n",
    "print(\"Current embedding model:\", type(Settings.embed_model).__name__)\n",
    "\n",
    "# Test embedding generation\n",
    "test_text = \"This is a test sentence.\"\n",
    "test_embedding = Settings.embed_model.get_text_embedding(test_text)\n",
    "print(f\"Successfully generated embedding of length: {len(test_embedding)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Configurations\n",
    "DELETE_FLAG = True\n",
    "COLLECTION_NAME = \"financial_reports_2\"\n",
    "#DOCUMENT_PATH = \"./data/source_files/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChromaCollectionManager:\n",
    "    def __init__(self, collection_name: str, db_path: str = \"./chroma_db\"):\n",
    "        \"\"\"Initialize ChromaDB collection manager.\"\"\"\n",
    "        self.collection_name = collection_name\n",
    "        self.client = chromadb.PersistentClient(path=db_path)\n",
    "        \n",
    "    def get_or_create_collection(self, delete_existing: bool = False) -> chromadb.Collection:\n",
    "        \"\"\"Get existing collection or create new one.\"\"\"\n",
    "        try:\n",
    "            if delete_existing:\n",
    "                self.delete_collection()\n",
    "                \n",
    "            collection = self.client.get_or_create_collection(\n",
    "                name=self.collection_name,\n",
    "                metadata={\"created_at\": datetime.now().isoformat()}\n",
    "            )\n",
    "            print(f\"Successfully connected to collection '{self.collection_name}'\")\n",
    "            return collection\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error managing collection: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def delete_collection(self) -> None:\n",
    "        \"\"\"Delete collection if it exists.\"\"\"\n",
    "        try:\n",
    "            self.client.delete_collection(self.collection_name)\n",
    "            print(f\"Deleted existing collection '{self.collection_name}'\")\n",
    "        except Exception:\n",
    "            print(\"No existing collection to delete\")\n",
    "\n",
    "    def add_documents(self, collection: chromadb.Collection, \n",
    "                     documents_path: str,\n",
    "                     chunk_size: int = 1024,\n",
    "                     chunk_overlap: int = 50) -> None:\n",
    "        \"\"\"Add documents to collection with chunking.\"\"\"\n",
    "        try:\n",
    "            # Load and process documents\n",
    "            documents = SimpleDirectoryReader(documents_path).load_data()\n",
    "            splitter = SentenceSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap,\n",
    "            )\n",
    "            nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "            # Prepare batches (ChromaDB recommends batching)\n",
    "            batch_size = 100\n",
    "            for i in range(0, len(nodes), batch_size):\n",
    "                batch = nodes[i:i + batch_size]\n",
    "                \n",
    "                texts = [node.get_text() for node in batch]\n",
    "                embeddings = [Settings.embed_model.get_text_embedding(text) for text in texts]\n",
    "                ids = [f\"doc_{i + idx}\" for idx in range(len(batch))]\n",
    "                \n",
    "                collection.add(\n",
    "                    ids=ids,\n",
    "                    documents=texts,\n",
    "                    embeddings=embeddings\n",
    "                )\n",
    "                \n",
    "            print(f\"Stored {len(nodes)} document chunks in collection '{self.collection_name}'\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents_semantic(self, collection: chromadb.Collection, \n",
    "                              documents_path: str,\n",
    "                              chunk_size: int = 1024,\n",
    "                              chunk_overlap: int = 50,\n",
    "                              buffer_size: int = 1,\n",
    "                              breakpoint_percentile: int = 95,\n",
    "                              debug_mode: bool = False) -> None:\n",
    "        \"\"\"Add documents to collection using semantic chunking only.\n",
    "        \n",
    "        Args:\n",
    "            collection: ChromaDB collection to add documents to\n",
    "            documents_path: Path to documents to add\n",
    "            chunk_size: Base chunk size before semantic splitting\n",
    "            chunk_overlap: Amount of overlap between chunks\n",
    "            buffer_size: Number of sentences to consider when calculating similarity\n",
    "            breakpoint_percentile: Percentile threshold for creating chunk boundaries (higher = more chunks)\n",
    "            debug_mode: Whether to print debug information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load documents\n",
    "            documents = SimpleDirectoryReader(documents_path).load_data()\n",
    "            print(f\"Loaded {len(documents)} documents\")\n",
    "            \n",
    "            # First use a basic sentence splitter to get initial chunks\n",
    "            # This creates a baseline before semantic refinement\n",
    "            basic_splitter = SentenceSplitter(\n",
    "                chunk_size=chunk_size,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            base_nodes = basic_splitter.get_nodes_from_documents(documents)\n",
    "            \n",
    "            # Apply semantic splitting\n",
    "            semantic_splitter = SemanticSplitterNodeParser.from_defaults(\n",
    "                buffer_size=buffer_size,\n",
    "                breakpoint_percentile_threshold=breakpoint_percentile,\n",
    "                embed_model=Settings.embed_model,\n",
    "                include_metadata=True,\n",
    "                include_prev_next_rel=True\n",
    "            )\n",
    "            semantic_nodes = semantic_splitter.get_nodes_from_documents(base_nodes)\n",
    "            \n",
    "            print(f\"Created {len(semantic_nodes)} semantic nodes from {len(base_nodes)} base nodes\")\n",
    "            \n",
    "            # Debug information if requested\n",
    "            if debug_mode:\n",
    "                base_lengths = [len(n.get_text()) for n in base_nodes]\n",
    "                semantic_lengths = [len(n.get_text()) for n in semantic_nodes]\n",
    "                \n",
    "                print(\"\\n=== Semantic Splitting Statistics ===\")\n",
    "                print(f\"Before: {len(base_nodes)} base nodes\")\n",
    "                print(f\"After:  {len(semantic_nodes)} semantic nodes\")\n",
    "                print(f\"Change: {len(semantic_nodes) - len(base_nodes)} additional nodes created\")\n",
    "                \n",
    "                print(f\"\\nAvg length before: {sum(base_lengths)/len(base_lengths):.2f} chars\")\n",
    "                print(f\"Avg length after:  {sum(semantic_lengths)/len(semantic_lengths):.2f} chars\")\n",
    "                print(f\"Max length before: {max(base_lengths)}\")\n",
    "                print(f\"Max length after:  {max(semantic_lengths)}\")\n",
    "                \n",
    "                # Sample a node to inspect\n",
    "                if semantic_nodes:\n",
    "                    sample = semantic_nodes[0]\n",
    "                    print(\"\\n=== Sample Semantic Node ===\")\n",
    "                    print(f\"Text preview: {sample.get_text()[:100]}...\")\n",
    "                    print(\"\\nMetadata:\")\n",
    "                    for key, value in sample.metadata.items():\n",
    "                        print(f\"  {key}: {value}\")\n",
    "            \n",
    "            # Store the nodes in batches\n",
    "            self._store_nodes_in_batches(collection, semantic_nodes, \"semantic\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents with semantic chunking: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents_hierarchical(self, collection: chromadb.Collection, \n",
    "                                  documents_path: str,\n",
    "                                  chunk_sizes: list = [1024, 512, 128],\n",
    "                                  chunk_overlap: int = 50,\n",
    "                                  debug_mode: bool = False) -> None:\n",
    "        \"\"\"Add documents to collection using hierarchical chunking only.\n",
    "        \n",
    "        Args:\n",
    "            collection: ChromaDB collection to add documents to\n",
    "            documents_path: Path to documents to add\n",
    "            chunk_sizes: List of chunk sizes for different hierarchy levels\n",
    "            chunk_overlap: Amount of overlap between chunks\n",
    "            debug_mode: Whether to print debug information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load documents\n",
    "            documents = SimpleDirectoryReader(documents_path).load_data()\n",
    "            print(f\"Loaded {len(documents)} documents\")\n",
    "            \n",
    "            # Create hierarchical nodes\n",
    "            h_node_parser = HierarchicalNodeParser.from_defaults(\n",
    "                chunk_sizes=chunk_sizes,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            hierarchical_nodes = h_node_parser.get_nodes_from_documents(documents)\n",
    "            \n",
    "            print(f\"Created {len(hierarchical_nodes)} hierarchical nodes\")\n",
    "            \n",
    "            # Debug information if requested\n",
    "            if debug_mode:\n",
    "                # Create a hierarchy map\n",
    "                hierarchy_map = {}\n",
    "                for node in hierarchical_nodes:\n",
    "                    level = node.metadata.get(\"level\", 0)\n",
    "                    if level not in hierarchy_map:\n",
    "                        hierarchy_map[level] = 0\n",
    "                    hierarchy_map[level] += 1\n",
    "                \n",
    "                # Display hierarchy statistics\n",
    "                print(\"\\n=== Hierarchical Structure ===\")\n",
    "                for level in sorted(hierarchy_map.keys()):\n",
    "                    print(f\"Level {level}: {hierarchy_map[level]} nodes\")\n",
    "                \n",
    "                # Sample nodes from each level for inspection\n",
    "                print(\"\\n=== Sample Nodes by Level ===\")\n",
    "                for level in sorted(hierarchy_map.keys()):\n",
    "                    sample_nodes = [n for n in hierarchical_nodes if n.metadata.get(\"level\", 0) == level]\n",
    "                    if sample_nodes:\n",
    "                        sample = sample_nodes[0]\n",
    "                        print(f\"\\nLEVEL {level} SAMPLE (token length: {len(sample.get_text().split())})\")\n",
    "                        # Print truncated text\n",
    "                        sample_text = sample.get_text()\n",
    "                        print(f\"{sample_text[:100]}...\" if len(sample_text) > 100 else sample_text)\n",
    "                        \n",
    "                        # Show metadata\n",
    "                        print(\"\\nMetadata:\")\n",
    "                        for key, value in sample.metadata.items():\n",
    "                            # Truncate lengthy metadata values\n",
    "                            if isinstance(value, list) and len(value) > 3:\n",
    "                                print(f\"  {key}: {value[:3]}... ({len(value)} items)\")\n",
    "                            else:\n",
    "                                print(f\"  {key}: {value}\")\n",
    "            \n",
    "            # Store the nodes in batches\n",
    "            self._store_nodes_in_batches(collection, hierarchical_nodes, \"hierarchical\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents with hierarchical chunking: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def add_documents_hierarchical_semantic(self, collection: chromadb.Collection, \n",
    "                                           documents_path: str,\n",
    "                                           chunk_sizes: list = [1024, 512, 128],\n",
    "                                           chunk_overlap: int = 50,\n",
    "                                           breakpoint_percentile: int = 95,\n",
    "                                           debug_mode: bool = False) -> None:\n",
    "        \"\"\"Add documents using both hierarchical and semantic chunking.\n",
    "        \n",
    "        Args:\n",
    "            collection: ChromaDB collection to add documents to\n",
    "            documents_path: Path to documents to add\n",
    "            chunk_sizes: List of chunk sizes for different hierarchy levels\n",
    "            chunk_overlap: Amount of overlap between chunks\n",
    "            breakpoint_percentile: Percentile threshold for semantic splitting\n",
    "            debug_mode: Whether to print debug information\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Load documents\n",
    "            documents = SimpleDirectoryReader(documents_path).load_data()\n",
    "            print(f\"Loaded {len(documents)} documents\")\n",
    "            \n",
    "            # Step 1: Create hierarchical nodes\n",
    "            h_node_parser = HierarchicalNodeParser.from_defaults(\n",
    "                chunk_sizes=chunk_sizes,\n",
    "                chunk_overlap=chunk_overlap\n",
    "            )\n",
    "            hierarchical_nodes = h_node_parser.get_nodes_from_documents(documents)\n",
    "            \n",
    "            print(f\"Created {len(hierarchical_nodes)} hierarchical nodes\")\n",
    "            \n",
    "            # Debug hierarchical structure if requested\n",
    "            if debug_mode:\n",
    "                self._debug_hierarchical_nodes(hierarchical_nodes)\n",
    "            \n",
    "            # Step 2: Refine with semantic splitting\n",
    "            s_node_parser = SemanticSplitterNodeParser.from_defaults(\n",
    "                buffer_size=1,\n",
    "                breakpoint_percentile_threshold=breakpoint_percentile,\n",
    "                embed_model=Settings.embed_model,\n",
    "                include_metadata=True,\n",
    "                include_prev_next_rel=True\n",
    "            )\n",
    "            semantic_nodes = s_node_parser.get_nodes_from_documents(hierarchical_nodes)\n",
    "            \n",
    "            print(f\"Further refined into {len(semantic_nodes)} semantic nodes\")\n",
    "            \n",
    "            # Debug semantic impact if requested\n",
    "            if debug_mode:\n",
    "                self._debug_semantic_impact(hierarchical_nodes, semantic_nodes)\n",
    "            \n",
    "            # Store the nodes in batches\n",
    "            self._store_nodes_in_batches(collection, semantic_nodes, \"hierarchical-semantic\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error adding documents with hierarchical-semantic chunking: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "    def _store_nodes_in_batches(self, collection, nodes, chunking_type=\"default\", batch_size=100):\n",
    "        \"\"\"Helper method to store nodes in batches with appropriate metadata.\"\"\"\n",
    "        total_added = 0\n",
    "        \n",
    "        for i in range(0, len(nodes), batch_size):\n",
    "            batch = nodes[i:i + batch_size]\n",
    "            \n",
    "            texts = [node.get_text() for node in batch]\n",
    "            \n",
    "            # Extract and enrich metadata\n",
    "            metadatas = []\n",
    "            for node in batch:\n",
    "                # Start with basic metadata\n",
    "                metadata = {\n",
    "                    \"document_id\": node.id_,\n",
    "                    \"file_name\": node.metadata.get(\"file_name\", \"\"),\n",
    "                    \"chunking_type\": chunking_type\n",
    "                }\n",
    "                \n",
    "                # Add hierarchical metadata if available\n",
    "                if \"level\" in node.metadata:\n",
    "                    metadata[\"hierarchy_level\"] = node.metadata[\"level\"]\n",
    "                if \"parent_id\" in node.metadata:\n",
    "                    metadata[\"parent_id\"] = node.metadata[\"parent_id\"]\n",
    "                if \"child_ids\" in node.metadata:\n",
    "                    metadata[\"child_count\"] = len(node.metadata[\"child_ids\"])\n",
    "                    metadata[\"child_ids\"] = str(node.metadata[\"child_ids\"])  # Convert list to string for Chroma\n",
    "                \n",
    "                # Add semantic relationship metadata if available\n",
    "                if \"next_node_id\" in node.metadata:\n",
    "                    metadata[\"next_node_id\"] = node.metadata[\"next_node_id\"]\n",
    "                if \"prev_node_id\" in node.metadata:\n",
    "                    metadata[\"prev_node_id\"] = node.metadata[\"prev_node_id\"]\n",
    "                \n",
    "                metadatas.append(metadata)\n",
    "            \n",
    "            # Generate embeddings\n",
    "            embeddings = [Settings.embed_model.get_text_embedding(text) for text in texts]\n",
    "            \n",
    "            # Create unique IDs\n",
    "            ids = [f\"{chunking_type}_{total_added + idx}\" for idx in range(len(batch))]\n",
    "            \n",
    "            # Add to collection\n",
    "            collection.add(\n",
    "                ids=ids,\n",
    "                documents=texts,\n",
    "                embeddings=embeddings,\n",
    "                metadatas=metadatas\n",
    "            )\n",
    "            \n",
    "            total_added += len(batch)\n",
    "        \n",
    "        print(f\"Stored {total_added} {chunking_type} document chunks in collection '{self.collection_name}'\")\n",
    "    \n",
    "    def _debug_hierarchical_nodes(self, nodes):\n",
    "        \"\"\"Helper method to display debug info for hierarchical nodes.\"\"\"\n",
    "        # Create a hierarchy map\n",
    "        hierarchy_map = {}\n",
    "        for node in nodes:\n",
    "            level = node.metadata.get(\"level\", 0)\n",
    "            if level not in hierarchy_map:\n",
    "                hierarchy_map[level] = 0\n",
    "            hierarchy_map[level] += 1\n",
    "        \n",
    "        # Display hierarchy statistics\n",
    "        print(\"\\n=== Hierarchical Structure ===\")\n",
    "        for level in sorted(hierarchy_map.keys()):\n",
    "            print(f\"Level {level}: {hierarchy_map[level]} nodes\")\n",
    "        \n",
    "        # Sample nodes from each level for inspection\n",
    "        print(\"\\n=== Sample Nodes by Level ===\")\n",
    "        for level in sorted(hierarchy_map.keys()):\n",
    "            sample_nodes = [n for n in nodes if n.metadata.get(\"level\", 0) == level]\n",
    "            if sample_nodes:\n",
    "                sample = sample_nodes[0]\n",
    "                print(f\"\\nLEVEL {level} SAMPLE (token length: {len(sample.get_text().split())})\")\n",
    "                # Print truncated text\n",
    "                sample_text = sample.get_text()\n",
    "                print(f\"{sample_text[:100]}...\" if len(sample_text) > 100 else sample_text)\n",
    "                \n",
    "                # Show metadata\n",
    "                print(\"\\nMetadata:\")\n",
    "                for key, value in sample.metadata.items():\n",
    "                    # Truncate lengthy metadata values\n",
    "                    if isinstance(value, list) and len(value) > 3:\n",
    "                        print(f\"  {key}: {value[:3]}... ({len(value)} items)\")\n",
    "                    else:\n",
    "                        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    def _debug_semantic_impact(self, before_nodes, after_nodes):\n",
    "        \"\"\"Helper method to display debug info comparing before/after semantic splitting.\"\"\"\n",
    "        # Analyze text length changes\n",
    "        before_lengths = [len(n.get_text()) for n in before_nodes]\n",
    "        after_lengths = [len(n.get_text()) for n in after_nodes]\n",
    "        \n",
    "        print(\"\\n=== Semantic Splitting Impact ===\")\n",
    "        print(f\"Before: {len(before_nodes)} nodes\")\n",
    "        print(f\"After:  {len(after_nodes)} nodes\")\n",
    "        print(f\"Change: {len(after_nodes) - len(before_nodes)} additional nodes created\")\n",
    "        \n",
    "        print(f\"\\nAvg length before: {sum(before_lengths)/len(before_lengths):.2f} chars\")\n",
    "        print(f\"Avg length after:  {sum(after_lengths)/len(after_lengths):.2f} chars\")\n",
    "        print(f\"Max length before: {max(before_lengths)}\")\n",
    "        print(f\"Max length after:  {max(after_lengths)}\")\n",
    "\n",
    "    # def add_documents_hierarchical_semantic(self, collection: chromadb.Collection, \n",
    "    #                                     documents_path: str,\n",
    "    #                                     chunk_sizes: list = [2048, 512, 128],\n",
    "    #                                     chunk_overlap: int = 50,\n",
    "    #                                     debug_mode: bool = True,\n",
    "    #                                     semantic_enabled: bool = True) -> None:\n",
    "    #     \"\"\"Add documents to collection using hierarchical and optionally semantic chunking.\n",
    "        \n",
    "    #     Args:\n",
    "    #         collection: ChromaDB collection to add documents to\n",
    "    #         documents_path: Path to documents to add\n",
    "    #         chunk_sizes: List of chunk sizes for different hierarchy levels\n",
    "    #         chunk_overlap: Amount of overlap between chunks\n",
    "    #         debug_mode: Whether to print debug information about the hierarchy\n",
    "    #         semantic_enabled: Whether to enable semantic splitting after hierarchical parsing\n",
    "    #     \"\"\"\n",
    "    #     try:\n",
    "    #         # Verify embedding model before processing\n",
    "    #         if not isinstance(Settings.embed_model, HuggingFaceEmbedding):\n",
    "    #             raise ValueError(f\"Expected HuggingFaceEmbedding, but got {type(Settings.embed_model).__name__}\")\n",
    "            \n",
    "    #         # Load documents\n",
    "    #         documents = SimpleDirectoryReader(documents_path).load_data()\n",
    "    #         print(f\"Loaded {len(documents)} documents\")\n",
    "            \n",
    "    #         # Step 1: Create hierarchical nodes\n",
    "    #         h_node_parser = HierarchicalNodeParser.from_defaults(\n",
    "    #             chunk_sizes=chunk_sizes,\n",
    "    #             chunk_overlap=chunk_overlap\n",
    "    #         )\n",
    "    #         hierarchical_nodes = h_node_parser.get_nodes_from_documents(documents)\n",
    "            \n",
    "    #         print(f\"Created {len(hierarchical_nodes)} hierarchical nodes\")\n",
    "            \n",
    "    #         # Debug: Visualize the hierarchical structure\n",
    "    #         if debug_mode:\n",
    "    #             # Create a hierarchy map\n",
    "    #             hierarchy_map = {}\n",
    "    #             for node in hierarchical_nodes:\n",
    "    #                 level = node.metadata.get(\"level\", 0)\n",
    "    #                 if level not in hierarchy_map:\n",
    "    #                     hierarchy_map[level] = 0\n",
    "    #                 hierarchy_map[level] += 1\n",
    "                \n",
    "    #             # Display hierarchy statistics\n",
    "    #             print(\"\\n=== Hierarchical Structure ===\")\n",
    "    #             for level in sorted(hierarchy_map.keys()):\n",
    "    #                 print(f\"Level {level}: {hierarchy_map[level]} nodes\")\n",
    "                \n",
    "    #             # Sample nodes from each level for inspection\n",
    "    #             print(\"\\n=== Sample Nodes by Level ===\")\n",
    "    #             for level in sorted(hierarchy_map.keys()):\n",
    "    #                 sample_nodes = [n for n in hierarchical_nodes if n.metadata.get(\"level\", 0) == level]\n",
    "    #                 if sample_nodes:\n",
    "    #                     sample = sample_nodes[0]\n",
    "    #                     print(f\"\\nLEVEL {level} SAMPLE (token length: {len(sample.get_text().split())})\")\n",
    "    #                     # Print truncated text\n",
    "    #                     sample_text = sample.get_text()\n",
    "    #                     print(f\"{sample_text[:100]}...\" if len(sample_text) > 100 else sample_text)\n",
    "                        \n",
    "    #                     # Show metadata\n",
    "    #                     print(\"\\nMetadata:\")\n",
    "    #                     for key, value in sample.metadata.items():\n",
    "    #                         # Truncate lengthy metadata values\n",
    "    #                         if isinstance(value, list) and len(value) > 3:\n",
    "    #                             print(f\"  {key}: {value[:3]}... ({len(value)} items)\")\n",
    "    #                         else:\n",
    "    #                             print(f\"  {key}: {value}\")\n",
    "            \n",
    "    #         # Choose whether to apply semantic parsing\n",
    "    #         if semantic_enabled:\n",
    "    #             # Step 2: Refine with semantic splitting\n",
    "    #             s_node_parser = SemanticSplitterNodeParser.from_defaults(\n",
    "    #                 buffer_size=1,\n",
    "    #                 breakpoint_percentile_threshold=95,\n",
    "    #                 embed_model=Settings.embed_model,\n",
    "    #                 include_metadata=True,\n",
    "    #                 include_prev_next_rel=True\n",
    "    #             )\n",
    "    #             final_nodes = s_node_parser.get_nodes_from_documents(hierarchical_nodes)\n",
    "    #             print(f\"Further refined into {len(final_nodes)} semantic nodes\")\n",
    "                \n",
    "    #             # Debug: Compare pre and post semantic splitting\n",
    "    #             if debug_mode:\n",
    "    #                 # Display semantic splitting statistics\n",
    "    #                 print(\"\\n=== Semantic Splitting Impact ===\")\n",
    "    #                 print(f\"Before: {len(hierarchical_nodes)} hierarchical nodes\")\n",
    "    #                 print(f\"After:  {len(final_nodes)} semantic nodes\")\n",
    "    #                 print(f\"Change: {len(final_nodes) - len(hierarchical_nodes)} additional nodes created\")\n",
    "                    \n",
    "    #                 # Analyze text length changes\n",
    "    #                 h_lengths = [len(n.get_text()) for n in hierarchical_nodes]\n",
    "    #                 s_lengths = [len(n.get_text()) for n in final_nodes]\n",
    "                    \n",
    "    #                 print(f\"\\nAvg length before: {sum(h_lengths)/len(h_lengths):.2f} chars\")\n",
    "    #                 print(f\"Avg length after:  {sum(s_lengths)/len(s_lengths):.2f} chars\")\n",
    "    #                 print(f\"Max length before: {max(h_lengths)}\")\n",
    "    #                 print(f\"Max length after:  {max(s_lengths)}\")\n",
    "    #         else:\n",
    "    #             # Use hierarchical nodes directly\n",
    "    #             final_nodes = hierarchical_nodes\n",
    "    #             print(f\"Using {len(final_nodes)} hierarchical nodes without semantic refinement\")\n",
    "            \n",
    "    #         # Prepare batches with enhanced metadata\n",
    "    #         batch_size = 100\n",
    "    #         total_added = 0\n",
    "            \n",
    "    #         for i in range(0, len(final_nodes), batch_size):\n",
    "    #             batch = final_nodes[i:i + batch_size]\n",
    "                \n",
    "    #             texts = [node.get_text() for node in batch]\n",
    "                \n",
    "    #             # Extract hierarchical metadata\n",
    "    #             metadatas = []\n",
    "    #             for node in batch:\n",
    "    #                 # Extract basic metadata\n",
    "    #                 metadata = {\n",
    "    #                     \"document_id\": node.id_,\n",
    "    #                     \"file_name\": node.metadata.get(\"file_name\", \"\"),\n",
    "    #                     \"hierarchy_level\": node.metadata.get(\"level\", 0),\n",
    "    #                 }\n",
    "                    \n",
    "    #                 # Add relationship metadata if available\n",
    "    #                 if \"parent_id\" in node.metadata:\n",
    "    #                     metadata[\"parent_id\"] = node.metadata[\"parent_id\"]\n",
    "    #                 if \"child_ids\" in node.metadata:\n",
    "    #                     metadata[\"child_count\"] = len(node.metadata[\"child_ids\"])\n",
    "                    \n",
    "    #                 # Add semantic relationship info if available\n",
    "    #                 if \"next_node_id\" in node.metadata:\n",
    "    #                     metadata[\"next_node_id\"] = node.metadata[\"next_node_id\"]\n",
    "    #                 if \"prev_node_id\" in node.metadata:\n",
    "    #                     metadata[\"prev_node_id\"] = node.metadata[\"prev_node_id\"]\n",
    "                    \n",
    "    #                 metadatas.append(metadata)\n",
    "                \n",
    "    #             # Generate embeddings\n",
    "    #             embeddings = [Settings.embed_model.get_text_embedding(text) for text in texts]\n",
    "                \n",
    "    #             # Create unique IDs\n",
    "    #             ids = [f\"node_{total_added + idx}\" for idx in range(len(batch))]\n",
    "                \n",
    "    #             # Add to collection\n",
    "    #             collection.add(\n",
    "    #                 ids=ids,\n",
    "    #                 documents=texts,\n",
    "    #                 embeddings=embeddings,\n",
    "    #                 metadatas=metadatas\n",
    "    #             )\n",
    "                \n",
    "    #             total_added += len(batch)\n",
    "            \n",
    "    #         parsing_type = \"hierarchical-semantic\" if semantic_enabled else \"hierarchical\"\n",
    "    #         print(f\"Stored {total_added} {parsing_type} document chunks in collection '{self.collection_name}'\")\n",
    "            \n",
    "    #     except Exception as e:\n",
    "    #         print(f\"Error adding documents: {str(e)}\")\n",
    "    #         raise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 99 documents\n"
     ]
    }
   ],
   "source": [
    "def testing(documents_path = DOCUMENT_PATH_PERSONAL, chunk_sizes = [2048, 512, 128], chunk_overlap = 50):\n",
    "    documents = SimpleDirectoryReader(documents_path).load_data()\n",
    "    print(f\"Loaded {len(documents)} documents\")\n",
    "\n",
    "    # Step 1: Create hierarchical nodes\n",
    "    h_node_parser = HierarchicalNodeParser.from_defaults(\n",
    "        chunk_sizes=chunk_sizes,\n",
    "        chunk_overlap=chunk_overlap\n",
    "    )\n",
    "    hierarchical_nodes = h_node_parser.get_nodes_from_documents(documents)\n",
    "\n",
    "    return hierarchical_nodes\n",
    "\n",
    "test_df = testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TextNode(id_='578c9d3b-060e-41b5-a7b5-4baced5c31cf', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3'), RelatedNodeInfo(node_id='93a0cdbc-f0e2-46e3-b25a-19df9ccbc4b6', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='6c120932d1c10a41bdc60043096f43742ffc2b6cb5b9c52ef53c52eb8b464172')]}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Alphabet Announces Fourth Quarter and Fiscal Year 2024 Results\\nMOUNTAIN VIEW, Calif. – February 4, 2025 – Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial \\nresults for the quarter and fiscal year ended December 31, 2024.\\n• Consolidated Alphabet revenues in Q4 2024 increased 12% year over year to $96.5 billion reflecting robust \\nmomentum across the business.\\n• Google Services revenues increased 10% to $84.1 billion, reflecting the strong momentum  across Google \\nSearch & other and YouTube ads.\\n• Google Cloud revenues increased 30% to $12.0 billion led by growth in Google Cloud Platform (GCP) across \\ncore GCP products, AI Infrastructure, and Generative AI Solutions.\\n• Total operating income increased 31% and operating margin expanded by 5% percentage points to 32%.\\n• Net income increased 28% and EPS increased 31% to $2.15.\\nSundar Pichai, CEO, said: “Q4 was a strong quarter driven by our leadership in AI and momentum across the \\nbusiness. We are building, testing, and launching products and models faster than ever, and making significant \\nprogress in compute and driving efficiencies. In Search, advances like AI Overviews and Circle to Search are \\nincreasing user engagement. Our AI-powered Google Cloud portfolio is seeing stronger customer demand, and \\nYouTube continues to be the leader in streaming watchtime and podcasts. Together, Cloud and YouTube exited 2024 \\nat an annual revenue run rate of $110 billion. Our results show the power of our differentiated full-stack approach to \\nAI innovation and the continued strength of our core businesses. We are confident about the opportunities ahead, \\nand to accelerate our progress, we expect to invest approximately $75 billion in capital expenditures in 2025.”\\nQ4 2024 Financial Highlights\\nThe following table summarizes our consolidated financial results for the quarter and fiscal year ended December 31, \\n2023 and 2024 (in millions, except for per share information and percentages). \\nQuarter Ended December 31, Year Ended December 31,\\n2023 2024 2023 2024\\n(unaudited) (unaudited)\\nRevenues $ 86,310 $ 96,469 $ 307,394 $ 350,018 \\nChange in revenues year over year  13 %  12 %  9 %  14 %\\nChange in constant currency revenues year over year(1)  13 %  12 %  10 %  15 %\\nOperating income $ 23,697 $ 30,972 $ 84,293 $ 112,390 \\nOperating margin  27 %  32 %  27 %  32 %\\nOther income (expense), net $ 715 $ 1,271 $ 1,424 $ 7,425 \\nNet income $ 20,687 $ 26,536 $ 73,795 $ 100,118 \\nDiluted EPS $ 1.64 $ 2.15 $ 5.80 $ 8.04 \\n(1) Non-GAAP measure. See the section captioned “ Reconciliation from GAAP Revenues to Non-GAAP Constant Currency \\nRevenues and GAAP Percentage Change in Revenues to Non-GAAP Percentage Change in Constant Currency Revenues ” \\nfor more details.\\n1', mimetype='text/plain', start_char_idx=0, end_char_idx=2737, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='578c9d3b-060e-41b5-a7b5-4baced5c31cf', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='9857b72a2252cb4eaa9817f88bb7bae5a8863c374faae94e7c1f3d0c5c10caf9'), <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='03a55faf-ce5a-4452-b7d0-6255d00e6f73', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='b559a0a2dee91ca020cd5ba93f1c7ad2edfc7c38507ffffe52725fcf0730d26c'), RelatedNodeInfo(node_id='20908ade-38f6-42fc-b739-0a60a5604bc5', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='c9e153c0eb485de1913689e5e2939d677309f39354656caa0e2ca64df4339a9c'), RelatedNodeInfo(node_id='086cb411-911e-45e3-b5dd-70719acefb18', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='9360c0de0f52a36698b155359c8a97154a4a3934c8df083260d61c776b8990df'), RelatedNodeInfo(node_id='47db4caf-c026-4f2d-9855-672f4ca5a9eb', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='1453a405557b799893dedacc468f71136599f64949a5d50c79f53150077bf952'), RelatedNodeInfo(node_id='2e23d56a-281c-495e-9489-ba7d653bb97a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='3283ad86fcadc213356bd13b4aa7f868afbf60e2ac411f319df8c457966ff71a'), RelatedNodeInfo(node_id='e77c6b33-8f76-44cd-b270-5c1336ffdb0e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='5d4737e6468b2843b4a680cf21935032c2292e4ef619e386cc3ef307989c5985'), RelatedNodeInfo(node_id='72557630-5ad2-4188-ba17-205e769f38f0', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='ee48eb7554daa71bbd129fa7d48011315877870497e1d44eca5828657f61b589'), RelatedNodeInfo(node_id='19f259b5-f301-463e-8a21-149cf362dea0', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='e77568d9ee278777f37b29ee74f218eba34cc52c2c1b48590a0f1963979d1567'), RelatedNodeInfo(node_id='e6da8248-f9b9-4556-93c3-5b1be9177c39', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='2bb8a774dd56357fd3f9c67918d60f2ee5bb84b54e59197fa1f4adcd606fd428'), RelatedNodeInfo(node_id='75e7ad95-5c71-4ac4-a121-a0f763644d9a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='ab28249fd9d561e4588e30ae41473df27695c5c8ad0efd42a3d816ac97b1b4f4'), RelatedNodeInfo(node_id='af72f5f1-9a31-442b-a252-bd9ca43e5440', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='c77eb794fd94f514d6af33dce94dbe6d711bc33bcb473a132e648de912e46d2d')]}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Alphabet Announces Fourth Quarter and Fiscal Year 2024 Results\\nMOUNTAIN VIEW, Calif. – February 4, 2025 – Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial \\nresults for the quarter and fiscal year ended December 31, 2024.\\n• Consolidated Alphabet revenues in Q4 2024 increased 12% year over year to $96.5 billion reflecting robust \\nmomentum across the business.\\n• Google Services revenues increased 10% to $84.1 billion, reflecting the strong momentum  across Google \\nSearch & other and YouTube ads.\\n• Google Cloud revenues increased 30% to $12.0 billion led by growth in Google Cloud Platform (GCP) across \\ncore GCP products, AI Infrastructure, and Generative AI Solutions.\\n• Total operating income increased 31% and operating margin expanded by 5% percentage points to 32%.\\n• Net income increased 28% and EPS increased 31% to $2.15.\\nSundar Pichai, CEO, said: “Q4 was a strong quarter driven by our leadership in AI and momentum across the \\nbusiness. We are building, testing, and launching products and models faster than ever, and making significant \\nprogress in compute and driving efficiencies. In Search, advances like AI Overviews and Circle to Search are \\nincreasing user engagement. Our AI-powered Google Cloud portfolio is seeing stronger customer demand, and \\nYouTube continues to be the leader in streaming watchtime and podcasts. Together, Cloud and YouTube exited 2024 \\nat an annual revenue run rate of $110 billion. Our results show the power of our differentiated full-stack approach to \\nAI innovation and the continued strength of our core businesses. We are confident about the opportunities ahead, \\nand to accelerate our progress, we expect to invest approximately $75 billion in capital expenditures in 2025.”\\nQ4 2024 Financial Highlights\\nThe following table summarizes our consolidated financial results for the quarter and fiscal year ended December 31, \\n2023 and 2024 (in millions, except for per share information and percentages).', mimetype='text/plain', start_char_idx=0, end_char_idx=1966, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='93a0cdbc-f0e2-46e3-b25a-19df9ccbc4b6', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='578c9d3b-060e-41b5-a7b5-4baced5c31cf', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='9857b72a2252cb4eaa9817f88bb7bae5a8863c374faae94e7c1f3d0c5c10caf9'), <NodeRelationship.CHILD: '5'>: [RelatedNodeInfo(node_id='24c03e49-a35c-4455-a1ed-c4b403042eef', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='48bad602aaa4d3ca3aa698e2e08466b6df01bd256bec99e148f86075be6f97bd'), RelatedNodeInfo(node_id='00fc4148-4872-432e-ab69-9f32110ec396', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d22b8b6d871044034d6009ad2b565e56f28c9e49b90bd4249f7c62ca31d2c88e'), RelatedNodeInfo(node_id='baa1cfd2-af55-4ec3-85dd-4fe0a0242838', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='89b117397d59db0d6503a38f7841c23a82502d2819e597dfa93242dbd62a5806'), RelatedNodeInfo(node_id='4482433e-3f48-47ea-a745-279399369b98', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='f4f597951ea8f0b98c61efc6420421a9035e2bda9e1ee329a48478d4b34ddff1'), RelatedNodeInfo(node_id='f5e1e526-e49e-4821-94aa-bb14cc11d6bc', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d7a90b6b85cdbb9853a50f6b9e131ed3523c30493af828c6fc4f0ef81cda2880'), RelatedNodeInfo(node_id='3a8051e9-da33-4f14-b64a-73cab2b492ce', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='c7b29b0f1bde90a599535ae25abef09dc8299c8e91de50d9792382c823093aee')]}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Quarter Ended December 31, Year Ended December 31,\\n2023 2024 2023 2024\\n(unaudited) (unaudited)\\nRevenues $ 86,310 $ 96,469 $ 307,394 $ 350,018 \\nChange in revenues year over year  13 %  12 %  9 %  14 %\\nChange in constant currency revenues year over year(1)  13 %  12 %  10 %  15 %\\nOperating income $ 23,697 $ 30,972 $ 84,293 $ 112,390 \\nOperating margin  27 %  32 %  27 %  32 %\\nOther income (expense), net $ 715 $ 1,271 $ 1,424 $ 7,425 \\nNet income $ 20,687 $ 26,536 $ 73,795 $ 100,118 \\nDiluted EPS $ 1.64 $ 2.15 $ 5.80 $ 8.04 \\n(1) Non-GAAP measure. See the section captioned “ Reconciliation from GAAP Revenues to Non-GAAP Constant Currency \\nRevenues and GAAP Percentage Change in Revenues to Non-GAAP Percentage Change in Constant Currency Revenues ” \\nfor more details.\\n1', mimetype='text/plain', start_char_idx=1968, end_char_idx=2737, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='03a55faf-ce5a-4452-b7d0-6255d00e6f73', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Alphabet Announces Fourth Quarter and Fiscal Year 2024 Results\\nMOUNTAIN VIEW, Calif. – February 4, 2025 – Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial \\nresults for the quarter and fiscal year ended December 31, 2024.', mimetype='text/plain', start_char_idx=0, end_char_idx=233, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='20908ade-38f6-42fc-b739-0a60a5604bc5', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='03a55faf-ce5a-4452-b7d0-6255d00e6f73', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='b559a0a2dee91ca020cd5ba93f1c7ad2edfc7c38507ffffe52725fcf0730d26c'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='– February 4, 2025 – Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial \\nresults for the quarter and fiscal year ended December 31, 2024.\\n• Consolidated Alphabet revenues in Q4 2024 increased 12% year over year to $96.5 billion reflecting robust \\nmomentum across the business.', mimetype='text/plain', start_char_idx=85, end_char_idx=372, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='086cb411-911e-45e3-b5dd-70719acefb18', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='20908ade-38f6-42fc-b739-0a60a5604bc5', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='c9e153c0eb485de1913689e5e2939d677309f39354656caa0e2ca64df4339a9c'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='• Consolidated Alphabet revenues in Q4 2024 increased 12% year over year to $96.5 billion reflecting robust \\nmomentum across the business.\\n• Google Services revenues increased 10% to $84.1 billion, reflecting the strong momentum  across Google \\nSearch & other and YouTube ads.', mimetype='text/plain', start_char_idx=234, end_char_idx=510, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='47db4caf-c026-4f2d-9855-672f4ca5a9eb', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='086cb411-911e-45e3-b5dd-70719acefb18', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='9360c0de0f52a36698b155359c8a97154a4a3934c8df083260d61c776b8990df'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='• Google Services revenues increased 10% to $84.1 billion, reflecting the strong momentum  across Google \\nSearch & other and YouTube ads.\\n• Google Cloud revenues increased 30% to $12.0 billion led by growth in Google Cloud Platform (GCP) across \\ncore GCP products, AI Infrastructure, and Generative AI Solutions.', mimetype='text/plain', start_char_idx=373, end_char_idx=685, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='2e23d56a-281c-495e-9489-ba7d653bb97a', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='47db4caf-c026-4f2d-9855-672f4ca5a9eb', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='1453a405557b799893dedacc468f71136599f64949a5d50c79f53150077bf952'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='• Google Cloud revenues increased 30% to $12.0 billion led by growth in Google Cloud Platform (GCP) across \\ncore GCP products, AI Infrastructure, and Generative AI Solutions.\\n• Total operating income increased 31% and operating margin expanded by 5% percentage points to 32%.\\n• Net income increased 28% and EPS increased 31% to $2.15.', mimetype='text/plain', start_char_idx=511, end_char_idx=845, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='e77c6b33-8f76-44cd-b270-5c1336ffdb0e', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='2e23d56a-281c-495e-9489-ba7d653bb97a', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='3283ad86fcadc213356bd13b4aa7f868afbf60e2ac411f319df8c457966ff71a'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='• Total operating income increased 31% and operating margin expanded by 5% percentage points to 32%.\\n• Net income increased 28% and EPS increased 31% to $2.15.\\nSundar Pichai, CEO, said: “Q4 was a strong quarter driven by our leadership in AI and momentum across the \\nbusiness.', mimetype='text/plain', start_char_idx=686, end_char_idx=962, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}'),\n",
       " TextNode(id_='72557630-5ad2-4188-ba17-205e769f38f0', embedding=None, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, excluded_embed_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], excluded_llm_metadata_keys=['file_name', 'file_type', 'file_size', 'creation_date', 'last_modified_date', 'last_accessed_date'], relationships={<NodeRelationship.SOURCE: '1'>: RelatedNodeInfo(node_id='f2ed16ed-8b55-46de-8ddd-86b1d4c4bb1d', node_type=<ObjectType.DOCUMENT: '4'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='707a822e587bef43fed652e10863acb35d7592ee62acd84684c977bf5ec21142'), <NodeRelationship.PREVIOUS: '2'>: RelatedNodeInfo(node_id='e77c6b33-8f76-44cd-b270-5c1336ffdb0e', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='5d4737e6468b2843b4a680cf21935032c2292e4ef619e386cc3ef307989c5985'), <NodeRelationship.PARENT: '4'>: RelatedNodeInfo(node_id='92b5ff4f-ae75-4e8f-9560-76000c0f74ee', node_type=<ObjectType.TEXT: '1'>, metadata={'page_label': '1', 'file_name': '2024q4-alphabet-earnings-release.pdf', 'file_path': '/Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf', 'file_type': 'application/pdf', 'file_size': 148726, 'creation_date': '2025-02-19', 'last_modified_date': '2025-02-12'}, hash='d192593370408e998b7cb9a748551d14f2ca1bb63594db3edc632cdd8dc9e4a3')}, metadata_template='{key}: {value}', metadata_separator='\\n', text='Sundar Pichai, CEO, said: “Q4 was a strong quarter driven by our leadership in AI and momentum across the \\nbusiness. We are building, testing, and launching products and models faster than ever, and making significant \\nprogress in compute and driving efficiencies. In Search, advances like AI Overviews and Circle to Search are \\nincreasing user engagement.', mimetype='text/plain', start_char_idx=846, end_char_idx=1202, metadata_seperator='\\n', text_template='{metadata_str}\\n\\n{content}')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing collection 'financial_reports_2'\n",
      "Successfully connected to collection 'financial_reports_2'\n",
      "Loaded 99 documents\n",
      "Created 207 semantic nodes from 110 base nodes\n",
      "\n",
      "=== Semantic Splitting Statistics ===\n",
      "Before: 110 base nodes\n",
      "After:  207 semantic nodes\n",
      "Change: 97 additional nodes created\n",
      "\n",
      "Avg length before: 2225.07 chars\n",
      "Avg length after:  1182.41 chars\n",
      "Max length before: 5237\n",
      "Max length after:  5188\n",
      "\n",
      "=== Sample Semantic Node ===\n",
      "Text preview: Alphabet Announces Fourth Quarter and Fiscal Year 2024 Results\n",
      "MOUNTAIN VIEW, Calif. – February 4, 2...\n",
      "\n",
      "Metadata:\n",
      "  page_label: 1\n",
      "  file_name: 2024q4-alphabet-earnings-release.pdf\n",
      "  file_path: /Users/danielmak/Documents/local_RAG/Document_Personal/2024q4-alphabet-earnings-release.pdf\n",
      "  file_type: application/pdf\n",
      "  file_size: 148726\n",
      "  creation_date: 2025-02-19\n",
      "  last_modified_date: 2025-02-12\n",
      "Stored 207 semantic document chunks in collection 'financial_reports_2'\n"
     ]
    }
   ],
   "source": [
    "# Initialize manager\n",
    "collection_manager = ChromaCollectionManager(COLLECTION_NAME)\n",
    "\n",
    "# Get or create collection (with optional deletion of existing)\n",
    "chroma_collection = collection_manager.get_or_create_collection(delete_existing=DELETE_FLAG)\n",
    "\n",
    "# Add documents if needed\n",
    "# if DELETE_FLAG or chroma_collection.count() == 0:\n",
    "#     collection_manager.add_documents(\n",
    "#         collection=chroma_collection,\n",
    "#         documents_path=DOCUMENT_PATH_PERSONAL\n",
    "#     )\n",
    "\n",
    "if DELETE_FLAG or chroma_collection.count() == 0:\n",
    "    collection_manager.add_documents_semantic(\n",
    "        collection=chroma_collection,\n",
    "        documents_path=DOCUMENT_PATH_PERSONAL,\n",
    "        chunk_size=1024,\n",
    "        chunk_overlap=50,\n",
    "        debug_mode=True\n",
    "    )\n",
    "\n",
    "# Set up vector store\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load From Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/bm25s/__init__.py:302: RuntimeWarning: Mean of empty slice.\n",
      "  avg_doc_len = np.array([len(doc_ids) for doc_ids in corpus_token_ids]).mean()\n",
      "/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 17\u001b[0m\n\u001b[1;32m     12\u001b[0m vector_retriever \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_retriever(\n\u001b[1;32m     13\u001b[0m     similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     14\u001b[0m )\n\u001b[1;32m     16\u001b[0m \u001b[38;5;66;03m# 2. BM25 Retriever - uses docstore\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m bm25_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mBM25Retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Uses the docstore from your index\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\n\u001b[1;32m     20\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Hybrid fusion using both retrievers\u001b[39;00m\n\u001b[1;32m     23\u001b[0m fusion_retriever \u001b[38;5;241m=\u001b[39m HybridFusionRetriever(\n\u001b[1;32m     24\u001b[0m     [vector_retriever, bm25_retriever],\n\u001b[1;32m     25\u001b[0m     similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/llama_index/retrievers/bm25/base.py:142\u001b[0m, in \u001b[0;36mBM25Retriever.from_defaults\u001b[0;34m(cls, index, nodes, docstore, stemmer, language, similarity_top_k, verbose, skip_stemming, token_pattern, tokenizer)\u001b[0m\n\u001b[1;32m    136\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m cast(List[BaseNode], \u001b[38;5;28mlist\u001b[39m(docstore\u001b[38;5;241m.\u001b[39mdocs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m     nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass exactly one of index, nodes, or docstore.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstemmer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilarity_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_stemming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_stemming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/llama_index/retrievers/bm25/base.py:99\u001b[0m, in \u001b[0;36mBM25Retriever.__init__\u001b[0;34m(self, nodes, stemmer, language, existing_bm25, similarity_top_k, callback_manager, objects, object_map, verbose, skip_stemming, token_pattern)\u001b[0m\n\u001b[1;32m     91\u001b[0m     corpus_tokens \u001b[38;5;241m=\u001b[39m bm25s\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[1;32m     92\u001b[0m         [node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes],\n\u001b[1;32m     93\u001b[0m         stopwords\u001b[38;5;241m=\u001b[39mlanguage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm25 \u001b[38;5;241m=\u001b[39m bm25s\u001b[38;5;241m.\u001b[39mBM25()\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    101\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m    102\u001b[0m     object_map\u001b[38;5;241m=\u001b[39mobject_map,\n\u001b[1;32m    103\u001b[0m     objects\u001b[38;5;241m=\u001b[39mobjects,\n\u001b[1;32m    104\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    105\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/bm25s/__init__.py:485\u001b[0m, in \u001b[0;36mBM25.index\u001b[0;34m(self, corpus, create_empty_token, show_progress, leave_progress)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m vocab_dict:\n\u001b[0;32m--> 485\u001b[0m         vocab_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m scores\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_dict \u001b[38;5;241m=\u001b[39m vocab_dict\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"financial_reports_2\")\n",
    "#chroma_collection = chroma_client.get_or_create_collection(\"datalab_demo\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context\n",
    ")\n",
    "# 1. Vector Retriever - uses vector store\n",
    "vector_retriever = index.as_retriever(\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "# 2. BM25 Retriever - uses docstore\n",
    "bm25_retriever = BM25Retriever.from_defaults(\n",
    "    docstore=index.docstore,  # Uses the docstore from your index\n",
    "    similarity_top_k=5\n",
    ")\n",
    "\n",
    "# Hybrid fusion using both retrievers\n",
    "fusion_retriever = HybridFusionRetriever(\n",
    "    [vector_retriever, bm25_retriever],\n",
    "    similarity_top_k=5\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Rewrite the user's follow-up question as a standalone question.\n",
    "\n",
    "1. Include all relevant past context.\n",
    "2. Keep it natural and grammatically correct.\n",
    "3. If already standalone, return it unchanged.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<User's Follow-Up Question>\n",
    "{question}\n",
    "\n",
    "<Rewritten Standalone Question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "response_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "You are an AI assistant providing structured responses.\n",
    "\n",
    "### **Instructions:**\n",
    "- Answer clearly and concisely.\n",
    "- Summarize retrieved context to avoid duplication.\n",
    "- Summarize the key facts efficiently.\n",
    "- If the context lacks enough details, say: \"I don’t have enough information.\"\n",
    "- Format responses in natural sentences.\n",
    "\n",
    "<Retrieved Context>\n",
    "{context}\n",
    "\n",
    "<User's Query>\n",
    "{question}\n",
    "\n",
    "### **AI Response:**\n",
    "\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of documents in docstore: 0\n"
     ]
    }
   ],
   "source": [
    "# 1. First, make sure your index has documents\n",
    "print(f\"Number of documents in docstore: {len(index.docstore.docs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/bm25s/__init__.py:302: RuntimeWarning: Mean of empty slice.\n",
      "  avg_doc_len = np.array([len(doc_ids) for doc_ids in corpus_token_ids]).mean()\n",
      "/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/numpy/_core/_methods.py:145: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "max() arg is an empty sequence",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 7\u001b[0m\n\u001b[1;32m      2\u001b[0m vector_retriever \u001b[38;5;241m=\u001b[39m index\u001b[38;5;241m.\u001b[39mas_retriever(\n\u001b[1;32m      3\u001b[0m     similarity_top_k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,  \u001b[38;5;66;03m# Retrieve more candidates for fusion\u001b[39;00m\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# 2. Create a BM25 retriever (sparse keyword-based retrieval)\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m bm25_retriever \u001b[38;5;241m=\u001b[39m \u001b[43mBM25Retriever\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_defaults\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocstore\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocstore\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use the same document store\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Match the vector retriever\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/llama_index/retrievers/bm25/base.py:142\u001b[0m, in \u001b[0;36mBM25Retriever.from_defaults\u001b[0;34m(cls, index, nodes, docstore, stemmer, language, similarity_top_k, verbose, skip_stemming, token_pattern, tokenizer)\u001b[0m\n\u001b[1;32m    136\u001b[0m     nodes \u001b[38;5;241m=\u001b[39m cast(List[BaseNode], \u001b[38;5;28mlist\u001b[39m(docstore\u001b[38;5;241m.\u001b[39mdocs\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[1;32m    139\u001b[0m     nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    140\u001b[0m ), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease pass exactly one of index, nodes, or docstore.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstemmer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstemmer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlanguage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    146\u001b[0m \u001b[43m    \u001b[49m\u001b[43msimilarity_top_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msimilarity_top_k\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    148\u001b[0m \u001b[43m    \u001b[49m\u001b[43mskip_stemming\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_stemming\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtoken_pattern\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtoken_pattern\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/llama_index/retrievers/bm25/base.py:99\u001b[0m, in \u001b[0;36mBM25Retriever.__init__\u001b[0;34m(self, nodes, stemmer, language, existing_bm25, similarity_top_k, callback_manager, objects, object_map, verbose, skip_stemming, token_pattern)\u001b[0m\n\u001b[1;32m     91\u001b[0m     corpus_tokens \u001b[38;5;241m=\u001b[39m bm25s\u001b[38;5;241m.\u001b[39mtokenize(\n\u001b[1;32m     92\u001b[0m         [node\u001b[38;5;241m.\u001b[39mget_content(metadata_mode\u001b[38;5;241m=\u001b[39mMetadataMode\u001b[38;5;241m.\u001b[39mEMBED) \u001b[38;5;28;01mfor\u001b[39;00m node \u001b[38;5;129;01min\u001b[39;00m nodes],\n\u001b[1;32m     93\u001b[0m         stopwords\u001b[38;5;241m=\u001b[39mlanguage,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     96\u001b[0m         show_progress\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m     97\u001b[0m     )\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbm25 \u001b[38;5;241m=\u001b[39m bm25s\u001b[38;5;241m.\u001b[39mBM25()\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbm25\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorpus_tokens\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    101\u001b[0m     callback_manager\u001b[38;5;241m=\u001b[39mcallback_manager,\n\u001b[1;32m    102\u001b[0m     object_map\u001b[38;5;241m=\u001b[39mobject_map,\n\u001b[1;32m    103\u001b[0m     objects\u001b[38;5;241m=\u001b[39mobjects,\n\u001b[1;32m    104\u001b[0m     verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    105\u001b[0m )\n",
      "File \u001b[0;32m/opt/miniconda3/envs/local_RAG/lib/python3.10/site-packages/bm25s/__init__.py:485\u001b[0m, in \u001b[0;36mBM25.index\u001b[0;34m(self, corpus, create_empty_token, show_progress, leave_progress)\u001b[0m\n\u001b[1;32m    482\u001b[0m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m vocab_dict:\n\u001b[0;32m--> 485\u001b[0m         vocab_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mvocab_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscores \u001b[38;5;241m=\u001b[39m scores\n\u001b[1;32m    488\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_dict \u001b[38;5;241m=\u001b[39m vocab_dict\n",
      "\u001b[0;31mValueError\u001b[0m: max() arg is an empty sequence"
     ]
    }
   ],
   "source": [
    "# 2. Create a query engine with the fusion retriever\n",
    "fusion_query_engine = RetrieverQueryEngine.from_args(\n",
    "    retriever=fusion_retriever,\n",
    "    response_synthesizer=index.as_query_engine().response_synthesizer,\n",
    "    response_mode=\"compact\",\n",
    "    response_prompt=response_prompt,\n",
    "    max_tokens=MAX_TOKENS_GENERATE,\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "# 3. Create a chat engine with the fusion query engine\n",
    "fusion_chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=fusion_query_engine,\n",
    "    memory=memory,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"compact\",\n",
    "    response_prompt=response_prompt,\n",
    "    similarity_top_k=TOP_K,\n",
    "    max_tokens = MAX_TOKENS_GENERATE,\n",
    "    streaming=False\n",
    ")\n",
    "memory = ChatMemoryBuffer.from_defaults(token_limit=MEMORY_LENGTH)\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    memory=memory,\n",
    "    #condense_question_prompt=custom_prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "# print(\"✅ Chat engine initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "The context information suggests that the document is related to a company's earnings presentation for the fourth quarter of 2024. The presentation includes financial metrics such as revenue and financial facts sheet. The company uses non-GAAP financial measures like free cash flow, constant currency revenues, and percentage change in constant currency revenues to supplement their consolidated financial statements prepared in accordance with GAAP. The non-GAAP financial measures are used for financial and operational decision-making and to evaluate period-to-period comparisons. The company believes that these non-GAAP financial measures provide meaningful supplemental information and are useful to investors. However, it is important to note that these non-GAAP financial measures are not intended to be considered in isolation or as a substitute for GAAP financial information. The limitations of using non-GAAP financial measures are compensated by providing specific information regarding the GAAP amounts excluded from these non-GAAP financial measures and evaluating these non-GAAP financial measures together with their relevant financial measures in accordance with GAAP. For more information on these non-GAAP financial measures, please refer to the sections captioned \"Reconciliation from GAAP Net Cash Provided by Operating Activities to Non-GAAP Free Cash Flow\" and \"Reconciliation from GAAP Revenues to Non-GAAP Constant Currency Revenues and GAAP Percentage Change in Revenues to Non-GAAP Percentage Change in Constant Currency Revenues\" included at the end of the release.\n",
      "\n",
      "⏳ Response Time: 30.09 seconds\n"
     ]
    }
   ],
   "source": [
    "question = \"What insights can you share about the financial performance?\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "response = chat_engine.chat(question)\n",
    "end_time = time.time()\n",
    "\n",
    "print(response)\n",
    "print(f\"\\n⏳ Response Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 AI Response:\n",
      "\n",
      "\n",
      "The financial performance of the company's cloud business is strong, as evidenced by a 30% increase in revenues to $12.0 billion in Q4 2024 compared to the previous year. This growth is attributed to growth in Google Cloud Platform (GCP) across core GCP products, AI Infrastructure, and Generative AI Solutions.\n",
      "\n",
      "⏳ Response Time: 22.00 seconds\n"
     ]
    }
   ],
   "source": [
    "question = \"How is the cloud business doing?\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "response = chat_engine.chat(question)\n",
    "end_time = time.time()\n",
    "print(\"\\n🧠 AI Response:\\n\")\n",
    "print(response)\n",
    "print(f\"\\n⏳ Response Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Any developments for the upcoming year in these areas?\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "response = chat_engine.chat(question)\n",
    "end_time = time.time()\n",
    "print(\"\\n🧠 AI Response:\\n\")\n",
    "print(response)\n",
    "print(f\"\\n⏳ Response Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "memory_out = memory.to_string()\n",
    "formatted_json = json.loads(memory_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(formatted_json, indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "from llama_index.core import VectorStoreIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic RAG pipeline, uses service context defaults\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# generate prediction dataset\n",
    "prediction_dataset = await rag_dataset.amake_predictions_with(\n",
    "    query_engine=query_engine, show_progress=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
