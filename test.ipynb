{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'llama_cpp' has no attribute 'llama_backend'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mllama_cpp\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mllama_cpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllama_backend\u001b[49m()) \n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'llama_cpp' has no attribute 'llama_backend'"
     ]
    }
   ],
   "source": [
    "import llama_cpp\n",
    "print(llama_cpp.llama_backend()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from config import DOCUMENT_PATH, EMBEDDING_MODEL_NAME, EMBEDDING_MODEL_PATH, TOP_K\n",
    "from llm_loader import load_llm\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_load_from_file_impl: using device Metal (Apple M3) - 10922 MiB free\n",
      "llama_model_loader: loaded meta data with 33 key-value pairs and 292 tensors from ./quantized_model/Llama-3.1-8B-Instruct-Q5KM.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Llama 3.1 8B Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = Instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Llama-3.1\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = 8B\n",
      "llama_model_loader: - kv   6:                            general.license str              = llama3.1\n",
      "llama_model_loader: - kv   7:                   general.base_model.count u32              = 1\n",
      "llama_model_loader: - kv   8:                  general.base_model.0.name str              = Meta Llama 3.1 8B\n",
      "llama_model_loader: - kv   9:          general.base_model.0.organization str              = Meta Llama\n",
      "llama_model_loader: - kv  10:              general.base_model.0.repo_url str              = https://huggingface.co/meta-llama/Met...\n",
      "llama_model_loader: - kv  11:                               general.tags arr[str,6]       = [\"facebook\", \"meta\", \"pytorch\", \"llam...\n",
      "llama_model_loader: - kv  12:                          general.languages arr[str,8]       = [\"en\", \"de\", \"fr\", \"it\", \"pt\", \"hi\", ...\n",
      "llama_model_loader: - kv  13:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv  14:                       llama.context_length u32              = 131072\n",
      "llama_model_loader: - kv  15:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv  16:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv  17:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  18:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv  19:                       llama.rope.freq_base f32              = 500000.000000\n",
      "llama_model_loader: - kv  20:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  21:                           llama.vocab_size u32              = 128256\n",
      "llama_model_loader: - kv  22:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = gpt2\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = llama-bpe\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n",
      "llama_model_loader: - kv  26:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n",
      "llama_model_loader: - kv  27:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 128000\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 128009\n",
      "llama_model_loader: - kv  30:                    tokenizer.chat_template str              = {{- bos_token }}\\n{%- if custom_tools ...\n",
      "llama_model_loader: - kv  31:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - kv  32:                          general.file_type u32              = 17\n",
      "llama_model_loader: - type  f32:   66 tensors\n",
      "llama_model_loader: - type q5_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q5_K - Medium\n",
      "print_info: file size   = 5.33 GiB (5.70 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 2\n",
      "load: control token: 128254 '<|reserved_special_token_246|>' is not marked as EOG\n",
      "load: control token: 128252 '<|reserved_special_token_244|>' is not marked as EOG\n",
      "load: control token: 128251 '<|reserved_special_token_243|>' is not marked as EOG\n",
      "load: control token: 128250 '<|reserved_special_token_242|>' is not marked as EOG\n",
      "load: control token: 128248 '<|reserved_special_token_240|>' is not marked as EOG\n",
      "load: control token: 128247 '<|reserved_special_token_239|>' is not marked as EOG\n",
      "load: control token: 128245 '<|reserved_special_token_237|>' is not marked as EOG\n",
      "load: control token: 128244 '<|reserved_special_token_236|>' is not marked as EOG\n",
      "load: control token: 128243 '<|reserved_special_token_235|>' is not marked as EOG\n",
      "load: control token: 128240 '<|reserved_special_token_232|>' is not marked as EOG\n",
      "load: control token: 128238 '<|reserved_special_token_230|>' is not marked as EOG\n",
      "load: control token: 128235 '<|reserved_special_token_227|>' is not marked as EOG\n",
      "load: control token: 128234 '<|reserved_special_token_226|>' is not marked as EOG\n",
      "load: control token: 128229 '<|reserved_special_token_221|>' is not marked as EOG\n",
      "load: control token: 128227 '<|reserved_special_token_219|>' is not marked as EOG\n",
      "load: control token: 128226 '<|reserved_special_token_218|>' is not marked as EOG\n",
      "load: control token: 128224 '<|reserved_special_token_216|>' is not marked as EOG\n",
      "load: control token: 128223 '<|reserved_special_token_215|>' is not marked as EOG\n",
      "load: control token: 128221 '<|reserved_special_token_213|>' is not marked as EOG\n",
      "load: control token: 128219 '<|reserved_special_token_211|>' is not marked as EOG\n",
      "load: control token: 128218 '<|reserved_special_token_210|>' is not marked as EOG\n",
      "load: control token: 128217 '<|reserved_special_token_209|>' is not marked as EOG\n",
      "load: control token: 128216 '<|reserved_special_token_208|>' is not marked as EOG\n",
      "load: control token: 128215 '<|reserved_special_token_207|>' is not marked as EOG\n",
      "load: control token: 128213 '<|reserved_special_token_205|>' is not marked as EOG\n",
      "load: control token: 128211 '<|reserved_special_token_203|>' is not marked as EOG\n",
      "load: control token: 128210 '<|reserved_special_token_202|>' is not marked as EOG\n",
      "load: control token: 128209 '<|reserved_special_token_201|>' is not marked as EOG\n",
      "load: control token: 128208 '<|reserved_special_token_200|>' is not marked as EOG\n",
      "load: control token: 128207 '<|reserved_special_token_199|>' is not marked as EOG\n",
      "load: control token: 128204 '<|reserved_special_token_196|>' is not marked as EOG\n",
      "load: control token: 128202 '<|reserved_special_token_194|>' is not marked as EOG\n",
      "load: control token: 128197 '<|reserved_special_token_189|>' is not marked as EOG\n",
      "load: control token: 128195 '<|reserved_special_token_187|>' is not marked as EOG\n",
      "load: control token: 128194 '<|reserved_special_token_186|>' is not marked as EOG\n",
      "load: control token: 128191 '<|reserved_special_token_183|>' is not marked as EOG\n",
      "load: control token: 128190 '<|reserved_special_token_182|>' is not marked as EOG\n",
      "load: control token: 128188 '<|reserved_special_token_180|>' is not marked as EOG\n",
      "load: control token: 128187 '<|reserved_special_token_179|>' is not marked as EOG\n",
      "load: control token: 128185 '<|reserved_special_token_177|>' is not marked as EOG\n",
      "load: control token: 128184 '<|reserved_special_token_176|>' is not marked as EOG\n",
      "load: control token: 128183 '<|reserved_special_token_175|>' is not marked as EOG\n",
      "load: control token: 128178 '<|reserved_special_token_170|>' is not marked as EOG\n",
      "load: control token: 128177 '<|reserved_special_token_169|>' is not marked as EOG\n",
      "load: control token: 128176 '<|reserved_special_token_168|>' is not marked as EOG\n",
      "load: control token: 128175 '<|reserved_special_token_167|>' is not marked as EOG\n",
      "load: control token: 128174 '<|reserved_special_token_166|>' is not marked as EOG\n",
      "load: control token: 128173 '<|reserved_special_token_165|>' is not marked as EOG\n",
      "load: control token: 128172 '<|reserved_special_token_164|>' is not marked as EOG\n",
      "load: control token: 128169 '<|reserved_special_token_161|>' is not marked as EOG\n",
      "load: control token: 128167 '<|reserved_special_token_159|>' is not marked as EOG\n",
      "load: control token: 128166 '<|reserved_special_token_158|>' is not marked as EOG\n",
      "load: control token: 128160 '<|reserved_special_token_152|>' is not marked as EOG\n",
      "load: control token: 128159 '<|reserved_special_token_151|>' is not marked as EOG\n",
      "load: control token: 128157 '<|reserved_special_token_149|>' is not marked as EOG\n",
      "load: control token: 128156 '<|reserved_special_token_148|>' is not marked as EOG\n",
      "load: control token: 128154 '<|reserved_special_token_146|>' is not marked as EOG\n",
      "load: control token: 128152 '<|reserved_special_token_144|>' is not marked as EOG\n",
      "load: control token: 128151 '<|reserved_special_token_143|>' is not marked as EOG\n",
      "load: control token: 128150 '<|reserved_special_token_142|>' is not marked as EOG\n",
      "load: control token: 128147 '<|reserved_special_token_139|>' is not marked as EOG\n",
      "load: control token: 128144 '<|reserved_special_token_136|>' is not marked as EOG\n",
      "load: control token: 128142 '<|reserved_special_token_134|>' is not marked as EOG\n",
      "load: control token: 128141 '<|reserved_special_token_133|>' is not marked as EOG\n",
      "load: control token: 128140 '<|reserved_special_token_132|>' is not marked as EOG\n",
      "load: control token: 128133 '<|reserved_special_token_125|>' is not marked as EOG\n",
      "load: control token: 128130 '<|reserved_special_token_122|>' is not marked as EOG\n",
      "load: control token: 128128 '<|reserved_special_token_120|>' is not marked as EOG\n",
      "load: control token: 128127 '<|reserved_special_token_119|>' is not marked as EOG\n",
      "load: control token: 128126 '<|reserved_special_token_118|>' is not marked as EOG\n",
      "load: control token: 128125 '<|reserved_special_token_117|>' is not marked as EOG\n",
      "load: control token: 128124 '<|reserved_special_token_116|>' is not marked as EOG\n",
      "load: control token: 128123 '<|reserved_special_token_115|>' is not marked as EOG\n",
      "load: control token: 128122 '<|reserved_special_token_114|>' is not marked as EOG\n",
      "load: control token: 128121 '<|reserved_special_token_113|>' is not marked as EOG\n",
      "load: control token: 128120 '<|reserved_special_token_112|>' is not marked as EOG\n",
      "load: control token: 128119 '<|reserved_special_token_111|>' is not marked as EOG\n",
      "load: control token: 128116 '<|reserved_special_token_108|>' is not marked as EOG\n",
      "load: control token: 128115 '<|reserved_special_token_107|>' is not marked as EOG\n",
      "load: control token: 128114 '<|reserved_special_token_106|>' is not marked as EOG\n",
      "load: control token: 128113 '<|reserved_special_token_105|>' is not marked as EOG\n",
      "load: control token: 128111 '<|reserved_special_token_103|>' is not marked as EOG\n",
      "load: control token: 128110 '<|reserved_special_token_102|>' is not marked as EOG\n",
      "load: control token: 128107 '<|reserved_special_token_99|>' is not marked as EOG\n",
      "load: control token: 128106 '<|reserved_special_token_98|>' is not marked as EOG\n",
      "load: control token: 128105 '<|reserved_special_token_97|>' is not marked as EOG\n",
      "load: control token: 128104 '<|reserved_special_token_96|>' is not marked as EOG\n",
      "load: control token: 128103 '<|reserved_special_token_95|>' is not marked as EOG\n",
      "load: control token: 128100 '<|reserved_special_token_92|>' is not marked as EOG\n",
      "load: control token: 128097 '<|reserved_special_token_89|>' is not marked as EOG\n",
      "load: control token: 128096 '<|reserved_special_token_88|>' is not marked as EOG\n",
      "load: control token: 128094 '<|reserved_special_token_86|>' is not marked as EOG\n",
      "load: control token: 128093 '<|reserved_special_token_85|>' is not marked as EOG\n",
      "load: control token: 128090 '<|reserved_special_token_82|>' is not marked as EOG\n",
      "load: control token: 128089 '<|reserved_special_token_81|>' is not marked as EOG\n",
      "load: control token: 128087 '<|reserved_special_token_79|>' is not marked as EOG\n",
      "load: control token: 128085 '<|reserved_special_token_77|>' is not marked as EOG\n",
      "load: control token: 128080 '<|reserved_special_token_72|>' is not marked as EOG\n",
      "load: control token: 128077 '<|reserved_special_token_69|>' is not marked as EOG\n",
      "load: control token: 128076 '<|reserved_special_token_68|>' is not marked as EOG\n",
      "load: control token: 128073 '<|reserved_special_token_65|>' is not marked as EOG\n",
      "load: control token: 128070 '<|reserved_special_token_62|>' is not marked as EOG\n",
      "load: control token: 128069 '<|reserved_special_token_61|>' is not marked as EOG\n",
      "load: control token: 128067 '<|reserved_special_token_59|>' is not marked as EOG\n",
      "load: control token: 128064 '<|reserved_special_token_56|>' is not marked as EOG\n",
      "load: control token: 128062 '<|reserved_special_token_54|>' is not marked as EOG\n",
      "load: control token: 128061 '<|reserved_special_token_53|>' is not marked as EOG\n",
      "load: control token: 128060 '<|reserved_special_token_52|>' is not marked as EOG\n",
      "load: control token: 128054 '<|reserved_special_token_46|>' is not marked as EOG\n",
      "load: control token: 128045 '<|reserved_special_token_37|>' is not marked as EOG\n",
      "load: control token: 128044 '<|reserved_special_token_36|>' is not marked as EOG\n",
      "load: control token: 128043 '<|reserved_special_token_35|>' is not marked as EOG\n",
      "load: control token: 128042 '<|reserved_special_token_34|>' is not marked as EOG\n",
      "load: control token: 128038 '<|reserved_special_token_30|>' is not marked as EOG\n",
      "load: control token: 128037 '<|reserved_special_token_29|>' is not marked as EOG\n",
      "load: control token: 128035 '<|reserved_special_token_27|>' is not marked as EOG\n",
      "load: control token: 128034 '<|reserved_special_token_26|>' is not marked as EOG\n",
      "load: control token: 128033 '<|reserved_special_token_25|>' is not marked as EOG\n",
      "load: control token: 128032 '<|reserved_special_token_24|>' is not marked as EOG\n",
      "load: control token: 128030 '<|reserved_special_token_22|>' is not marked as EOG\n",
      "load: control token: 128029 '<|reserved_special_token_21|>' is not marked as EOG\n",
      "load: control token: 128028 '<|reserved_special_token_20|>' is not marked as EOG\n",
      "load: control token: 128026 '<|reserved_special_token_18|>' is not marked as EOG\n",
      "load: control token: 128025 '<|reserved_special_token_17|>' is not marked as EOG\n",
      "load: control token: 128024 '<|reserved_special_token_16|>' is not marked as EOG\n",
      "load: control token: 128022 '<|reserved_special_token_14|>' is not marked as EOG\n",
      "load: control token: 128020 '<|reserved_special_token_12|>' is not marked as EOG\n",
      "load: control token: 128017 '<|reserved_special_token_9|>' is not marked as EOG\n",
      "load: control token: 128016 '<|reserved_special_token_8|>' is not marked as EOG\n",
      "load: control token: 128015 '<|reserved_special_token_7|>' is not marked as EOG\n",
      "load: control token: 128014 '<|reserved_special_token_6|>' is not marked as EOG\n",
      "load: control token: 128013 '<|reserved_special_token_5|>' is not marked as EOG\n",
      "load: control token: 128011 '<|reserved_special_token_3|>' is not marked as EOG\n",
      "load: control token: 128010 '<|python_tag|>' is not marked as EOG\n",
      "load: control token: 128006 '<|start_header_id|>' is not marked as EOG\n",
      "load: control token: 128003 '<|reserved_special_token_1|>' is not marked as EOG\n",
      "load: control token: 128002 '<|reserved_special_token_0|>' is not marked as EOG\n",
      "load: control token: 128000 '<|begin_of_text|>' is not marked as EOG\n",
      "load: control token: 128041 '<|reserved_special_token_33|>' is not marked as EOG\n",
      "load: control token: 128063 '<|reserved_special_token_55|>' is not marked as EOG\n",
      "load: control token: 128046 '<|reserved_special_token_38|>' is not marked as EOG\n",
      "load: control token: 128007 '<|end_header_id|>' is not marked as EOG\n",
      "load: control token: 128065 '<|reserved_special_token_57|>' is not marked as EOG\n",
      "load: control token: 128171 '<|reserved_special_token_163|>' is not marked as EOG\n",
      "load: control token: 128162 '<|reserved_special_token_154|>' is not marked as EOG\n",
      "load: control token: 128165 '<|reserved_special_token_157|>' is not marked as EOG\n",
      "load: control token: 128057 '<|reserved_special_token_49|>' is not marked as EOG\n",
      "load: control token: 128050 '<|reserved_special_token_42|>' is not marked as EOG\n",
      "load: control token: 128056 '<|reserved_special_token_48|>' is not marked as EOG\n",
      "load: control token: 128230 '<|reserved_special_token_222|>' is not marked as EOG\n",
      "load: control token: 128098 '<|reserved_special_token_90|>' is not marked as EOG\n",
      "load: control token: 128153 '<|reserved_special_token_145|>' is not marked as EOG\n",
      "load: control token: 128084 '<|reserved_special_token_76|>' is not marked as EOG\n",
      "load: control token: 128082 '<|reserved_special_token_74|>' is not marked as EOG\n",
      "load: control token: 128102 '<|reserved_special_token_94|>' is not marked as EOG\n",
      "load: control token: 128253 '<|reserved_special_token_245|>' is not marked as EOG\n",
      "load: control token: 128179 '<|reserved_special_token_171|>' is not marked as EOG\n",
      "load: control token: 128071 '<|reserved_special_token_63|>' is not marked as EOG\n",
      "load: control token: 128135 '<|reserved_special_token_127|>' is not marked as EOG\n",
      "load: control token: 128161 '<|reserved_special_token_153|>' is not marked as EOG\n",
      "load: control token: 128164 '<|reserved_special_token_156|>' is not marked as EOG\n",
      "load: control token: 128134 '<|reserved_special_token_126|>' is not marked as EOG\n",
      "load: control token: 128249 '<|reserved_special_token_241|>' is not marked as EOG\n",
      "load: control token: 128004 '<|finetune_right_pad_id|>' is not marked as EOG\n",
      "load: control token: 128036 '<|reserved_special_token_28|>' is not marked as EOG\n",
      "load: control token: 128148 '<|reserved_special_token_140|>' is not marked as EOG\n",
      "load: control token: 128181 '<|reserved_special_token_173|>' is not marked as EOG\n",
      "load: control token: 128222 '<|reserved_special_token_214|>' is not marked as EOG\n",
      "load: control token: 128075 '<|reserved_special_token_67|>' is not marked as EOG\n",
      "load: control token: 128241 '<|reserved_special_token_233|>' is not marked as EOG\n",
      "load: control token: 128051 '<|reserved_special_token_43|>' is not marked as EOG\n",
      "load: control token: 128068 '<|reserved_special_token_60|>' is not marked as EOG\n",
      "load: control token: 128149 '<|reserved_special_token_141|>' is not marked as EOG\n",
      "load: control token: 128201 '<|reserved_special_token_193|>' is not marked as EOG\n",
      "load: control token: 128058 '<|reserved_special_token_50|>' is not marked as EOG\n",
      "load: control token: 128146 '<|reserved_special_token_138|>' is not marked as EOG\n",
      "load: control token: 128143 '<|reserved_special_token_135|>' is not marked as EOG\n",
      "load: control token: 128023 '<|reserved_special_token_15|>' is not marked as EOG\n",
      "load: control token: 128039 '<|reserved_special_token_31|>' is not marked as EOG\n",
      "load: control token: 128132 '<|reserved_special_token_124|>' is not marked as EOG\n",
      "load: control token: 128101 '<|reserved_special_token_93|>' is not marked as EOG\n",
      "load: control token: 128212 '<|reserved_special_token_204|>' is not marked as EOG\n",
      "load: control token: 128189 '<|reserved_special_token_181|>' is not marked as EOG\n",
      "load: control token: 128225 '<|reserved_special_token_217|>' is not marked as EOG\n",
      "load: control token: 128129 '<|reserved_special_token_121|>' is not marked as EOG\n",
      "load: control token: 128005 '<|reserved_special_token_2|>' is not marked as EOG\n",
      "load: control token: 128078 '<|reserved_special_token_70|>' is not marked as EOG\n",
      "load: control token: 128163 '<|reserved_special_token_155|>' is not marked as EOG\n",
      "load: control token: 128072 '<|reserved_special_token_64|>' is not marked as EOG\n",
      "load: control token: 128112 '<|reserved_special_token_104|>' is not marked as EOG\n",
      "load: control token: 128186 '<|reserved_special_token_178|>' is not marked as EOG\n",
      "load: control token: 128095 '<|reserved_special_token_87|>' is not marked as EOG\n",
      "load: control token: 128109 '<|reserved_special_token_101|>' is not marked as EOG\n",
      "load: control token: 128099 '<|reserved_special_token_91|>' is not marked as EOG\n",
      "load: control token: 128138 '<|reserved_special_token_130|>' is not marked as EOG\n",
      "load: control token: 128193 '<|reserved_special_token_185|>' is not marked as EOG\n",
      "load: control token: 128199 '<|reserved_special_token_191|>' is not marked as EOG\n",
      "load: control token: 128048 '<|reserved_special_token_40|>' is not marked as EOG\n",
      "load: control token: 128088 '<|reserved_special_token_80|>' is not marked as EOG\n",
      "load: control token: 128192 '<|reserved_special_token_184|>' is not marked as EOG\n",
      "load: control token: 128136 '<|reserved_special_token_128|>' is not marked as EOG\n",
      "load: control token: 128092 '<|reserved_special_token_84|>' is not marked as EOG\n",
      "load: control token: 128158 '<|reserved_special_token_150|>' is not marked as EOG\n",
      "load: control token: 128001 '<|end_of_text|>' is not marked as EOG\n",
      "load: control token: 128049 '<|reserved_special_token_41|>' is not marked as EOG\n",
      "load: control token: 128031 '<|reserved_special_token_23|>' is not marked as EOG\n",
      "load: control token: 128255 '<|reserved_special_token_247|>' is not marked as EOG\n",
      "load: control token: 128182 '<|reserved_special_token_174|>' is not marked as EOG\n",
      "load: control token: 128066 '<|reserved_special_token_58|>' is not marked as EOG\n",
      "load: control token: 128180 '<|reserved_special_token_172|>' is not marked as EOG\n",
      "load: control token: 128233 '<|reserved_special_token_225|>' is not marked as EOG\n",
      "load: control token: 128079 '<|reserved_special_token_71|>' is not marked as EOG\n",
      "load: control token: 128081 '<|reserved_special_token_73|>' is not marked as EOG\n",
      "load: control token: 128231 '<|reserved_special_token_223|>' is not marked as EOG\n",
      "load: control token: 128196 '<|reserved_special_token_188|>' is not marked as EOG\n",
      "load: control token: 128047 '<|reserved_special_token_39|>' is not marked as EOG\n",
      "load: control token: 128083 '<|reserved_special_token_75|>' is not marked as EOG\n",
      "load: control token: 128139 '<|reserved_special_token_131|>' is not marked as EOG\n",
      "load: control token: 128131 '<|reserved_special_token_123|>' is not marked as EOG\n",
      "load: control token: 128118 '<|reserved_special_token_110|>' is not marked as EOG\n",
      "load: control token: 128053 '<|reserved_special_token_45|>' is not marked as EOG\n",
      "load: control token: 128220 '<|reserved_special_token_212|>' is not marked as EOG\n",
      "load: control token: 128108 '<|reserved_special_token_100|>' is not marked as EOG\n",
      "load: control token: 128091 '<|reserved_special_token_83|>' is not marked as EOG\n",
      "load: control token: 128203 '<|reserved_special_token_195|>' is not marked as EOG\n",
      "load: control token: 128059 '<|reserved_special_token_51|>' is not marked as EOG\n",
      "load: control token: 128019 '<|reserved_special_token_11|>' is not marked as EOG\n",
      "load: control token: 128170 '<|reserved_special_token_162|>' is not marked as EOG\n",
      "load: control token: 128205 '<|reserved_special_token_197|>' is not marked as EOG\n",
      "load: control token: 128040 '<|reserved_special_token_32|>' is not marked as EOG\n",
      "load: control token: 128200 '<|reserved_special_token_192|>' is not marked as EOG\n",
      "load: control token: 128236 '<|reserved_special_token_228|>' is not marked as EOG\n",
      "load: control token: 128145 '<|reserved_special_token_137|>' is not marked as EOG\n",
      "load: control token: 128168 '<|reserved_special_token_160|>' is not marked as EOG\n",
      "load: control token: 128214 '<|reserved_special_token_206|>' is not marked as EOG\n",
      "load: control token: 128137 '<|reserved_special_token_129|>' is not marked as EOG\n",
      "load: control token: 128232 '<|reserved_special_token_224|>' is not marked as EOG\n",
      "load: control token: 128239 '<|reserved_special_token_231|>' is not marked as EOG\n",
      "load: control token: 128055 '<|reserved_special_token_47|>' is not marked as EOG\n",
      "load: control token: 128228 '<|reserved_special_token_220|>' is not marked as EOG\n",
      "load: control token: 128206 '<|reserved_special_token_198|>' is not marked as EOG\n",
      "load: control token: 128018 '<|reserved_special_token_10|>' is not marked as EOG\n",
      "load: control token: 128012 '<|reserved_special_token_4|>' is not marked as EOG\n",
      "load: control token: 128198 '<|reserved_special_token_190|>' is not marked as EOG\n",
      "load: control token: 128021 '<|reserved_special_token_13|>' is not marked as EOG\n",
      "load: control token: 128086 '<|reserved_special_token_78|>' is not marked as EOG\n",
      "load: control token: 128074 '<|reserved_special_token_66|>' is not marked as EOG\n",
      "load: control token: 128027 '<|reserved_special_token_19|>' is not marked as EOG\n",
      "load: control token: 128242 '<|reserved_special_token_234|>' is not marked as EOG\n",
      "load: control token: 128155 '<|reserved_special_token_147|>' is not marked as EOG\n",
      "load: control token: 128052 '<|reserved_special_token_44|>' is not marked as EOG\n",
      "load: control token: 128246 '<|reserved_special_token_238|>' is not marked as EOG\n",
      "load: control token: 128117 '<|reserved_special_token_109|>' is not marked as EOG\n",
      "load: control token: 128237 '<|reserved_special_token_229|>' is not marked as EOG\n",
      "load: special tokens cache size = 256\n",
      "load: token to piece cache size = 0.7999 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 131072\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 500000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 131072\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 8B\n",
      "print_info: model params     = 8.03 B\n",
      "print_info: general.name     = Llama 3.1 8B Instruct\n",
      "print_info: vocab type       = BPE\n",
      "print_info: n_vocab          = 128256\n",
      "print_info: n_merges         = 280147\n",
      "print_info: BOS token        = 128000 '<|begin_of_text|>'\n",
      "print_info: EOS token        = 128009 '<|eot_id|>'\n",
      "print_info: EOT token        = 128009 '<|eot_id|>'\n",
      "print_info: EOM token        = 128008 '<|eom_id|>'\n",
      "print_info: LF token         = 128 'Ä'\n",
      "print_info: EOG token        = 128008 '<|eom_id|>'\n",
      "print_info: EOG token        = 128009 '<|eot_id|>'\n",
      "print_info: max token length = 256\n",
      "load_tensors: layer   0 assigned to device Metal\n",
      "load_tensors: layer   1 assigned to device Metal\n",
      "load_tensors: layer   2 assigned to device Metal\n",
      "load_tensors: layer   3 assigned to device Metal\n",
      "load_tensors: layer   4 assigned to device Metal\n",
      "load_tensors: layer   5 assigned to device Metal\n",
      "load_tensors: layer   6 assigned to device Metal\n",
      "load_tensors: layer   7 assigned to device Metal\n",
      "load_tensors: layer   8 assigned to device Metal\n",
      "load_tensors: layer   9 assigned to device Metal\n",
      "load_tensors: layer  10 assigned to device Metal\n",
      "load_tensors: layer  11 assigned to device Metal\n",
      "load_tensors: layer  12 assigned to device Metal\n",
      "load_tensors: layer  13 assigned to device Metal\n",
      "load_tensors: layer  14 assigned to device Metal\n",
      "load_tensors: layer  15 assigned to device Metal\n",
      "load_tensors: layer  16 assigned to device Metal\n",
      "load_tensors: layer  17 assigned to device Metal\n",
      "load_tensors: layer  18 assigned to device Metal\n",
      "load_tensors: layer  19 assigned to device Metal\n",
      "load_tensors: layer  20 assigned to device Metal\n",
      "load_tensors: layer  21 assigned to device Metal\n",
      "load_tensors: layer  22 assigned to device Metal\n",
      "load_tensors: layer  23 assigned to device Metal\n",
      "load_tensors: layer  24 assigned to device Metal\n",
      "load_tensors: layer  25 assigned to device Metal\n",
      "load_tensors: layer  26 assigned to device Metal\n",
      "load_tensors: layer  27 assigned to device Metal\n",
      "load_tensors: layer  28 assigned to device Metal\n",
      "load_tensors: layer  29 assigned to device Metal\n",
      "load_tensors: layer  30 assigned to device Metal\n",
      "load_tensors: layer  31 assigned to device Metal\n",
      "load_tensors: layer  32 assigned to device Metal\n",
      "load_tensors: tensor 'token_embd.weight' (q5_K) (and 0 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "ggml_backend_metal_log_allocated_size: allocated buffer, size =  5459.95 MiB, ( 5460.03 / 10922.67)\n",
      "load_tensors: offloading 32 repeating layers to GPU\n",
      "load_tensors: offloading output layer to GPU\n",
      "load_tensors: offloaded 33/33 layers to GPU\n",
      "load_tensors: Metal_Mapped model buffer size =  5459.94 MiB\n",
      "load_tensors:   CPU_Mapped model buffer size =   344.44 MiB\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 3904\n",
      "llama_init_from_model: n_ctx_per_seq = 3904\n",
      "llama_init_from_model: n_batch       = 512\n",
      "llama_init_from_model: n_ubatch      = 512\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 500000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (3904) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: allocating\n",
      "ggml_metal_init: found device: Apple M3\n",
      "ggml_metal_init: picking default device: Apple M3\n",
      "ggml_metal_init: using embedded metal library\n",
      "ggml_metal_init: GPU name:   Apple M3\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyApple9  (1009)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyCommon3 (3003)\n",
      "ggml_metal_init: GPU family: MTLGPUFamilyMetal3  (5001)\n",
      "ggml_metal_init: simdgroup reduction   = true\n",
      "ggml_metal_init: simdgroup matrix mul. = true\n",
      "ggml_metal_init: has residency sets    = false\n",
      "ggml_metal_init: has bfloat            = true\n",
      "ggml_metal_init: use bfloat            = false\n",
      "ggml_metal_init: hasUnifiedMemory      = true\n",
      "ggml_metal_init: recommendedMaxWorkingSetSize  = 11453.25 MB\n",
      "ggml_metal_init: loaded kernel_add                                    0x15a98a660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_add_row                                0x15a984f70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub                                    0x154ec4790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sub_row                                0x158ca1a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul                                    0x149996070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_row                                0x15a4bedc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div                                    0x15a98bd90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_div_row                                0x149999230 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f32                             0x15a98bfc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_f16                             0x15a98b310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i32                             0x15a4bf5e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_repeat_i16                             0x15a989940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale                                  0x15a989c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_scale_4                                0x149994b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_clamp                                  0x149994d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_tanh                                   0x15af423e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_relu                                   0x158ca23b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sigmoid                                0x15a4bf910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu                                   0x15a4bfb40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_4                                 0x15a4bfd70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick                             0x15a98b6b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_gelu_quick_4                           0x158ca3700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu                                   0x158ca3d60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_silu_4                                 0x15af42730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_elu                                    0x158ca3f90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16                           0x15a4c1950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f16_4                         0x15a98b8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32                           0x15a98d270 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_soft_max_f32_4                         0x15a4c0b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf                          0x15af42960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_diag_mask_inf_8                        0x15af43c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f32                           0x15af43e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_f16                           0x15a98d8d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_0                          0x15af44060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_1                          0x15a4c23d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_0                          0x15a4c03c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_1                          0x15a4c2e60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q8_0                          0x15a4c3820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q2_K                          0x158ca5990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q3_K                          0x158ca5120 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q4_K                          0x15af445a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q5_K                          0x15af44a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_q6_K                          0x15a98e440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xxs                       0x15af44c30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_xs                        0x158ca5430 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_xxs                       0x158ca5d10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq3_s                         0x158ca6720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq2_s                         0x15a4c2780 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_s                         0x15a4c3ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq1_m                         0x158ca5f40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_nl                        0x15a98f2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_iq4_xs                        0x15a98e950 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_get_rows_i32                           0x15af45020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rms_norm                               0x15a4c4730 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_group_norm                             0x15a4c4960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_norm                                   0x15af45250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_conv_f32                           0x158ca6b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_ssm_scan_f32                           0x158ca6f60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f32_f32                         0x15af45480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32                         0x158ca7b10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_1row                    0x15a4c55f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f32_l4                      0x15af459e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_f16_f16                         0x15a98eb80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_0_f32                        0x158ca7190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_1_f32                        0x15a4c4c40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_0_f32                        0x15a4c5bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_1_f32                        0x15af463b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q8_0_f32                        0x15af465e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_2                0x15a4c6160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_3                0x15af46810 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_4                0x15a4c6390 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_f16_f32_r1_5                0x15af47090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_2               0x158ca7620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_3               0x158ca8020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_4               0x15a9906a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_0_f32_r1_5               0x15a98f930 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_2               0x158ca8d20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_3               0x158ca9050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_4               0x15a98fe70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_1_f32_r1_5               0x15af48a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_2               0x15af46ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_3               0x15af48510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_4               0x158caa090 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_0_f32_r1_5               0x15af47bc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_2               0x158ca9a40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_3               0x158ca9650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_4               0x15a991740 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_1_f32_r1_5               0x158ca9380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_2               0x15af490d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_3               0x15a991a00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_4               0x15a9922b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q8_0_f32_r1_5               0x158caaa80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_2               0x15a990a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_3               0x158caacb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_4               0x158cab0b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q4_K_f32_r1_5               0x15a4c6970 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_2               0x15a4c7580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_3               0x15a991060 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_4               0x158cab520 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q5_K_f32_r1_5               0x15a992aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_2               0x158cab8e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_3               0x158cac400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_4               0x15af49870 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_q6_K_f32_r1_5               0x15af49300 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_2             0x15a4c78f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_3             0x158cac800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_4             0x15a4c6ba0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_ext_iq4_nl_f32_r1_5             0x158cacb70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q2_K_f32                        0x15af49e10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q3_K_f32                        0x15a992d80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q4_K_f32                        0x15af4af30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q5_K_f32                        0x15af4a0d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_q6_K_f32                        0x15a993ee0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xxs_f32                     0x15af4bf10 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_xs_f32                      0x15a4c6dd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_xxs_f32                     0x15af4b800 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq3_s_f32                       0x15af4a6a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq2_s_f32                       0x15a993980 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_s_f32                       0x15a993720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq1_m_f32                       0x15af4c340 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_nl_f32                      0x15a994420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_iq4_xs_f32                      0x15a994650 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f32_f32                      0x15a4c8480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_f16_f32                      0x15a994940 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_0_f32                     0x15a994b70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_1_f32                     0x15af4c570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_0_f32                     0x158cacfe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_1_f32                     0x15af4c7a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q8_0_f32                     0x15a4ca190 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q2_K_f32                     0x158cad600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q3_K_f32                     0x15af4e160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q4_K_f32                     0x15a4c86b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q5_K_f32                     0x158caeb90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_q6_K_f32                     0x158caedf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xxs_f32                  0x15a4ca9c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_xs_f32                   0x15a4c9620 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_xxs_f32                  0x158cad860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq3_s_f32                    0x15a995ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq2_s_f32                    0x158cae030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_s_f32                    0x158cafdd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq1_m_f32                    0x158cb08b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_nl_f32                   0x15af4ca50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mv_id_iq4_xs_f32                   0x15af4dc00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f32_f32                         0x15af4ed50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_f16_f32                         0x158cb0030 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_0_f32                        0x158cb0260 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_1_f32                        0x15a4c9a80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_0_f32                        0x15a996350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_1_f32                        0x158caf4b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q8_0_f32                        0x15a996580 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q2_K_f32                        0x15a996a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q3_K_f32                        0x15af4d440 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q4_K_f32                        0x15a997380 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q5_K_f32                        0x15a9975b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_q6_K_f32                        0x15af4f2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xxs_f32                     0x15a997c80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_xs_f32                      0x15a998400 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_xxs_f32                     0x15af4e750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq3_s_f32                       0x15af4f820 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq2_s_f32                       0x15af4cd50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_s_f32                       0x15af4fa50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq1_m_f32                       0x158cb1210 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_nl_f32                      0x15af50480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_iq4_xs_f32                      0x15a4cac20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f32_f32                      0x15a4cb080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_f16_f32                      0x15a4cae50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_0_f32                     0x15a4cb2b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_1_f32                     0x15af4ffb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_0_f32                     0x15a9988d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_1_f32                     0x158cb1570 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q8_0_f32                     0x15a999cf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q2_K_f32                     0x15af501e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q3_K_f32                     0x158cb27d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q4_K_f32                     0x15af51aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q5_K_f32                     0x15a99a2f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_q6_K_f32                     0x15af50ae0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xxs_f32                  0x15af52070 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_xs_f32                   0x15af52680 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_xxs_f32                  0x15af50de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq3_s_f32                    0x15af52c00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq2_s_f32                    0x158cb1dc0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_s_f32                    0x15af53aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq1_m_f32                    0x15af54040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_nl_f32                   0x15a4cc2d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_mul_mm_id_iq4_xs_f32                   0x15a99a8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f32                          0x15af54660 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_norm_f16                          0x15a4cbaf0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f32                          0x15a998ec0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_rope_neox_f16                          0x158cb3350 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f16                             0x15a99b510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_f32                             0x158cb2da0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f16                         0x15a99baa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_im2col_ext_f32                         0x158cb3e20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f32_f32              0x15a99c9d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_conv_transpose_1d_f16_f32              0x158cb4a60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_upscale_f32                            0x15a99cc60 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_f32                                0x15a99ce90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pad_reflect_1d_f32                     0x15a99dd80 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_timestep_embedding_f32                 0x15af54cb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_arange_f32                             0x15af533c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_asc                    0x15af52e30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argsort_f32_i32_desc                   0x158cb17a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_leaky_relu_f32                         0x15a99ae40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h64                 0x15a99e960 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h80                 0x15a99ef30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h96                 0x15a99f500 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h112                0x158cb5420 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h128                0x15af564f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_f16_h256                0x15a4cc640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h64                0x15a99e180 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h80                0x15a4cd240 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h96                0x158cb5690 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h112               0x158cb6a90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h128               0x15a99e3b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_0_h256               0x15af56c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h64                0x158cb5a50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h80                0x15a9a0040 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h96                0x15af570e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h112               0x15af57bd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h128               0x15a99d6c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q4_1_h256               0x15a4ce7f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h64                0x15a99f760 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h80                0x158cb7640 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h96                0x158cb6de0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h112               0x15a4cdd20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h128               0x158cb7080 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_0_h256               0x15a4cd8c0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h64                0x15a9a0fe0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h80                0x15a99f9f0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h96                0x15a9a0560 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h112               0x15af57f20 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h128               0x15af58610 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q5_1_h256               0x15af58b00 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h64                0x15af590e0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h80                0x15af59ca0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h96                0x15af59700 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h112               0x15af5a750 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h128               0x15a4cf510 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_q8_0_h256               0x15a9a1910 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h128            0x15a9a1b40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h128           0x158cb8050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h128           0x158cb7990 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h128           0x15a4cef40 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h128           0x158cb8aa0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h128           0x15a9a25a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_f16_h256            0x15a4d0160 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_0_h256           0x158cb9860 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q4_1_h256           0x15a9a2cd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_0_h256           0x15af573b0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q5_1_h256           0x15a9a3050 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_flash_attn_ext_vec_q8_0_h256           0x15a4cfc50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_f32                                0x15a4d0c90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_set_i32                                0x158cb9bb0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f32                            0x15af55d70 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_f16                            0x15af5ade0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f32                            0x15af5b010 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f16_f16                            0x15a9a0790 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q8_0                           0x15af5bad0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_0                           0x15af5b5a0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q4_1                           0x15af5c020 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_0                           0x15af5c250 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_q5_1                           0x15af5c480 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cpy_f32_iq4_nl                         0x15a4d1600 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_concat                                 0x15af5cbd0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqr                                    0x15a4d1c50 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sqrt                                   0x15af5d720 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sin                                    0x15af5df30 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_cos                                    0x15af5ee90 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_sum_rows                               0x15a4d2410 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_argmax                                 0x15af5f3d0 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_avg_f32                        0x158cb9310 | th_max = 1024 | th_width =   32\n",
      "ggml_metal_init: loaded kernel_pool_2d_max_f32                        0x158cb8e70 | th_max = 1024 | th_width =   32\n",
      "llama_kv_cache_init: kv_size = 3904, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:      Metal KV buffer size =   488.00 MiB\n",
      "llama_init_from_model: KV self size  =  488.00 MiB, K (f16):  244.00 MiB, V (f16):  244.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.49 MiB\n",
      "llama_init_from_model:      Metal compute buffer size =   283.63 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =    15.63 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 2\n",
      "Metal : EMBED_LIBRARY = 1 | CPU : NEON = 1 | ARM_FMA = 1 | FP16_VA = 1 | MATMUL_INT8 = 1 | DOTPROD = 1 | MATMUL_INT8 = 1 | ACCELERATE = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'tokenizer.chat_template': '{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- set date_string = \"26 Jul 2024\" %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0][\\'role\\'] == \\'system\\' %}\\n    {%- set system_message = messages[0][\\'content\\']|trim %}\\n    {%- set messages = messages[1:] %}\\n{%- else %}\\n    {%- set system_message = \"\" %}\\n{%- endif %}\\n\\n{#- System message + builtin tools #}\\n{{- \"<|start_header_id|>system<|end_header_id|>\\\\n\\\\n\" }}\\n{%- if builtin_tools is defined or tools is not none %}\\n    {{- \"Environment: ipython\\\\n\" }}\\n{%- endif %}\\n{%- if builtin_tools is defined %}\\n    {{- \"Tools: \" + builtin_tools | reject(\\'equalto\\', \\'code_interpreter\\') | join(\", \") + \"\\\\n\\\\n\"}}\\n{%- endif %}\\n{{- \"Cutting Knowledge Date: December 2023\\\\n\" }}\\n{{- \"Today Date: \" + date_string + \"\\\\n\\\\n\" }}\\n{%- if tools is not none and not tools_in_user_message %}\\n    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n{%- endif %}\\n{{- system_message }}\\n{{- \"<|eot_id|>\" }}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0][\\'content\\']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\"Cannot put tools in the first user message when there\\'s no first user message!\") }}\\n{%- endif %}\\n    {{- \\'<|start_header_id|>user<|end_header_id|>\\\\n\\\\n\\' -}}\\n    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\\n    {{- \"with its proper arguments that best answers the given prompt.\\\\n\\\\n\" }}\\n    {{- \\'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.\\' }}\\n    {{- \"Do not use variables.\\\\n\\\\n\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \"\\\\n\\\\n\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \"<|eot_id|>\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == \\'ipython\\' or message.role == \\'tool\\' or \\'tool_calls\\' in message) %}\\n        {{- \\'<|start_header_id|>\\' + message[\\'role\\'] + \\'<|end_header_id|>\\\\n\\\\n\\'+ message[\\'content\\'] | trim + \\'<|eot_id|>\\' }}\\n    {%- elif \\'tool_calls\\' in message %}\\n        {%- if not message.tool_calls|length == 1 %}\\n            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\\n        {%- endif %}\\n        {%- set tool_call = message.tool_calls[0].function %}\\n        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\\n            {%- for arg_name, arg_val in tool_call.arguments | items %}\\n                {{- arg_name + \\'=\"\\' + arg_val + \\'\"\\' }}\\n                {%- if not loop.last %}\\n                    {{- \", \" }}\\n                {%- endif %}\\n                {%- endfor %}\\n            {{- \")\" }}\\n        {%- else  %}\\n            {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' -}}\\n            {{- \\'{\"name\": \"\\' + tool_call.name + \\'\", \\' }}\\n            {{- \\'\"parameters\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \"}\" }}\\n        {%- endif %}\\n        {%- if builtin_tools is defined %}\\n            {#- This means we\\'re in ipython mode #}\\n            {{- \"<|eom_id|>\" }}\\n        {%- else %}\\n            {{- \"<|eot_id|>\" }}\\n        {%- endif %}\\n    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\\n        {{- \"<|start_header_id|>ipython<|end_header_id|>\\\\n\\\\n\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \"<|eot_id|>\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|start_header_id|>assistant<|end_header_id|>\\\\n\\\\n\\' }}\\n{%- endif %}\\n', 'tokenizer.ggml.eos_token_id': '128009', 'llama.embedding_length': '4096', 'llama.vocab_size': '128256', 'llama.attention.head_count_kv': '8', 'general.size_label': '8B', 'general.base_model.0.name': 'Meta Llama 3.1 8B', 'llama.block_count': '32', 'general.base_model.0.organization': 'Meta Llama', 'tokenizer.ggml.pre': 'llama-bpe', 'general.base_model.count': '1', 'general.quantization_version': '2', 'llama.rope.dimension_count': '128', 'general.license': 'llama3.1', 'general.base_model.0.repo_url': 'https://huggingface.co/meta-llama/Meta-Llama-3.1-8B', 'llama.attention.head_count': '32', 'llama.context_length': '131072', 'general.file_type': '17', 'general.finetune': 'Instruct', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'general.architecture': 'llama', 'general.basename': 'Llama-3.1', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.bos_token_id': '128000', 'general.type': 'model', 'tokenizer.ggml.model': 'gpt2', 'general.name': 'Llama 3.1 8B Instruct'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {{- bos_token }}\n",
      "{%- if custom_tools is defined %}\n",
      "    {%- set tools = custom_tools %}\n",
      "{%- endif %}\n",
      "{%- if not tools_in_user_message is defined %}\n",
      "    {%- set tools_in_user_message = true %}\n",
      "{%- endif %}\n",
      "{%- if not date_string is defined %}\n",
      "    {%- set date_string = \"26 Jul 2024\" %}\n",
      "{%- endif %}\n",
      "{%- if not tools is defined %}\n",
      "    {%- set tools = none %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- This block extracts the system message, so we can slot it into the right place. #}\n",
      "{%- if messages[0]['role'] == 'system' %}\n",
      "    {%- set system_message = messages[0]['content']|trim %}\n",
      "    {%- set messages = messages[1:] %}\n",
      "{%- else %}\n",
      "    {%- set system_message = \"\" %}\n",
      "{%- endif %}\n",
      "\n",
      "{#- System message + builtin tools #}\n",
      "{{- \"<|start_header_id|>system<|end_header_id|>\\n\\n\" }}\n",
      "{%- if builtin_tools is defined or tools is not none %}\n",
      "    {{- \"Environment: ipython\\n\" }}\n",
      "{%- endif %}\n",
      "{%- if builtin_tools is defined %}\n",
      "    {{- \"Tools: \" + builtin_tools | reject('equalto', 'code_interpreter') | join(\", \") + \"\\n\\n\"}}\n",
      "{%- endif %}\n",
      "{{- \"Cutting Knowledge Date: December 2023\\n\" }}\n",
      "{{- \"Today Date: \" + date_string + \"\\n\\n\" }}\n",
      "{%- if tools is not none and not tools_in_user_message %}\n",
      "    {{- \"You have access to the following functions. To call a function, please respond with JSON for a function call.\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "{%- endif %}\n",
      "{{- system_message }}\n",
      "{{- \"<|eot_id|>\" }}\n",
      "\n",
      "{#- Custom tools are passed in a user message with some extra guidance #}\n",
      "{%- if tools_in_user_message and not tools is none %}\n",
      "    {#- Extract the first user message so we can plug it in here #}\n",
      "    {%- if messages | length != 0 %}\n",
      "        {%- set first_user_message = messages[0]['content']|trim %}\n",
      "        {%- set messages = messages[1:] %}\n",
      "    {%- else %}\n",
      "        {{- raise_exception(\"Cannot put tools in the first user message when there's no first user message!\") }}\n",
      "{%- endif %}\n",
      "    {{- '<|start_header_id|>user<|end_header_id|>\\n\\n' -}}\n",
      "    {{- \"Given the following functions, please respond with a JSON for a function call \" }}\n",
      "    {{- \"with its proper arguments that best answers the given prompt.\\n\\n\" }}\n",
      "    {{- 'Respond in the format {\"name\": function name, \"parameters\": dictionary of argument name and its value}.' }}\n",
      "    {{- \"Do not use variables.\\n\\n\" }}\n",
      "    {%- for t in tools %}\n",
      "        {{- t | tojson(indent=4) }}\n",
      "        {{- \"\\n\\n\" }}\n",
      "    {%- endfor %}\n",
      "    {{- first_user_message + \"<|eot_id|>\"}}\n",
      "{%- endif %}\n",
      "\n",
      "{%- for message in messages %}\n",
      "    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\n",
      "        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' }}\n",
      "    {%- elif 'tool_calls' in message %}\n",
      "        {%- if not message.tool_calls|length == 1 %}\n",
      "            {{- raise_exception(\"This model only supports single tool-calls at once!\") }}\n",
      "        {%- endif %}\n",
      "        {%- set tool_call = message.tool_calls[0].function %}\n",
      "        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- \"<|python_tag|>\" + tool_call.name + \".call(\" }}\n",
      "            {%- for arg_name, arg_val in tool_call.arguments | items %}\n",
      "                {{- arg_name + '=\"' + arg_val + '\"' }}\n",
      "                {%- if not loop.last %}\n",
      "                    {{- \", \" }}\n",
      "                {%- endif %}\n",
      "                {%- endfor %}\n",
      "            {{- \")\" }}\n",
      "        {%- else  %}\n",
      "            {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' -}}\n",
      "            {{- '{\"name\": \"' + tool_call.name + '\", ' }}\n",
      "            {{- '\"parameters\": ' }}\n",
      "            {{- tool_call.arguments | tojson }}\n",
      "            {{- \"}\" }}\n",
      "        {%- endif %}\n",
      "        {%- if builtin_tools is defined %}\n",
      "            {#- This means we're in ipython mode #}\n",
      "            {{- \"<|eom_id|>\" }}\n",
      "        {%- else %}\n",
      "            {{- \"<|eot_id|>\" }}\n",
      "        {%- endif %}\n",
      "    {%- elif message.role == \"tool\" or message.role == \"ipython\" %}\n",
      "        {{- \"<|start_header_id|>ipython<|end_header_id|>\\n\\n\" }}\n",
      "        {%- if message.content is mapping or message.content is iterable %}\n",
      "            {{- message.content | tojson }}\n",
      "        {%- else %}\n",
      "            {{- message.content }}\n",
      "        {%- endif %}\n",
      "        {{- \"<|eot_id|>\" }}\n",
      "    {%- endif %}\n",
      "{%- endfor %}\n",
      "{%- if add_generation_prompt %}\n",
      "    {{- '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}\n",
      "{%- endif %}\n",
      "\n",
      "Using chat eos_token: <|eot_id|>\n",
      "Using chat bos_token: <|begin_of_text|>\n"
     ]
    }
   ],
   "source": [
    "llm = load_llm()\n",
    "Settings.llm = llm  # Assign model globally in LlamaIndex\n",
    "\n",
    "# Load embedding model from local storage\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=EMBEDDING_MODEL_NAME,\n",
    "#     cache_folder=EMBEDDING_MODEL_PATH\n",
    "# )\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_folder=\"./embedding_cache\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model_name': 'sentence-transformers/all-MiniLM-L6-v2', 'embed_batch_size': 10, 'callback_manager': <llama_index.core.callbacks.base.CallbackManager object at 0x1150f8400>, 'num_workers': None, 'max_length': 256, 'normalize': True, 'query_instruction': None, 'text_instruction': None, 'cache_folder': None}\n"
     ]
    }
   ],
   "source": [
    "print(vars(Settings.embed_model))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up ChromaDB with Persistent Storage\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=\"financial_reports_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 12 document chunks in ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "# Load and index documents\n",
    "documents = SimpleDirectoryReader(DOCUMENT_PATH).load_data()\n",
    "\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    text = node.get_text()\n",
    "    embedding = Settings.embed_model.get_text_embedding(text)\n",
    "    \n",
    "    chroma_collection.add(ids=[f\"doc_{i}\"], documents=[text], embeddings=[embedding])\n",
    "print(f\"Stored {len(nodes)} document chunks in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load From Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"financial_reports_2\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_folder=\"./embedding_cache\"\n",
    ")\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔍 **Retrieved Chunks:**\n",
      "\n",
      "--- Chunk 1 ---\n",
      "Segment Results\n",
      "The following table presents our segment revenues and operating income (loss) (in millions; unaudited):\n",
      "Quarter Ended December 31,\n",
      "2023 2024\n",
      "Revenues:\n",
      "Google Services $ 76,311 $ 84,094 \n",
      "Google Cloud  9,192  11,955 \n",
      "Other Bets  657  400 \n",
      "Hedging gains (losses)  150  20 \n",
      "Total revenues $ 86,310 $ 96,469 \n",
      "Operating income (loss):\n",
      "Google Services $ 26,730 $ 32,836 \n",
      "Google Cloud  864  2,093 \n",
      "Other Bets  (863)  (1,174) \n",
      "Alphabet-level activities  (3,034)  (2,783) \n",
      "Total income from operations $ 23,697 $ 30,972 \n",
      "We report our segment results as Google Services, Google Cloud, and Other Bets:\n",
      "• Google Services includes products and services such as ads, Android, Chrome, devices, Google Maps, \n",
      "Google Play, Search, and YouTube. Google Services generates revenues primarily from advertising; fees \n",
      "received for consumer subscription-based products such as YouTube TV, YouTube Music and Premium, \n",
      "and NFL Sunday Ticket, as well as Google One; the sale of apps and in-app purchases; and devices.\n",
      "• Google Cloud includes infrastructure and platform services, applications, and other services for enterprise \n",
      "customers. Google Cloud generates revenues primarily from consumption-based fees and subscriptions \n",
      "received for Google Cloud Platform services, Google Workspace communication and collaboration tools, \n",
      "and other enterprise services.\n",
      "• Other Bets is a combination of multiple operating segments that are not individually material. Revenues \n",
      "from Other Bets are generated primarily from the sale of healthcare-related services and internet services.\n",
      "Certain costs are not allocated to our segments because they represent Alphabet-level activities. These costs \n",
      "primarily include certain AI-focused shared R&D activities, including development costs of our general AI models; \n",
      "corporate initiatives such as our philanthropic activities; corporate shared costs such as certain finance, human \n",
      "resource, and legal costs, including certain fines and settlements. Charges associated with employee severance \n",
      "and office space reductions during 2023 and 2024 were also not allocated to our segments. Additionally, hedging \n",
      "gains (losses) related to revenue are not allocated to our segments.\n",
      "7\n",
      "\n",
      "\n",
      "--- Chunk 2 ---\n",
      "Alphabet Announces Fourth Quarter and Fiscal Year 2024 Results\n",
      "MOUNTAIN VIEW, Calif. – February 4, 2025 – Alphabet Inc. (NASDAQ: GOOG, GOOGL) today announced financial \n",
      "results for the quarter and fiscal year ended December 31, 2024.\n",
      "• Consolidated Alphabet revenues in Q4 2024 increased 12% year over year to $96.5 billion reflecting robust \n",
      "momentum across the business.\n",
      "• Google Services revenues increased 10% to $84.1 billion, reflecting the strong momentum  across Google \n",
      "Search & other and YouTube ads.\n",
      "• Google Cloud revenues increased 30% to $12.0 billion led by growth in Google Cloud Platform (GCP) across \n",
      "core GCP products, AI Infrastructure, and Generative AI Solutions.\n",
      "• Total operating income increased 31% and operating margin expanded by 5% percentage points to 32%.\n",
      "• Net income increased 28% and EPS increased 31% to $2.15.\n",
      "Sundar Pichai, CEO, said: “Q4 was a strong quarter driven by our leadership in AI and momentum across the \n",
      "business. We are building, testing, and launching products and models faster than ever, and making significant \n",
      "progress in compute and driving efficiencies. In Search, advances like AI Overviews and Circle to Search are \n",
      "increasing user engagement. Our AI-powered Google Cloud portfolio is seeing stronger customer demand, and \n",
      "YouTube continues to be the leader in streaming watchtime and podcasts. Together, Cloud and YouTube exited 2024 \n",
      "at an annual revenue run rate of $110 billion. Our results show the power of our differentiated full-stack approach to \n",
      "AI innovation and the continued strength of our core businesses. We are confident about the opportunities ahead, \n",
      "and to accelerate our progress, we expect to invest approximately $75 billion in capital expenditures in 2025.”\n",
      "Q4 2024 Financial Highlights\n",
      "The following table summarizes our consolidated financial results for the quarter and fiscal year ended December 31, \n",
      "2023 and 2024 (in millions, except for per share information and percentages). \n",
      "Quarter Ended December 31, Year Ended December 31,\n",
      "2023 2024 2023 2024\n",
      "(unaudited) (unaudited)\n",
      "Revenues $ 86,310 $ 96,469 $ 307,394 $ 350,018 \n",
      "Change in revenues year over year  13 %  12 %  9 %  14 %\n",
      "Change in constant currency revenues year over year(1)  13 %  12 %  10 %  15 %\n",
      "Operating income $ 23,697 $ 30,972 $ 84,293 $ 112,390 \n",
      "Operating margin  27 %  32 %  27 %  32 %\n",
      "Other income (expense), net $ 715 $ 1,271 $ 1,424 $ 7,425 \n",
      "Net income $ 20,687 $ 26,536 $ 73,795 $ 100,118 \n",
      "Diluted EPS $ 1.64 $ 2.15 $ 5.80 $ 8.04 \n",
      "(1) Non-GAAP measure. See the section captioned “ Reconciliation from GAAP Revenues to Non-GAAP Constant Currency \n",
      "Revenues and GAAP Percentage Change in Revenues to Non-GAAP Percentage Change in Constant Currency Revenues ” \n",
      "for more details.\n",
      "1\n",
      "\n",
      "\n",
      "--- Chunk 3 ---\n",
      "Q4 2024 Supplemental Information (in millions, except for number of employees; unaudited)\n",
      "Revenues, Traffic Acquisition Costs (TAC), and Number of Employees\n",
      "Quarter Ended December 31,\n",
      "2023 2024\n",
      "Google Search & other $ 48,020 $ 54,034 \n",
      "YouTube ads  9,200  10,473 \n",
      "Google Network  8,297  7,954 \n",
      "Google advertising  65,517  72,461 \n",
      "Google subscriptions, platforms, and devices  10,794  11,633 \n",
      "Google Services total  76,311  84,094 \n",
      "Google Cloud  9,192  11,955 \n",
      "Other Bets  657  400 \n",
      "Hedging gains (losses)  150  20 \n",
      "Total revenues $ 86,310 $ 96,469 \n",
      "Total TAC $ 13,986 $ 14,848 \n",
      "Number of employees  182,502  183,323 \n",
      "Segment Operating Results \n",
      "As announced on October 17, 2024,  the Gemini app team that is developing the direct consumer interface to our \n",
      "Gemini models joined Google DeepMind  in the quarter ended December 31, 2024. The costs associated with the \n",
      "Gemini app team continue to be reported within our Google Services segment.\n",
      "Quarter Ended December 31,\n",
      "2023 2024\n",
      "Operating income (loss):\n",
      "Google Services $ 26,730 $ 32,836 \n",
      "Google Cloud  864 $ 2,093 \n",
      "Other Bets  (863) $ (1,174) \n",
      "Alphabet-level activities(1)  (3,034) $ (2,783) \n",
      "Total income from operations $ 23,697 $ 30,972 \n",
      "(1) In addition to the costs included in Alphabet-level activities, hedging gains (losses) related to revenue were $150 million and \n",
      "$20 million for the three months ended December 31, 2023  and 2024, respectively. For the three months ended December \n",
      "31, 2023 and 2024, Alphabet-level activities included substantially all of the charges related to employee severance and our \n",
      "office space charges. \n",
      "Additional Information Relating to the Quarter Ended December 31, 2024 (unaudited)\n",
      "Dividend Program\n",
      "Dividend payments to stockholders of Class A, Class B, and Class C shares, were $1.2 billion, $172 million, and $1.1 \n",
      "billion, respectively, totaling $2.4 billion for the three months ended December 31, 2024.\n",
      "2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define a test question\n",
    "query = \"What is Google Cloud Revenue change like?\"\n",
    "\n",
    "# ✅ Retrieve relevant document chunks\n",
    "retriever = index.as_retriever(similarity_top_k=3)  # Adjust similarity_top_k as needed\n",
    "retrieved_docs = retriever.retrieve(query)\n",
    "\n",
    "# ✅ Print retrieved chunks\n",
    "print(\"\\n🔍 **Retrieved Chunks:**\")\n",
    "for i, doc in enumerate(retrieved_docs):\n",
    "    print(f\"\\n--- Chunk {i+1} ---\\n{doc.text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Query engine initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Rewrite the user's follow-up question as a standalone question.\n",
    "\n",
    "1. Include all relevant past context.\n",
    "2. Keep it natural and grammatically correct.\n",
    "3. If already standalone, return it unchanged.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<User's Follow-Up Question>\n",
    "{question}\n",
    "\n",
    "<Rewritten Standalone Question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "response_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "You are an AI assistant providing structured responses.\n",
    "\n",
    "### **Instructions:**\n",
    "- Answer clearly and concisely.\n",
    "- Summarize retrieved context to avoid duplication.\n",
    "- Summarize the key facts efficiently.\n",
    "- If the context lacks enough details, say: \"I don’t have enough information.\"\n",
    "- Format responses in natural sentences.\n",
    "\n",
    "<Retrieved Context>\n",
    "{context}\n",
    "\n",
    "<User's Query>\n",
    "{question}\n",
    "\n",
    "### **AI Response:**\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"compact\",\n",
    "    response_prompt=response_prompt,\n",
    "    similarity_top_k=1,\n",
    "    max_tokens = 300,\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "print(\"✅ Query engine initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 520 prefix-match hit, remaining 1 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   13954.12 ms\n",
      "llama_perf_context_print: prompt eval time =       0.00 ms /     1 tokens (    0.00 ms per token,      inf tokens per second)\n",
      "llama_perf_context_print:        eval time =   79804.68 ms /  1024 runs   (   77.93 ms per token,    12.83 tokens per second)\n",
      "llama_perf_context_print:       total time =   80489.36 ms /  1025 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🤖 **AI Response:**\n",
      "  Google Cloud's revenue increased from $9,192 million in 2023 to $11,955 million in 2024, showing a growth of $2,763 million or 30% year-over-year. \n",
      "This indicates that Google Cloud's revenue is expanding, and the segment is contributing more to Alphabet's overall revenue. The growth rate is significant, suggesting that Google Cloud is gaining traction and increasing its market share. This is a positive trend for Alphabet and its investors. \n",
      "The growth in Google Cloud's revenue can be attributed to the increasing demand for cloud computing services, the expansion of Alphabet's enterprise customer base, and the company's efforts to enhance its cloud offerings. As a result, Google Cloud is becoming a more significant contributor to Alphabet's revenue and profitability. \n",
      "This development has implications for Alphabet's business strategy, as the company may need to invest more in Google Cloud's infrastructure and talent to sustain its growth. Additionally, Alphabet may need to consider how to leverage Google Cloud's strengths to drive growth in other areas of its business. \n",
      "Overall, the growth in Google Cloud's revenue is a positive development for Alphabet and its investors, and it highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Google Cloud's revenue change like?\"\n",
    "\n",
    "# ✅ Directly get the AI-generated response\n",
    "response = query_engine.query(question)\n",
    "\n",
    "# ✅ Print only the final response\n",
    "print(\"\\n🤖 **AI Response:**\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## 🤖 AI Response:\n",
       " Google Cloud's revenue increased from $9,192 million in 2023 to $11,955 million in 2024, showing a growth of $2,763 million or 30% year-over-year. \n",
       "This indicates that Google Cloud's revenue is expanding, and the segment is contributing more to Alphabet's overall revenue. The growth rate is significant, suggesting that Google Cloud is gaining traction and increasing its market share. This is a positive trend for Alphabet and its investors. \n",
       "The growth in Google Cloud's revenue can be attributed to the increasing demand for cloud computing services, the expansion of Alphabet's enterprise customer base, and the company's efforts to enhance its cloud offerings. As a result, Google Cloud is becoming a more significant contributor to Alphabet's revenue and profitability. \n",
       "This development has implications for Alphabet's business strategy, as the company may need to invest more in Google Cloud's infrastructure and talent to sustain its growth. Additionally, Alphabet may need to consider how to leverage Google Cloud's strengths to drive growth in other areas of its business. \n",
       "Overall, the growth in Google Cloud's revenue is a positive development for Alphabet and its investors, and it highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The growth in Google Cloud's revenue is a positive trend for Alphabet and its investors.  It highlights the company's increasing focus on cloud computing and enterprise services.  The growth rate of 30% is a significant increase and indicates that Google Cloud is gaining traction in the market.  This growth is a result of the increasing demand for cloud computing services and Alphabet's efforts to enhance its cloud offerings.  The\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython.display as display\n",
    "formatted_response = f\"\"\"\n",
    "## 🤖 AI Response:\n",
    "{response}\n",
    "\"\"\"\n",
    "\n",
    "# ✅ Display the formatted response in Jupyter Notebook\n",
    "display.Markdown(formatted_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1024)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Chat engine initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    memory=memory,\n",
    "    #condense_question_prompt=custom_prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"✅ Chat engine initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =   13954.12 ms\n",
      "llama_perf_context_print: prompt eval time =   13953.47 ms /  1781 tokens (    7.83 ms per token,   127.64 tokens per second)\n",
      "llama_perf_context_print:        eval time =   83156.27 ms /  1023 runs   (   81.29 ms per token,    12.30 tokens per second)\n",
      "llama_perf_context_print:       total time =   97794.40 ms /  2804 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30% increase in Google Cloud Revenue to $12.0 billion in Q4 2024 compared to the same period in 2023. This growth is led by expansion in Google Cloud Platform (GCP) across core GCP products, AI Infrastructure, and Generative AI Solutions. The revenue run rate for Cloud and YouTube at the end of 2024 is $110 billion. \n",
      "Note: The original query was not provided, but based on the context, it seems like the query was related to Google Cloud Revenue. The answer has been provided based on the information available in the context. \n",
      "\n",
      "Please let me know if you want me to rephrase or provide more information. \n",
      "\n",
      "Also, if you want me to answer any other question based on the context, feel free to ask. \n",
      "\n",
      "I will be happy to help. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "I hope this helps! Please let me know if you have any further questions. \n",
      "\n",
      "Best regards,\n",
      "[Your Name] \n",
      "\n",
      "Please let me know if you need any further assistance. \n",
      "\n",
      "Best\n",
      "\n",
      "⏳ Response Time: 98.04 seconds\n"
     ]
    }
   ],
   "source": [
    "question = \"What is Google Cloud Revenue change like?\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "response = chat_engine.chat(question)\n",
    "end_time = time.time()\n",
    "\n",
    "print(response)\n",
    "print(f\"\\n⏳ Response Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1 prefix-match hit, remaining 580 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   22100.88 ms\n",
      "llama_perf_context_print: prompt eval time =    7188.48 ms /   580 tokens (   12.39 ms per token,    80.68 tokens per second)\n",
      "llama_perf_context_print:        eval time =   37449.29 ms /   511 runs   (   73.29 ms per token,    13.65 tokens per second)\n",
      "llama_perf_context_print:       total time =   44900.50 ms /  1091 tokens\n",
      "Llama.generate: 1 prefix-match hit, remaining 2285 prompt tokens to eval\n",
      "llama_perf_context_print:        load time =   22100.88 ms\n",
      "llama_perf_context_print: prompt eval time =   15713.03 ms /  2285 tokens (    6.88 ms per token,   145.42 tokens per second)\n",
      "llama_perf_context_print:        eval time =   41892.66 ms /   511 runs   (   81.98 ms per token,    12.20 tokens per second)\n",
      "llama_perf_context_print:       total time =   57916.07 ms /  2796 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🧠 AI Response:\n",
      "\n",
      " The performance of other product lines in Google Cloud besides Google Cloud Platform (GCP) and AI Infrastructure is not explicitly mentioned in the provided context. However, it can be inferred that the growth in Google Cloud is led by GCP across core GCP products, AI Infrastructure, and Generative AI Solutions, and that the YouTube exited 2024 at an annual revenue run rate of $110 billion, which is a significant contributor to the growth of Google Cloud. The Gemini app team that is developing the direct consumer interface to our Gemini models joined Google DeepMind in the quarter ended December 31, 2024, but the costs associated with the Gemini app team continue to be reported within our Google Services segment. Therefore, the performance of other product lines in Google Cloud besides GCP and AI Infrastructure is not explicitly mentioned in the provided context. \n",
      "\n",
      "However, the model can generate a question that captures all the relevant context from the conversation, including the mention of other product lines such as YouTube and Gemini app team. This shows that the model has a good understanding of the conversation and is able to use it to generate a new question that is relevant and accurate. \n",
      "\n",
      "The model's ability to perform this task is a measure of its conversational understanding and its ability to generate context-dependent questions. This is a key aspect of conversational AI, as it allows the model to engage in more natural and human-like conversations. \n",
      "\n",
      "In this case, the model is able to generate a question that captures all the relevant context from the conversation, including the mention of other product lines such as YouTube and Gemini app team. This shows that the model has a good understanding of the conversation and is able to use it to generate a new question that is relevant and accurate. \n",
      "\n",
      "Therefore, the performance of other product lines in Google Cloud besides GCP and AI Infrastructure is not explicitly mentioned in the provided context, but the model can generate a question that captures all the relevant context from the conversation. \n",
      "\n",
      "The model's ability to perform this task is a measure of its conversational understanding and its ability to generate context-dependent questions. This is a key aspect of conversational AI, as it allows the model to engage in more natural and human-like conversations. \n",
      "\n",
      "In this case, the model is able to generate a question that captures all the relevant context from the conversation, including the mention of other product lines such as YouTube and Gemini app team. This shows that the model has a good understanding of the conversation and is able to use it to generate a new question that is relevant and accurate. \n",
      "\n",
      "\n",
      "\n",
      "⏳ Response Time: 103.23 seconds\n"
     ]
    }
   ],
   "source": [
    "question = \"How about other product lines?\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "response = chat_engine.chat(question)\n",
    "end_time = time.time()\n",
    "print(\"\\n🧠 AI Response:\\n\")\n",
    "print(response)\n",
    "print(f\"\\n⏳ Response Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "memory_out = memory.to_string()\n",
    "formatted_json = json.loads(memory_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chat_store\": {\n",
      "        \"store\": {\n",
      "            \"chat_history\": [\n",
      "                {\n",
      "                    \"role\": \"user\",\n",
      "                    \"additional_kwargs\": {},\n",
      "                    \"blocks\": [\n",
      "                        {\n",
      "                            \"block_type\": \"text\",\n",
      "                            \"text\": \"How is the performance of Google Cloud?\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"role\": \"assistant\",\n",
      "                    \"additional_kwargs\": {},\n",
      "                    \"blocks\": [\n",
      "                        {\n",
      "                            \"block_type\": \"text\",\n",
      "                            \"text\": \" Google Cloud revenues increased 30% to $12.0 billion led by growth in Google Cloud Platform (GCP) across core GCP products, AI Infrastructure, and Generative AI Solutions. Total operating income increased 31% and operating margin expanded by 5% percentage points to 32%. Net income increased 28% and EPS increased 31% to $2.15. Google Cloud is seeing stronger customer demand. The AI-powered Google Cloud portfolio is seeing stronger customer demand, and YouTube continues to be the leader in streaming watchtime and podcasts. Together, Cloud and YouTube exited 2024 at an annual revenue run rate of $110 billion.  The costs associated with the Gemini app team continue to be reported within our Google Services segment. The Gemini app team that is developing the direct consumer interface to our Gemini models joined Google DeepMind in the quarter ended December 31, 2024.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The Gemini app team that is developing the direct consumer interface to our Gemini models joined Google DeepMind in the quarter ended December 31, 2024. The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within our Google Services segment.  The costs associated with the Gemini app team continue to be reported within\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"role\": \"user\",\n",
      "                    \"additional_kwargs\": {},\n",
      "                    \"blocks\": [\n",
      "                        {\n",
      "                            \"block_type\": \"text\",\n",
      "                            \"text\": \"How about other product lines?\"\n",
      "                        }\n",
      "                    ]\n",
      "                },\n",
      "                {\n",
      "                    \"role\": \"assistant\",\n",
      "                    \"additional_kwargs\": {},\n",
      "                    \"blocks\": [\n",
      "                        {\n",
      "                            \"block_type\": \"text\",\n",
      "                            \"text\": \" The performance of other product lines in Google Cloud besides Google Cloud Platform (GCP) and AI Infrastructure is not explicitly mentioned in the provided context. However, it can be inferred that the growth in Google Cloud is led by GCP across core GCP products, AI Infrastructure, and Generative AI Solutions, and that the YouTube exited 2024 at an annual revenue run rate of $110 billion, which is a significant contributor to the growth of Google Cloud. The Gemini app team that is developing the direct consumer interface to our Gemini models joined Google DeepMind in the quarter ended December 31, 2024, but the costs associated with the Gemini app team continue to be reported within our Google Services segment. Therefore, the performance of other product lines in Google Cloud besides GCP and AI Infrastructure is not explicitly mentioned in the provided context. \\n\\nHowever, the model can generate a question that captures all the relevant context from the conversation, including the mention of other product lines such as YouTube and Gemini app team. This shows that the model has a good understanding of the conversation and is able to use it to generate a new question that is relevant and accurate. \\n\\nThe model's ability to perform this task is a measure of its conversational understanding and its ability to generate context-dependent questions. This is a key aspect of conversational AI, as it allows the model to engage in more natural and human-like conversations. \\n\\nIn this case, the model is able to generate a question that captures all the relevant context from the conversation, including the mention of other product lines such as YouTube and Gemini app team. This shows that the model has a good understanding of the conversation and is able to use it to generate a new question that is relevant and accurate. \\n\\nTherefore, the performance of other product lines in Google Cloud besides GCP and AI Infrastructure is not explicitly mentioned in the provided context, but the model can generate a question that captures all the relevant context from the conversation. \\n\\nThe model's ability to perform this task is a measure of its conversational understanding and its ability to generate context-dependent questions. This is a key aspect of conversational AI, as it allows the model to engage in more natural and human-like conversations. \\n\\nIn this case, the model is able to generate a question that captures all the relevant context from the conversation, including the mention of other product lines such as YouTube and Gemini app team. This shows that the model has a good understanding of the conversation and is able to use it to generate a new question that is relevant and accurate. \\n\\n\"\n",
      "                        }\n",
      "                    ]\n",
      "                }\n",
      "            ]\n",
      "        },\n",
      "        \"class_name\": \"SimpleChatStore\"\n",
      "    },\n",
      "    \"chat_store_key\": \"chat_history\",\n",
      "    \"token_limit\": 1024,\n",
      "    \"class_name\": \"ChatMemoryBuffer\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(formatted_json, indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
