{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader, Settings\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import TokenTextSplitter\n",
    "from config import DOCUMENT_PATH, EMBEDDING_MODEL_NAME, EMBEDDING_MODEL_PATH, TOP_K\n",
    "from llm_loader import load_llm\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.memory import ChatMemoryBuffer\n",
    "\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.core.chat_engine import CondenseQuestionChatEngine\n",
    "from llama_index.core.llms import ChatMessage, MessageRole\n",
    "\n",
    "import chromadb\n",
    "from llama_index.core import StorageContext, VectorStoreIndex\n",
    "from llama_index.vector_stores.chroma import ChromaVectorStore\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (3904) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "ggml_metal_init: skipping kernel_get_rows_bf16                     (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_1row              (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_f32_l4                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_bf16_bf16                  (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mv_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_bf16_f32                   (not supported)\n",
      "ggml_metal_init: skipping kernel_mul_mm_id_bf16_f32                (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h64           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h80           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h96           (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h112          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h128          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_bf16_h256          (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h128      (not supported)\n",
      "ggml_metal_init: skipping kernel_flash_attn_ext_vec_bf16_h256      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_f32_bf16                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_f32                      (not supported)\n",
      "ggml_metal_init: skipping kernel_cpy_bf16_bf16                     (not supported)\n"
     ]
    }
   ],
   "source": [
    "llm = load_llm()\n",
    "Settings.llm = llm  # Assign model globally in LlamaIndex\n",
    "\n",
    "# Load embedding model from local storage\n",
    "# Settings.embed_model = HuggingFaceEmbedding(\n",
    "#     model_name=EMBEDDING_MODEL_NAME,\n",
    "#     cache_folder=EMBEDDING_MODEL_PATH\n",
    "# )\n",
    "\n",
    "Settings.embed_model = HuggingFaceEmbedding(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    cache_folder=\"./embedding_cache\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save into Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setting up ChromaDB with Persistent Storage\n",
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(name=\"financial_reports_2\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and index documents\n",
    "documents = SimpleDirectoryReader(DOCUMENT_PATH).load_data()\n",
    "\n",
    "\n",
    "splitter = SentenceSplitter(\n",
    "    chunk_size=1024,\n",
    "    chunk_overlap=50,\n",
    ")\n",
    "nodes = splitter.get_nodes_from_documents(documents)\n",
    "\n",
    "for i, node in enumerate(nodes):\n",
    "    text = node.get_text()\n",
    "    embedding = Settings.embed_model.get_text_embedding(text)\n",
    "    \n",
    "    chroma_collection.add(ids=[f\"doc_{i}\"], documents=[text], embeddings=[embedding])\n",
    "print(f\"Stored {len(nodes)} document chunks in ChromaDB.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load From Chroma DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "chroma_client = chromadb.PersistentClient(path=\"./chroma_db\")\n",
    "chroma_collection = chroma_client.get_or_create_collection(\"financial_reports_2\")\n",
    "vector_store = ChromaVectorStore(chroma_collection=chroma_collection)\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_vector_store(\n",
    "    vector_store=vector_store,\n",
    "    storage_context=storage_context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a test question\n",
    "# query = \"What is Google Cloud Revenue change like?\"\n",
    "\n",
    "# # ‚úÖ Retrieve relevant document chunks\n",
    "# retriever = index.as_retriever(similarity_top_k=3)  # Adjust similarity_top_k as needed\n",
    "# retrieved_docs = retriever.retrieve(query)\n",
    "\n",
    "# # ‚úÖ Print retrieved chunks\n",
    "# print(\"\\nüîç **Retrieved Chunks:**\")\n",
    "# for i, doc in enumerate(retrieved_docs):\n",
    "#     print(f\"\\n--- Chunk {i+1} ---\\n{doc.text}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Query engine initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "Rewrite the user's follow-up question as a standalone question.\n",
    "\n",
    "1. Include all relevant past context.\n",
    "2. Keep it natural and grammatically correct.\n",
    "3. If already standalone, return it unchanged.\n",
    "\n",
    "<Chat History>\n",
    "{chat_history}\n",
    "\n",
    "<User's Follow-Up Question>\n",
    "{question}\n",
    "\n",
    "<Rewritten Standalone Question>\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "response_prompt = PromptTemplate(\n",
    "    \"\"\"\\\n",
    "You are an AI assistant providing structured responses.\n",
    "\n",
    "### **Instructions:**\n",
    "- Answer clearly and concisely.\n",
    "- Summarize retrieved context to avoid duplication.\n",
    "- Summarize the key facts efficiently.\n",
    "- If the context lacks enough details, say: \"I don‚Äôt have enough information.\"\n",
    "- Format responses in natural sentences.\n",
    "\n",
    "<Retrieved Context>\n",
    "{context}\n",
    "\n",
    "<User's Query>\n",
    "{question}\n",
    "\n",
    "### **AI Response:**\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "\n",
    "query_engine = index.as_query_engine(\n",
    "    response_mode=\"compact\",\n",
    "    response_prompt=response_prompt,\n",
    "    similarity_top_k=3,\n",
    "    max_tokens = 300,\n",
    "    streaming=False\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Query engine initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ü§ñ **AI Response:**\n",
      " \n",
      "The context information provides details about Alphabet Inc.'s other income (expense), net, and the reconciliation from GAAP net cash provided by operating activities to non-GAAP free cash flow for the quarter ended December 31, 2024. The table shows that Alphabet Inc. had an other income (expense), net of $715 million in Q4 2024, compared to $1,271 million in Q4 2023. The fluctuations in the value of investments significantly contributed to the volatility of other income (expense), net.\n",
      "\n",
      "Additionally, the statement of cash flows shows that Alphabet Inc. had net cash provided by operating activities of $39,113 in Q4 2024, compared to $101,746 in Q4 2023. The decrease in net cash provided by operating activities was primarily due to a decrease in net income and an increase in capital expenditures.\n",
      "\n",
      "Furthermore, the context information includes a comparison of revenues from the year ended December 31, 2023, to the year ended December 31, \n"
     ]
    }
   ],
   "source": [
    "question = \"What insights can you share about the financial performance?\"\n",
    "\n",
    "# ‚úÖ Directly get the AI-generated response\n",
    "response = query_engine.query(question)\n",
    "\n",
    "# ‚úÖ Print only the final response\n",
    "print(\"\\nü§ñ **AI Response:**\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ChatMemoryBuffer.from_defaults(token_limit=1024)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Chat engine initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "chat_engine = CondenseQuestionChatEngine.from_defaults(\n",
    "    query_engine=query_engine,\n",
    "    memory=memory,\n",
    "    #condense_question_prompt=custom_prompt,\n",
    "    verbose=False,\n",
    ")\n",
    "\n",
    "\n",
    "print(\"‚úÖ Chat engine initialized successfully!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Based on the provided context information, Alphabet Inc.'s financial performance for the quarter and year ended December 31, 2024, shows an increase in revenues, other income (expense), net, and total assets compared to the same periods in 2023. The company's revenues increased by 15% in constant currency terms, driven by growth in all regions, including the United States, Europe, Middle East, and Africa (EMEA), Asia Pacific (APAC), and Other Americas. The increase in revenues was also reflected in the other income (expense), net, which saw a significant jump due to gains on equity securities and performance fees. The company's free cash flow also improved, with a net increase of $3,507 million compared to the same period in 2023. However, it is important to note that fluctuations in the value of investments can significantly contribute to the volatility of other income (expense) in future periods. Additionally, the company's net cash used in financing activities increased due to stock repurchases and net payments related to stock-based award activities. Overall, the financial performance indicates strong growth for Alphabet\n",
      "\n",
      "‚è≥ Response Time: 20.99 seconds\n"
     ]
    }
   ],
   "source": [
    "question = \"What insights can you share about the financial performance?\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "response = chat_engine.chat(question)\n",
    "end_time = time.time()\n",
    "\n",
    "print(response)\n",
    "print(f\"\\n‚è≥ Response Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How is the cloud business doing?\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "response = chat_engine.chat(question)\n",
    "end_time = time.time()\n",
    "print(\"\\nüß† AI Response:\\n\")\n",
    "print(response)\n",
    "print(f\"\\n‚è≥ Response Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Any developments for the upcoming year in these areas?\"\n",
    "\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "response = chat_engine.chat(question)\n",
    "end_time = time.time()\n",
    "print(\"\\nüß† AI Response:\\n\")\n",
    "print(response)\n",
    "print(f\"\\n‚è≥ Response Time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "memory_out = memory.to_string()\n",
    "formatted_json = json.loads(memory_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(json.dumps(formatted_json, indent=4)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retriever Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core.llama_dataset import download_llama_dataset\n",
    "from llama_index.core.llama_pack import download_llama_pack\n",
    "from llama_index.core import VectorStoreIndex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a basic RAG pipeline, uses service context defaults\n",
    "index = VectorStoreIndex.from_documents(documents=documents)\n",
    "query_engine = index.as_query_engine()\n",
    "\n",
    "# generate prediction dataset\n",
    "prediction_dataset = await rag_dataset.amake_predictions_with(\n",
    "    query_engine=query_engine, show_progress=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local_RAG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
